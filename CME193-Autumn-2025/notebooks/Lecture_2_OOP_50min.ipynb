### Presenter notes — Inheritance & Polymorphism
- **Interface**: `BaseRegressor` communicates expected behavior (`fit`, `predict`).
- **Substitutability**: Adapters show how different implementations can share an API.
- **Why adapters?**: Our existing classes differ slightly from the uniform interface; adapters are a lightweight bridge.
- **Client code**: `evaluate_model` treats any `BaseRegressor` the same; implementation details hidden.
- **Takeaway**: OOP enables architecture where components can be swapped with minimal friction.from __future__ import annotations
from typing import Protocol, runtime_checkable, Iterable
import numpy as np

@runtime_checkable
class BaseRegressor(Protocol):
    def fit(self, X: np.ndarray, y: np.ndarray) -> "BaseRegressor":
        ...
    def predict(self, X: np.ndarray) -> np.ndarray:
        ...

# Adapters: Provide a uniform interface for our existing classes
class UnivariateOLSAdapter:
    def __init__(self, fit_intercept: bool = True):
        self._impl = UnivariateOLS(fit_intercept=fit_intercept)
    def fit(self, X: np.ndarray, y: np.ndarray) -> "UnivariateOLSAdapter":
        self._impl.fit(X, y)
        return self
    def predict(self, X: np.ndarray) -> np.ndarray:
        return np.asarray(self._impl.predict(X))

class UnivariateOnlineOLSAdapter:
    def __init__(self, fit_intercept: bool = True):
        self._impl = UnivariateOnlineOLS(fit_intercept=fit_intercept)
    def fit(self, X: np.ndarray, y: np.ndarray) -> "UnivariateOnlineOLSAdapter":
        for x_i, y_i in zip(np.asarray(X).reshape(-1), np.asarray(y).reshape(-1)):
            self._impl += (float(x_i), float(y_i))
        return self
    def predict(self, X: np.ndarray) -> np.ndarray:
        return np.asarray(self._impl.predict(X))

# Client code that is agnostic to the specific implementation
def evaluate_model(model: BaseRegressor, X: np.ndarray, y: np.ndarray) -> float:
    model.fit(X, y)
    yhat = model.predict(X)
    u = y - yhat
    v = y - y.mean()
    den = float(np.dot(v, v))
    return 1.0 - float(np.dot(u, u)) / den if den != 0.0 else 0.0

# Demo
X_demo = X  # from earlier synthetic data cell
y_demo = y

batch_model = UnivariateOLSAdapter(True)
online_model = UnivariateOnlineOLSAdapter(True)

print("R^2 (batch adapter):", round(evaluate_model(batch_model, X_demo, y_demo), 4))
print("R^2 (online adapter):", round(evaluate_model(online_model, X_demo, y_demo), 4))## Stretch — Inheritance and Polymorphism (Extended)

- **Goal**: Show how a common interface enables interchangeable implementations.
- **Approach**: Define a minimal `BaseRegressor` with `fit` and `predict`.
- **Children**: `UnivariateOLS` (batch) and `UnivariateOnlineOLS` (streaming) implement the same API.
- **Polymorphism**: Write helper functions that work with any `BaseRegressor`.

Presenter note: Emphasize API shape over implementation details; show that client code doesn’t care whether it’s batch or streaming.### Presenter notes — Optional: Online OLS and operator overloading
- **Motivation**: Process data streams or large datasets incrementally; avoid storing all points.
- **State**: Maintain running sums (n, Σx, Σy, Σx², Σxy); compute parameters lazily when needed.
- **Design choices**:
  - `add(x, y)` mutates internal state; invalidates cached parameters.
  - `__iadd__` enables `m += (x, y)`; `__add__` returns a new object (non-mutating).
  - Properties `slope`/`intercept` trigger computation only when requested.
- **Teaching tip**: Ingest a few points interactively; compare with batch OLS on the same data.
- **OOP angle**: Operator overloading demonstrates polymorphism with built-in syntax; shows careful state invalidation/caching.
### Presenter notes — Block 5: Dunder methods `__str__`, `__repr__`
- **`__repr__`**: Unambiguous representation for debugging; include config and learned state.
- **`__str__`**: Friendly printing; switch message depending on fitted vs not fitted.
- **Teaching tip**: Print before and after fitting to emphasize object lifecycle.
- **OOP angle**: Integrates with Python tooling (print, REPL, logging).### Presenter notes — Block 4: `score`
- **Metric**: R² = 1 − SS_res / SS_tot (with intercept), else denominator uses ||y||².
- **Behavior**: Calls `predict`, computes residuals, then R²; handles zero denominator by returning 0.
- **Teaching tip**: Show clean vs noisy data R² to calibrate expectations.
- **OOP angle**: Demonstrates method composition and reuse (`predict` within `score`).### Presenter notes — Block 3: `predict`
- **Purpose**: Use learned parameters to generate outputs for new inputs.
- **Precondition**: Require model to be fit; otherwise raise informative error.
- **Design**: Return scalars or arrays; rely on NumPy broadcasting for ergonomics.
- **Teaching tip**: Compare predictions for intercept vs no-intercept models.
- **OOP angle**: Reads `self` state; no mutation if purely predictive.### Presenter notes — Block 2: `fit`
- **Purpose**: Compute parameters (a, b) from data; update object state.
- **Key lines**:
  - Input sanitation: ensure 1-D arrays, equal lengths; guard against shape mismatch.
  - Closed form: with intercept uses `n, Σx, Σy, Σx², Σxy`; without intercept uses `Sxy / Sxx`.
  - Edge cases: denominator zero when all x identical (with intercept) or all x zero (no intercept).
- **Teaching tip**: Print `(m.slope_, m.intercept_)` on a perfect line to build intuition.
- **OOP angle**: Method modifies `self` (learned state persists across method calls).### Presenter notes — Block 1: Class and `__init__`
- **Encapsulation**: Group configuration (`fit_intercept`) and learned parameters (`intercept_`, `slope_`).
- **Constructor**: Convert to `bool` defensively; initialize learned params to `None`.
- **Teaching tip**: Show `model.fit_intercept` and discuss object state vs local variables.
- **Gotcha**: Distinguish configuration (unchanging after construction) vs learned state (mutates after `fit`).### Presenter notes — Synthetic data + OLS plot
- **Purpose**: Make OLS tangible; show the goal visually before classes/methods.
- **Key callouts**:
  - Data: `X ~ Uniform[-3, 3]`, noise `ε ~ N(0, 0.8)`; true line `y = 2.5 + 1.8 x + ε`.
  - Closed-form solution uses sums Σx, Σy, Σx², Σxy; ensures we connect math → code.
  - The red line is the fitted model `y ≈ â + b̂ x`.
- **Transitions**: “We’ll now encapsulate this logic in a class to illustrate OOP.”# Synthetic data and initial OLS fit (closed-form)
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

rng = np.random.default_rng(193)
num_points = 80
X = rng.uniform(-3.0, 3.0, size=num_points)
true_intercept = 2.5
true_slope = 1.8
noise = rng.normal(0.0, 0.8, size=num_points)
y = true_intercept + true_slope * X + noise

# Closed-form OLS with intercept
Sx = X.sum()
Sy = y.sum()
Sxx = float(np.dot(X, X))
Sxy = float(np.dot(X, y))
den = num_points * Sxx - Sx * Sx
b_hat = (num_points * Sxy - Sx * Sy) / den
a_hat = (Sy - b_hat * Sx) / num_points

x_line = np.linspace(X.min() - 0.5, X.max() + 0.5, 100)
y_line = a_hat + b_hat * x_line

plt.figure(figsize=(6, 4))
plt.scatter(X, y, s=18, alpha=0.7, label="data")
plt.plot(x_line, y_line, color="crimson", linewidth=2.0,
         label=f"OLS fit: y ≈ {a_hat:.2f} + {b_hat:.2f} x")
plt.xlabel("x")
plt.ylabel("y")
plt.title("Synthetic data and OLS fit (closed-form)")
plt.legend()
plt.tight_layout()## OLS vs Online OLS — Problem Outline

- Ordinary Least Squares (batch OLS):
  - **Goal**: Fit y ≈ a + b x by minimizing sum of squared residuals.
  - **Inputs**: Full dataset available in memory.
  - **Output**: Closed-form parameters a (intercept), b (slope).
- Online (Streaming) OLS:
  - **Goal**: Update estimates as (x, y) arrive sequentially, without storing all data.
  - **State**: Maintain running statistics (n, Σx, Σy, Σx², Σxy) and compute a, b on demand.
  - **Benefit**: Works with data streams and memory-constrained settings.

Presenter note: Contrast batch vs streaming. Frame Online OLS as the same objective, different computation/constraints.# CME 193 — 50‑Minute Intro to OOP in Python (via Univariate OLS)

This lecture targets `https://web.stanford.edu/class/cme193/index.html` and is designed for slide-style delivery with concise presenter notes.

## Agenda (≈50 minutes)
- What is Univariate OLS? Primer and goals
- OLS vs Online (Streaming) OLS overview
- Block 1: Defining a class and `__init__` (encapsulation)
- Block 2: Instance methods (`fit`)
- Block 3: Using internal state (`predict`)
- Block 4: Quality metric (`score`), docstrings, type hints
- Block 5: Magic/dunder methods: `__str__` / `__repr__`
- Optional: Streaming OLS + operator overloading (`__iadd__`, `__add__`)
- Stretch: Inheritance and polymorphism

Presenter note: Emphasize OOP principles through a tangible example (OLS). Keep math light; focus on how object state and methods interact.