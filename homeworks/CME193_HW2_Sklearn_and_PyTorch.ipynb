{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CME193 HW2: Scikit-learn Pipeline and PyTorch Model for Bike Sharing Demand\n",
        "\n",
        "- **Semester**: Autumn 2025\n",
        "- **Instructor**: Tianyu Du (`tianyudu@stanford.edu`)\n",
        "\n",
        "### Deadline\n",
        "- Deadline: 11/28/2025 (Friday).\n",
        "- You may use your late days for this homework. The **final deadline** will be 12/14/2025 (Sunday). This is the university's grading deadline, you **must** submit your homework before this date, no late submission after this date will be accepted.\n",
        "\n",
        "### Grading rubric (guideline)\n",
        "- 50% correctness of implementations (API, numerics, shapes)\n",
        "- 20% on the performance of your PyTorch model; we have implemented a fairly weak Poisson regression model as a baseline in this homework, **we are expecting your PyTorch model to outperform this baseline**. If you follow our instructions closely, you should be able to achieve this performance easily.\n",
        "- 30% clarity of explanations/interpretations\n",
        "\n",
        "### Overview of the Homework\n",
        "In this homework, you will build a data processing and modeling pipeline for the Bike Sharing Demand dataset we explored in Lecture 7. In the lecture, we have explored a few model options for this dataset, and in this homework, you will be building a data-preprocessing pipeline for this dataset, and a PyTorch model for the prediction task. Since this is a relatively small dataset, you will not need GPU for this homework.\n",
        "\n",
        "### Submitting this homework\n",
        "Rename your notebook to `CME193_HW2_Sklearn_Pipeline_<YOUR_NAME_AND_STANFORD_EMAIL>.ipynb` and submit to **Canvas**. Please keep outputs visible (do not clear them) so we can review your results.\n",
        "\n",
        "### Expected time to complete this homework\n",
        "About 1 hour. If you get stuck, email me (`tianyudu@stanford.edu`) or come to office hours.\n",
        "\n",
        "### Academic integrity\n",
        "Follow the course academic integrity policy. Collaboration is allowed; list collaborators. Write your own code.\n",
        "\n",
        "### AI usage\n",
        "You may consult AI tools (e.g., ChatGPT) for guidance, but you must write and understand your own code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Core data manipulation and visualization libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scikit-learn imports for model building and evaluation\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_validate, RandomizedSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# PyTorch imports for neural network modeling\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Hyperparameter search helpers\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "# Joblib for saving/loading models\n",
        "import joblib\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)\n",
        "\n",
        "# Configure device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set matplotlib style for consistent plotting\n",
        "plt.style.use(\"seaborn-v0_8-whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [Do not modify this section] Data Loading and Preprocessing\n",
        "This block of code loads the dataset and does light preprocessing; please do not modify it.\n",
        "\n",
        "Note: The printed counts of numeric vs categorical columns reflect raw dtypes. For modeling, we treat time-based integers (`hour`, `month`, `weekday`) as categorical features later, so those counts may differ from the dtype summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Load the dataset (Bike Sharing) and define X, y\n",
        "\n",
        "bike = fetch_openml(name=\"Bike_Sharing_Demand\", as_frame=True, parser=\"auto\")\n",
        "df = bike.frame.copy()\n",
        "print(\"Bike Sharing shape:\", df.shape)\n",
        "\n",
        "# Normalize column names across OpenML versions\n",
        "rename_map = {\n",
        "    \"cnt\": \"count\",\n",
        "    \"atemp\": \"feel_temp\",\n",
        "    \"hum\": \"humidity\",\n",
        "    \"weathersit\": \"weather\",\n",
        "    \"hr\": \"hour\",\n",
        "    \"mnth\": \"month\",\n",
        "    \"yr\": \"year\",\n",
        "    \"dteday\": \"datetime\",\n",
        "}\n",
        "df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})\n",
        "\n",
        "target_candidates = [c for c in [\"count\", \"cnt\"] if c in df.columns]\n",
        "assert target_candidates, \"Target column not found.\"\n",
        "target = target_candidates[0]\n",
        "if target != \"count\":\n",
        "    df = df.rename(columns={target: \"count\"})\n",
        "target = \"count\"\n",
        "\n",
        "# Derive time-based features if needed\n",
        "datetime_candidates = [c for c in [\"datetime\", \"dteday\"] if c in df.columns]\n",
        "if datetime_candidates:\n",
        "    dt = pd.to_datetime(df[datetime_candidates[0]])\n",
        "    derived = {\n",
        "        \"year\": dt.dt.year,\n",
        "        \"month\": dt.dt.month,\n",
        "        \"day\": dt.dt.day,\n",
        "        \"hour\": dt.dt.hour,\n",
        "        \"weekday\": dt.dt.weekday,\n",
        "    }\n",
        "    for name, values in derived.items():\n",
        "        if name not in df.columns:\n",
        "            df[name] = values\n",
        "\n",
        "# Remove columns that would leak the target or are identifiers\n",
        "cols_to_drop = set(datetime_candidates + [\"casual\", \"registered\", \"instant\", \"index\", \"datetime\", \"dteday\"])\n",
        "cols_to_drop = [c for c in cols_to_drop if c in df.columns]\n",
        "if cols_to_drop:\n",
        "    df = df.drop(columns=cols_to_drop)\n",
        "\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "print(f\"Total features: {len(X.columns)}\")\n",
        "print(\"Numeric cols:\", X.select_dtypes(include=[np.number]).shape[1],\n",
        "      \"| Categorical cols:\", X.select_dtypes(exclude=[np.number]).shape[1])\n",
        "print(\"Target summary:\\n\", y.describe())\n",
        "\n",
        "# 2) Train/test split (chronological hold-out)\n",
        "# Sort by time and take the last 20% as test set to simulate future prediction\n",
        "sort_cols = [c for c in [\"year\", \"month\", \"day\", \"hour\"] if c in df.columns]\n",
        "if sort_cols:\n",
        "    df_sorted = df.sort_values(by=sort_cols).reset_index(drop=True)\n",
        "else:\n",
        "    df_sorted = df.reset_index(drop=True)\n",
        "\n",
        "X_sorted = df_sorted.drop(columns=[target])\n",
        "y_sorted = df_sorted[target]\n",
        "\n",
        "split_idx = int(len(df_sorted) * 0.8)\n",
        "X_train, X_test = X_sorted.iloc[:split_idx], X_sorted.iloc[split_idx:]\n",
        "y_train, y_test = y_sorted.iloc[:split_idx], y_sorted.iloc[split_idx:]\n",
        "\n",
        "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
        "\n",
        "# 3) Feature typing: numeric vs categorical (no cyclical encoding)\n",
        "categorical_features = [\"season\", \"weather\", \"holiday\", \"workingday\", \"weekday\", \"hour\", \"month\"]\n",
        "numeric_features = [c for c in X_train.columns if c not in categorical_features]\n",
        "\n",
        "print(f\"Categorical features: {categorical_features}\")\n",
        "print(f\"Numeric features: {numeric_features}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [TODO] Review the preprocessing pipeline (you may modify)\n",
        "A working baseline pipeline is provided below using `ColumnTransformer`, `SimpleImputer`, `StandardScaler`, and `OneHotEncoder`. \n",
        "You may tweak imputation strategies, scaling, or which columns are treated as categorical vs numeric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define preprocessing pipeline (no cyclical transforms)\n",
        "transformers = [\n",
        "    # TODO: Add your transformation here, please refer to Lecture 7 for more details.\n",
        "]\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [TODO] Briefly explain your preprocessing design (2–3 sentences)\n",
        "- What imputers and scalers did you choose and why?\n",
        "- Which columns are categorical vs numeric, and why?\n",
        "- Optional: any alternatives you considered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [Do not modify this section] Baseline Model: Ridge and Poisson Regression (Covered in Lecture)\n",
        "We now establish two baseline models, the Ridge regression (i.e., linear regression with L2 regularization you implemented in the previous homework) and the Poisson regression (covered in Lecture 7).\n",
        "\n",
        "These two models serve as weak baselines for the PyTorch model you will be building in this homework. You do not need to tune these baselines; just run them.\n",
        "\n",
        "**Grading Note**:\n",
        "<mark>\n",
        "Now we fit the model and evaluate the performance on the test set. To verify that your PyTorch model is working correctly, we are expecting it to outperform these two baselines.\n",
        "</mark>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import PoissonRegressor, Ridge\n",
        "\n",
        "\n",
        "ridge_pipe = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"model\", Ridge(max_iter=50000)),\n",
        "])\n",
        "\n",
        "poisson_pipeline = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"model\", PoissonRegressor(alpha=1.0, max_iter=1000))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit and evaluate Ridge\n",
        "ridge_pipe.fit(X_train, y_train)\n",
        "y_pred_ridge = ridge_pipe.predict(X_test)\n",
        "ridge_mae = mean_absolute_error(y_test, y_pred_ridge)\n",
        "ridge_rmse = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
        "ridge_r2 = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "print(f\"Ridge Regression Test MAE: {ridge_mae:,.1f}\")\n",
        "print(f\"Ridge Regression Test RMSE: {ridge_rmse:,.1f}\")\n",
        "print(f\"Ridge Regression Test R²: {ridge_r2:.3f}\")\n",
        "\n",
        "# Fit and evaluate Poisson\n",
        "poisson_pipeline.fit(X_train, y_train)\n",
        "y_pred_poisson = poisson_pipeline.predict(X_test)\n",
        "poisson_mae = mean_absolute_error(y_test, y_pred_poisson)\n",
        "poisson_rmse = np.sqrt(mean_squared_error(y_test, y_pred_poisson))\n",
        "poisson_r2 = r2_score(y_test, y_pred_poisson)\n",
        "\n",
        "print(f\"Poisson Regression Test MAE: {poisson_mae:,.1f}\")\n",
        "print(f\"Poisson Regression Test RMSE: {poisson_rmse:,.1f}\")\n",
        "print(f\"Poisson Regression Test R²: {poisson_r2:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTorch Residual Network\n",
        "\n",
        "This section mirrors the modeling pipeline with PyTorch. The data prep and training loop are provided; focus your effort on experimenting with the shallow residual network architecture and its hyperparameters. Fill in the TODOs to customize the model depth, hidden sizes, and residual connections.\n",
        "\n",
        "Please refer to Lecture 8 for more details on PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [Do not modify this section] PyTorch train/evaluate helper (dataloader + training loop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate_pytorch_model(model, X_train, y_train, X_test, y_test, preprocessor,\n",
        "                                      epochs=25, lr=5e-4, weight_decay=1e-5, batch_size=256,\n",
        "                                      patience=10, min_delta=1e-3):\n",
        "    \"\"\"\n",
        "    Train a PyTorch model from scratch and evaluate on test set.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model instance\n",
        "        X_train: Training features (pandas DataFrame)\n",
        "        y_train: Training labels (pandas Series)\n",
        "        X_test: Test features (pandas DataFrame)\n",
        "        y_test: Test labels (pandas Series)\n",
        "        preprocessor: sklearn preprocessor (fitted inside on X_train)\n",
        "        epochs: Number of training epochs\n",
        "        lr: Learning rate\n",
        "        weight_decay: L2 regularization strength\n",
        "        batch_size: Batch size for training\n",
        "        patience: Number of epochs with no improvement before early stopping\n",
        "        min_delta: Minimum improvement in validation loss to reset patience\n",
        "\n",
        "    Returns:\n",
        "        results: Dictionary with loss history, metrics, and test predictions\n",
        "    \"\"\"\n",
        "    # Recreate the preprocessing pipeline to obtain a dense design matrix for PyTorch\n",
        "    preprocessor.fit(X_train, y_train)\n",
        "\n",
        "    X_train_enc = preprocessor.transform(X_train)\n",
        "    X_val_enc = preprocessor.transform(X_test)\n",
        "\n",
        "\n",
        "    def to_tensor(matrix):\n",
        "        if hasattr(matrix, \"toarray\"):\n",
        "            matrix = matrix.toarray()\n",
        "        return torch.tensor(matrix, dtype=torch.float32)\n",
        "\n",
        "\n",
        "    X_train_tensor = to_tensor(X_train_enc)\n",
        "    X_val_tensor = to_tensor(X_val_enc)\n",
        "    y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
        "    y_val_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Training loop\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.MSELoss()\n",
        "    history = {\"train_loss\": [], \"val_loss\": []}\n",
        "    best_val = float(\"inf\")\n",
        "    best_state = None\n",
        "    best_epoch = -1\n",
        "    epochs_ran = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_train = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_train += loss.item() * xb.size(0)\n",
        "        train_loss = running_train / len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        running_val = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(device)\n",
        "                yb = yb.to(device)\n",
        "                preds = model(xb)\n",
        "                val_loss = criterion(preds, yb)\n",
        "                running_val += val_loss.item() * xb.size(0)\n",
        "        val_loss = running_val / len(val_loader.dataset)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        epochs_ran = epoch + 1\n",
        "\n",
        "        if val_loss + min_delta < best_val:\n",
        "            best_val = val_loss\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            best_epoch = epoch\n",
        "\n",
        "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "            status = \"*\" if epoch == best_epoch else \"\"\n",
        "            print(f\"Epoch {epoch + 1:02d} | train MSE: {train_loss:.2f} | val MSE: {val_loss:.2f} {status}\")\n",
        "\n",
        "        if best_epoch != -1 and (epoch - best_epoch) >= patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch + 1:02d} (best epoch {best_epoch + 1:02d})\")\n",
        "            break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_preds = model(X_val_tensor.to(device)).cpu().numpy().squeeze()\n",
        "\n",
        "\n",
        "    torch_rmse = np.sqrt(mean_squared_error(y_test, val_preds))\n",
        "    torch_mae = mean_absolute_error(y_test, val_preds)\n",
        "    torch_r2 = r2_score(y_test, val_preds)\n",
        "\n",
        "    metrics = {\"rmse\": torch_rmse, \"mae\": torch_mae, \"r2\": torch_r2}\n",
        "\n",
        "    print(f\"\\nPyTorch Test RMSE: {torch_rmse:,.1f}\")\n",
        "    print(f\"PyTorch Test MAE:  {torch_mae:,.1f}\")\n",
        "    print(f\"PyTorch Test R²:  {torch_r2:.3f}\")\n",
        "\n",
        "    results = {\n",
        "        \"history\": history,\n",
        "        \"metrics\": metrics,\n",
        "        \"predictions\": val_preds,\n",
        "        \"best_epoch\": best_epoch + 1 if best_epoch != -1 else epochs_ran,\n",
        "        \"epochs_ran\": epochs_ran,\n",
        "    }\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [TODO] Define the PyTorch Neural Network\n",
        "\n",
        "Implement a shallow residual neural network for regression using `torch.nn`. \n",
        "\n",
        "As covered in the lecture (Lecture 8: PyTorch models), a minimal PyTorch model is defined by subclassing `nn.Module`, and implementing both the `__init__` and `forward` methods.\n",
        "\n",
        "1. The `__init__` method is used to define different components of the model, such as the layers, activation functions, etc.\n",
        "2. The `forward` method is used to define the forward pass of the model, i.e., how the input data is transformed through the model and how the predictions are computed.\n",
        "\n",
        "Outside the model definition, we also need to define the dataloader, optimizer, and training loop. To reduce the complexity of this homework, we have provided the dataloader and training loop in the `train_and_evaluate_pytorch_model` function and you do not need to implement them in this homework. You are welcome to look into the code to understand how the dataloader and training loop are implemented.\n",
        "\n",
        "## Mathematical forward pass\n",
        "We begin by defining the mathematical structure of the three-layer neural network we will be implementing in this exercise.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{h}_1 &= \\mathrm{ReLU}(\\mathbf{W}_1\\,\\mathbf{x} + \\mathbf{b}_1) \\\\\n",
        "\\mathbf{r}   &= \\mathrm{ReLU}(\\mathbf{W}_2\\,\\mathbf{h}_1 + \\mathbf{b}_2) \\\\\n",
        "\\mathbf{h}_2 &= \\mathrm{ReLU}(\\mathbf{W}_3\\,\\mathbf{r} + \\mathbf{b}_3 + \\mathbf{h}_1) \\quad\\text{(residual add)}\\\\\n",
        "\\mathbf{d}   &= \\mathrm{Dropout}_p(\\mathbf{h}_2) \\quad\\text{(zero-out activations with prob. }p\\text{ during training, this does not affect dimensions)}\\\\\n",
        "\\hat{y}       &= \\mathbf{w}_4^{\\top}\\,\\mathbf{d} + b_4\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Let $d_\\text{input}$ denote the dimension of the input data $\\mathbf{x}$, and $d_\\text{hidden}$ denote the dimension of the hidden layers. \n",
        "The trainable parameters in this model are the following:\n",
        "1. Weights and biases for the first layer, mapping from the raw input ($\\mathbf{x}$) to the first set of hidden units ($\\mathbf{h}_1$), $\\mathbf{W}_1 \\in \\mathbb{R}^{d_\\text{input} \\times d_\\text{hidden}}$, $\\mathbf{b}_1 \\in \\mathbb{R}^{d_\\text{hidden}}$.\n",
        "2. There are no learnable parameters for the ReLU activation function.\n",
        "3. Weights and biases for the second layer, mapping from the first set of hidden units ($\\mathbf{h}_1$) to the residual component ($\\mathbf{r}$), $\\mathbf{W}_2 \\in \\mathbb{R}^{d_\\text{hidden} \\times d_\\text{hidden}}$, $\\mathbf{b}_2 \\in \\mathbb{R}^{d_\\text{hidden}}$.\n",
        "4. Weights and biases for the third layer, mapping from the residual component ($\\mathbf{r}$) to the second set of hidden units ($\\mathbf{h}_2$), $\\mathbf{W}_3 \\in \\mathbb{R}^{d_\\text{hidden} \\times d_\\text{hidden}}$, $\\mathbf{b}_3 \\in \\mathbb{R}^{d_\\text{hidden}}$.\n",
        "5. A dropout layer with rate $p$ is applied before the final output layer. Dropout has no trainable parameters; during training it multiplies $\\mathbf{h}_2$ by a Bernoulli mask (keep probability $1-p$) to discourage co-adaptation of hidden units, while during evaluation it passes activations through unchanged. There is no learnable parameters in the dropout layer.\n",
        "6. Finally, the output layer maps the (possibly dropped-out) hidden activations to the prediction ($\\hat{y}$), with $\\mathbf{w}_4 \\in \\mathbb{R}^{d_\\text{hidden}}$ and $b_4 \\in \\mathbb{R}$.\n",
        "\n",
        "You can always create PyTorch tensors (matrices) (i.e., using `nn.Parameter`) for the trainable parameters. But a more efficient way (as we discussed in the lecture) is to use `nn.Linear` to create the layers, and then use the `nn.ReLU` activation function. For regularization, you can also insert `nn.Dropout(p)` modules to randomly zero-out activations during training (e.g., `p=0.1`).\n",
        "\n",
        "To create a linear function, you can use `linear_layer = nn.Linear(input_dim, output_dim)` (by default, the bias/intercept is included). Calling this module with input $\\mathbf{x}$ will yield the following:\n",
        "\n",
        "$$\n",
        "\\texttt{linear\\_layer}(\\mathbf{x}) = \\mathbf{W}\\,\\mathbf{x} + \\mathbf{b}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{W} \\in \\mathbb{R}^{d_\\text{output} \\times d_\\text{input}}$ and $\\mathbf{b} \\in \\mathbb{R}^{d_\\text{output}}$.\n",
        "\n",
        "## Implementation guide\n",
        "Having a closer look at the model definition above, we have four linear mappings in total (i.e., four pairs of weights and biases). In this case, we would want to create four `nn.Linear` modules in the `__init__` method with the corresponding input and output dimensions.\n",
        "\n",
        "In `__init__` (define modules)\n",
        "  - `self.fc1 = nn.Linear(input_dim, hidden_dim)`     ← for $\\mathbf{W}_1,\\mathbf{b}_1$\n",
        "  - `self.fc2 = nn.Linear(hidden_dim, hidden_dim)`    ← for $\\mathbf{W}_2,\\mathbf{b}_2$\n",
        "  - `self.fc3 = nn.Linear(hidden_dim, hidden_dim)`    ← for $\\mathbf{W}_3,\\mathbf{b}_3$\n",
        "  - `self.out = nn.Linear(hidden_dim, 1)`             ← for $\\mathbf{w}_4,b_4$\n",
        "  - `self.act = nn.ReLU()`                            ← for ReLU activation\n",
        "  - `self.drop = nn.Dropout(p=0.1)`                   ← applied once after the residual block\n",
        "\n",
        "In `forward(self, x)`, follow the mathematical structure step by step:\n",
        "  - `h1 = self.act(self.fc1(x))`                      ← computes $\\mathbf{h}_1$\n",
        "  - `r  = self.act(self.fc2(h1))`                     ← computes $\\mathbf{r}$\n",
        "  - `h2 = self.act(self.fc3(r) + h1)`                 ← computes $\\mathbf{h}_2$ with residual add\n",
        "  - `d  = self.drop(h2)`                              ← applies dropout to obtain $\\mathbf{d}$\n",
        "  - `return self.out(d)`                              ← computes $\\hat{y}$\n",
        "\n",
        "## Training hint\n",
        "- Keep the output layer linear (no `ReLU`) after processing the final layer.\n",
        "- Dropout helps regularize the network by randomly dropping hidden units; try `p` in the 0.05–0.2 range if you observe overfitting.\n",
        "- Use `model.train()` during training and `model.eval()` during evaluation to enable/disable dropout as intended.\n",
        "- Start with `hidden_dim` in {64, 128}; `dropout=0.1` works well.\n",
        "- Train with `train_and_evaluate_pytorch_model(model, X_train, y_train, X_test, y_test, preprocessor, ...)`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# You do NOT need to modify this section.\n",
        "# Fit the preprocessor on the training data to learn scaling parameters\n",
        "preprocessor.fit(X_train, y_train)\n",
        "# Determine the number of features after preprocessing (for the neural network input layer)\n",
        "input_dim = preprocessor.transform(X_train).shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [TODO] Implement the `__init__` and `forward` methods in the `BikeDemandResNet` class below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BikeDemandResNet(nn.Module):\n",
        "    \"\"\"Three-layer MLP with a simple residual connection.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int, hidden_dim: int = 128, dropout: float=0.1) -> None:\n",
        "        super().__init__()\n",
        "        # TODO: (student) implement the `__init__` method here.\n",
        "        raise NotImplementedError(\"You need to implement the `__init__` method.\")  # comment this out after you have implemented the method\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # TODO: (student) implement the forward pass here.\n",
        "        raise NotImplementedError(\"You need to implement the `forward` method.\")  # comment this out after you have implemented the method\n",
        "        # return ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage with early stopping:\n",
        "torch_resnet_model = BikeDemandResNet(input_dim=input_dim, hidden_dim=128).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "resnet_run = train_and_evaluate_pytorch_model(\n",
        "    torch_resnet_model,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    preprocessor,\n",
        "    epochs=80,\n",
        "    lr=5e-4,\n",
        "    batch_size=256,\n",
        "    patience=8,\n",
        "    min_delta=5e-4,\n",
        ")\n",
        "print(\"Best epoch:\", resnet_run[\"best_epoch\"], \"of\", resnet_run[\"epochs_ran\"], \"epochs run\")\n",
        "print(\"Metrics:\", resnet_run[\"metrics\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [TODO] Run a few quick experiments and summarize (2-4 sentences)\n",
        "Try varying one or two hyperparameters and report what changed:\n",
        "- learning rate (e.g., 1e-3, 5e-4, 1e-4)\n",
        "- hidden_dim (e.g., 64, 128)\n",
        "- epochs or patience (e.g., 40 vs 80)\n",
        "\n",
        "Optional template to record results:\n",
        "\n",
        "| setting | best epoch | RMSE | MAE | R² |\n",
        "|---|---:|---:|---:|---:|\n",
        "| baseline (lr=5e-4, hidden=128) | | | | |\n",
        "| change 1 | | | | |\n",
        "| change 2 | | | | |\n",
        "\n",
        "Write a short takeaway on what helped or didn’t."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cme193-autumn-2025",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
