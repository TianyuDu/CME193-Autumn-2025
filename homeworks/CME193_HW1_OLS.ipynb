{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "741ad9e7",
   "metadata": {},
   "source": [
    "# CME 193 — Homework 1: Ordinary Least Squares and Regularization with Ridge Regression\n",
    "\n",
    "- **Semester**: Autumn 2025\n",
    "- **Instructor**: Tianyu Du (`tianyudu@stanford.edu`)\n",
    "\n",
    "Please install the following packages before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c705d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751e3f29",
   "metadata": {},
   "source": [
    "## Homework Instructions\n",
    "\n",
    "### Learning objectives\n",
    "- Implement closed-form OLS using efficient NumPy operations (matrix multiply, solve).\n",
    "- Implement predictions for linear models and compute RMSE and $R^2$.\n",
    "- Implement ridge regression with an unpenalized intercept and study the effect of $\\lambda$.\n",
    "- Practice basic object-oriented design in Python (fit/predict API).\n",
    "- Interpret plots and metrics to reason about bias–variance and generalization.\n",
    "\n",
    "### Prerequisites\n",
    "- Python and NumPy basics (arrays, broadcasting, `@`, `.T`, `np.linalg`)\n",
    "- Linear algebra: transpose, dot products, normal equations, we have covered this in the lecture.\n",
    "\n",
    "### What you need to do\n",
    "- Fill in the TODOs:\n",
    "  - `OLSClosedForm.fit` and `predict`\n",
    "  - `RMSE` and `R2`\n",
    "  - `RidgeClosedForm.fit` and `predict`\n",
    "- Run the provided cells to generate data, fit models, and create plots.\n",
    "- Answer the short discussion prompts after each part (a few sentences each).\n",
    "\n",
    "### Dataset used\n",
    "We provide a synthetic data generator that adds an intercept (first column of ones). Do not add a second intercept.\n",
    "\n",
    "### Deliverables\n",
    "- A single executed notebook (`.ipynb`) with:\n",
    "  - Working implementations for OLS, RMSE/R², and ridge\n",
    "  - Plots produced by the notebook\n",
    "  - Brief written answers to the prompts (Parts 1–3)\n",
    "\n",
    "### Grading rubric (guideline)\n",
    "- 70% correctness of implementations (API, numerics, shapes)\n",
    "- 30% clarity of explanations/interpretations\n",
    "\n",
    "### Submitting this homework\n",
    "Rename your notebook to `CME193_HW1_OLS_<YOUR_NAME_AND_STANFORD_EMAIL>.ipynb` and submit to **Canvas**. Please keep outputs visible (do not clear them) so we can review your results.\n",
    "\n",
    "### Expected time to complete this homework\n",
    "About 1 hour. If you get stuck, email me or come to office hours.\n",
    "\n",
    "### Deadline\n",
    "Due **10/31/2025 (Friday)**. You may use late days. For emergencies (medical, family, etc.), email me for a possible extension. See the course website for policy details.\n",
    "\n",
    "### Academic integrity\n",
    "Follow the course academic integrity policy. Collaboration is allowed; list collaborators. Write your own code.\n",
    "\n",
    "### AI usage\n",
    "You may consult AI tools (e.g., ChatGPT) for guidance, but you must write and understand your own code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf734a16",
   "metadata": {},
   "source": [
    "\n",
    "## Helper — Synthetic data generator\n",
    "\n",
    "Generate synthetic linear regression data with $P$ features and $N$ samples. Automatically adds an intercept column to $X$ (first column of ones), and includes the intercept in $\\beta_\\text{true}[0]$.\n",
    "\n",
    "Mathematical description:\n",
    "- Sample coefficients $\\beta_\\text{true} \\in \\mathbb{R}^{P}$ i.i.d. from $\\mathrm{Unif}(0, 10)$ (intercept at index 0).\n",
    "- Sample raw features $Z \\in \\mathbb{R}^{N \\times (P-1)}$ with entries i.i.d. $\\mathcal{N}(0,1)$, and form $X = [\\mathbf{1}, Z] \\in \\mathbb{R}^{N \\times P}$.\n",
    "- Compute the noiseless response $y = X\\,\\beta_\\text{true}$.\n",
    "- Split indices into training and testing sets (shuffle by default), with test fraction $\\text{test\\_size}$.\n",
    "- Return $(X_{\\text{train}}, X_{\\text{test}}, y_{\\text{train}}, y_{\\text{test}}, \\beta_\\text{true})$.\n",
    "\n",
    "Parameters:\n",
    "- $P$: number of non-intercept features.\n",
    "- $N$: number of samples.\n",
    "- $\\text{test\\_size}$: fraction of samples in the test set (default 0.2).\n",
    "- `shuffle`: whether to shuffle before split (default True).\n",
    "- `rng`: optional NumPy generator for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0191c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_linear_data(num_features: int | None = None, num_samples: int | None = None, *,\n",
    "                                   p: int | None = None, n: int | None = None,\n",
    "                                   test_size: float = 0.2, shuffle: bool = True,\n",
    "                                   noise_std: float = 0.5, noise: float | None = None,\n",
    "                                   rng: np.random.Generator | None = None, seed: int | None = None):\n",
    "    \"\"\"\n",
    "    Generate synthetic linear regression data and split into train/test sets.\n",
    "\n",
    "    Adds a column of ones as the first column of X (intercept) and returns the\n",
    "    ground-truth coefficients used to synthesize y.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_features, p : int, optional\n",
    "        Number of non-intercept features P. Provide either `num_features` or `p`.\n",
    "    num_samples, n : int, optional\n",
    "        Number of samples N. Provide either `num_samples` or `n`.\n",
    "    test_size : float, default=0.2\n",
    "        Fraction of samples assigned to the test set (0 < test_size < 1).\n",
    "    shuffle : bool, default=True\n",
    "        Whether to shuffle before splitting.\n",
    "    noise_std, noise : float, optional\n",
    "        Standard deviation of i.i.d. Gaussian noise added to y.\n",
    "        If both are provided, `noise` takes precedence. Use 0 for noiseless data.\n",
    "    rng : numpy.random.Generator, optional\n",
    "        PRNG to use for reproducibility.\n",
    "    seed : int, optional\n",
    "        Integer seed used to construct a default generator when `rng` is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train : ndarray of shape (N_train, P+1)\n",
    "        Training design matrix with intercept in column 0.\n",
    "    X_test : ndarray of shape (N_test, P+1)\n",
    "        Test design matrix with intercept in column 0.\n",
    "    y_train : ndarray of shape (N_train,)\n",
    "        Training responses.\n",
    "    y_test : ndarray of shape (N_test,)\n",
    "        Test responses.\n",
    "    beta_true : ndarray of shape (P+1,)\n",
    "        Ground-truth coefficients, with intercept at index 0.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Feature columns are orthogonalized (and made orthogonal to the intercept)\n",
    "      to promote a well-conditioned design matrix for this homework.\n",
    "    - The first column of X is ones; do not add another intercept elsewhere.\n",
    "    \"\"\"\n",
    "    # Resolve aliases\n",
    "    P = num_features if num_features is not None else p\n",
    "    N = num_samples if num_samples is not None else n\n",
    "    if P is None or N is None:\n",
    "        raise ValueError(\"Provide num_features (or p) and num_samples (or n).\")\n",
    "\n",
    "    # Resolve RNG\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Resolve noise parameter (support both names)\n",
    "    sigma = noise if noise is not None else noise_std\n",
    "\n",
    "    # Sample coefficients including intercept at index 0\n",
    "    beta_true = rng.uniform(0.0, 10.0, size=(P + 1,))\n",
    "\n",
    "    # Sample raw features\n",
    "    X_raw = rng.standard_normal(size=(N, P))\n",
    "\n",
    "    # Orthogonalize X_raw columns so that X = [1, X_raw] is (maximally) full rank.\n",
    "    # 1) Remove any component along the intercept direction (exact projection)\n",
    "    if P > 0:\n",
    "        one = np.ones((N, 1))\n",
    "        one_norm = one / np.linalg.norm(one)\n",
    "        X_raw = X_raw - one_norm @ (one_norm.T @ X_raw)\n",
    "        # 2) Within the subspace orthogonal to ones, orthogonalize columns\n",
    "        if N >= P:\n",
    "            Q, _ = np.linalg.qr(X_raw, mode='reduced')  # N x P with orthonormal columns\n",
    "            X_raw = Q\n",
    "        else:\n",
    "            # P > N: keep maximal rank via SVD-based orthogonalization\n",
    "            U, _, Vt = np.linalg.svd(X_raw, full_matrices=False)\n",
    "            X_raw = U @ Vt  # N x P with rank <= N-1 (still orthogonal to ones)\n",
    "\n",
    "    # Add intercept column\n",
    "    X = np.hstack([np.ones((N, 1)), X_raw])  # (N, P+1)\n",
    "\n",
    "    # Generate response\n",
    "    y = X @ beta_true\n",
    "    if sigma and sigma > 0.0:\n",
    "        y = y + sigma * rng.standard_normal(size=N)\n",
    "\n",
    "    if not (0.0 < test_size < 1.0):\n",
    "        raise ValueError(\"test_size must be a float in (0, 1).\")\n",
    "    n_test = int(round(N * test_size))\n",
    "    n_test = min(max(n_test, 1), max(N - 1, 1))\n",
    "    n_train = N - n_test\n",
    "\n",
    "    indices = rng.permutation(N) if shuffle else np.arange(N)\n",
    "    train_idx = indices[:n_train]\n",
    "    test_idx = indices[n_train:]\n",
    "\n",
    "    X_train = X[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, beta_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6986a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the synthetic data generator (with train/test split)\n",
    "rng = np.random.default_rng(193)\n",
    "P, N = 3, 100\n",
    "X_tr, X_te, y_tr, y_te, beta_demo = generate_synthetic_linear_data(P, N, test_size=0.2, rng=rng)\n",
    "print(\"beta_true:\", np.round(beta_demo, 3))\n",
    "print(\"X_train:\", X_tr.shape, \"y_train:\", y_tr.shape)\n",
    "print(\"X_test:\", X_te.shape, \"y_test:\", y_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756d978e",
   "metadata": {},
   "source": [
    "# Part 1: Implementing the OLS with NumPy and Object-Oriented Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79640c03",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Ordinary Least Squares (OLS)\n",
    "\n",
    "Imagine that you are given a dataset of $n$ samples, each with $p$ features $x_i \\in \\mathbb{R}^p$ (including an intercept) and an outcome (response) $y_i \\in \\mathbb{R}$ for observations $i=1, \\ldots, n$.\n",
    "\n",
    "Putting them together in matrix form, we have \n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "    x_1^\\top \\\\\n",
    "    x_2^\\top \\\\\n",
    "    \\vdots \\\\\n",
    "    x_n^\\top\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n \\times p}\n",
    "$$\n",
    "\n",
    "and a response vector\n",
    "$$\n",
    "y = \\begin{bmatrix}\n",
    "    y_1 \\\\\n",
    "    y_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    y_n\n",
    "\\end{bmatrix} \\in \\mathbb{R}^n.\n",
    "$$\n",
    "\n",
    "We assume a linear model for a response vector $y \\in \\mathbb{R}^n$ and features $X \\in \\mathbb{R}^{n \\times p}$:\n",
    "$$\n",
    " y \\approx X \\, \\beta \\quad \\text{with} \\quad \\beta \\in \\mathbb{R}^p.\n",
    "$$\n",
    "\n",
    "and the goal is to find a \"reasonable\" $\\beta \\in \\mathbb{R}^p$ that can predict the response $y$ from the features $X$.\n",
    "\n",
    "One of the most popular ways to find a \"reasonable\" $\\beta$ is to minimize the mean squared error (MSE):\n",
    "$$\n",
    " J(\\beta) = \\frac{1}{2n}\\,\\|X\\beta - y\\|_2^2.\n",
    "$$\n",
    "The **closed-form (normal equation)** solution (when $X^\\top X$ is invertible, which you can assume in this homework) is:\n",
    "$$\n",
    "\\boxed{\\;\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\\;}\n",
    "$$\n",
    "\n",
    "To make a prediction on some new data $X_{\\text{new}} \\in \\mathbb{R}^{n_{\\text{new}} \\times p}$, you can use the following formula:\n",
    "$$\n",
    "\\hat{y}_{\\text{new}} = X_{\\text{new}} \\hat{\\beta} \\in \\mathbb{R}^{n_{\\text{new}}}.\n",
    "$$\n",
    "\n",
    "### Optional: Derivation of the closed-form solution\n",
    "\n",
    "We derive the normal equation by setting the gradient of the objective to zero. Let $r(\\beta)=X\\beta-y$. Then\n",
    "$$\n",
    "J(\\beta) \\,=\\, \\tfrac{1}{2n}\\, r(\\beta)^\\top r(\\beta).\n",
    "$$\n",
    "Using $\\nabla_\\beta r(\\beta)=X$ and the chain rule,\n",
    "$$\n",
    "\\nabla_\\beta J(\\beta) \\,=\\, \\tfrac{1}{2n} \\cdot 2 X^\\top r(\\beta) \\,=\\, \\tfrac{1}{n} X^\\top (X\\beta - y).\n",
    "$$\n",
    "Setting this to zero yields\n",
    "$$\n",
    "X^\\top (X\\hat{\\beta} - y) = 0 \\;\\Longrightarrow\\; X^\\top X\\,\\hat{\\beta} = X^\\top y.\n",
    "$$\n",
    "If $X^\\top X$ is invertible, the unique solution is\n",
    "$$\n",
    "\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y.\n",
    "$$\n",
    "If not, use the Moore–Penrose pseudoinverse: $\\hat{\\beta}=X^\\dagger y$, but we can assume the design matrix is full rank so this is not needed in this homework.\n",
    "\n",
    "### What about the intercept?\n",
    "You may be thinking about \"do I need to add an intercept myself, and what does it do?\"\n",
    "\n",
    "**The answer is no**: the $X$ matrix generated by the synthetic data generator already includes a first column of ones, so that the first coefficient $\\beta_0$ is the intercept.\n",
    "\n",
    "Here is an example, suppose the original features are $X_{\\text{raw}} \\in \\mathbb{R}^{n \\times (p-1)}$ and the intercept is $\\beta_0 \\in \\mathbb{R}$. Let $X = [\\mathbf{1}, X_{\\text{raw}}] \\in \\mathbb{R}^{n \\times p}$ and $\\beta = [\\beta_0, \\beta_{1:p}]^\\top$. Then\n",
    "$$\n",
    "\\hat y = X \\beta = \\beta_0 \\mathbf{1} + X_{\\text{raw}} \\beta_{1:p}.\n",
    "$$\n",
    "The term $\\beta_0 \\mathbf{1}$ is the intercept.\n",
    "\n",
    "Example (small numeric illustration):\n",
    "$X_{\\text{raw}} = \\begin{bmatrix}2\\\\3\\\\4\\end{bmatrix}$,\n",
    "$X = \\begin{bmatrix}1 & 2\\\\ 1 & 3\\\\ 1 & 4\\end{bmatrix}$,\n",
    "$\\beta = \\begin{bmatrix}1.5\\\\0.5\\end{bmatrix}$.\n",
    "Then\n",
    "$$\n",
    "\\hat y = X \\beta = \\begin{bmatrix}1.5 + 0.5\\cdot2\\\\ 1.5 + 0.5\\cdot3\\\\ 1.5 + 0.5\\cdot4\\end{bmatrix}\n",
    "= \\begin{bmatrix}2.5\\\\3.0\\\\3.5\\end{bmatrix}\n",
    "= \\beta_0 \\mathbf{1} + X_{\\text{raw}} \\, \\beta_{1}.\n",
    "$$\n",
    "\n",
    "### Notation and shapes\n",
    "As a reminder, we have\n",
    "- $n$: number of observations/samples (rows of $X$).\n",
    "- $p$: number of columns of $X$ used in fitting. In this notebook, $p$ includes the intercept (the first column of ones). The synthetic data generator already adds this intercept column; you do not need to add it again.\n",
    "- Shapes we use: $X\\in\\mathbb{R}^{n\\times p}$, $y\\in\\mathbb{R}^{n}$, $\\beta\\in\\mathbb{R}^{p}$. The intercept is $\\beta_0$ (the first entry), corresponding to the first (all-ones) column of $X$.\n",
    "\n",
    "### Remarks\n",
    "- The factor $\\tfrac{1}{2n}$ is a convenient scaling and does not change the minimizer.\n",
    "- If $X^\\top X$ is singular or ill-conditioned, use the Moore–Penrose pseudoinverse or ridge regularization. In this notebook, the synthetic data generator de-means and orthogonalizes the raw features so they are orthogonal to the intercept, promoting full rank.\n",
    "- If you choose not to include an intercept, drop the ones column and use the resulting $p$ (without the intercept) instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba59c64a",
   "metadata": {},
   "source": [
    "### Reference Table for Useful NumPy Operations\n",
    "\n",
    "| Operation | Mathematical notation | Input shape(s) | Output shape | NumPy function(s) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Transpose | $A^\\top$ | (n, p) | (p, n) | `A.T`, `np.transpose(A)` |\n",
    "| Matrix multiplication | $A B$ | (n, p) × (p, m) | (n, m) | `A @ B`, `np.matmul(A, B)` |\n",
    "| Matrix–vector multiplication | $A x$ | (n, p) × (p,) | (n,) | `A @ x`, `np.matmul(A, x)` |\n",
    "| Scalar–matrix product | $\\lambda A$ | scalar × (p, p) | (p, p) | `lambda_ * A`, `np.multiply(lambda_, A)` |\n",
    "| Matrix addition | $A + B$ | (p, p) + (p, p) | (p, p) | `A + B`, `np.add(A, B)` |\n",
    "| Solve linear system | Solve $A x = b$ | A: (p, p), b: (p,) or (p, k) | x: (p,) or (p, k) | `np.linalg.solve(A, b)` |\n",
    "| Matrix inverse | $A^{-1}$ | (p, p) | (p, p) | `np.linalg.inv(A)` |\n",
    "| Identity matrix | $I_p$ | p (dimension) | (p, p) | `np.eye(p)` |\n",
    "| Horizontal stack / concatenate | $[\\mathbf{1}, Z]$ | (n, 1) with (n, p) | (n, p+1) | `np.hstack([ones, Z])` |\n",
    "| Elementwise subtraction (vector) | $u - v$ | (n,) − (n,) | (n,) | `u - v`, `np.subtract(u, v)` |\n",
    "| Elementwise square | $u^2$ | (n,) | (n,) | `u ** 2`, `np.square(u)` |\n",
    "| Sum (reduce) | $\\sum_i u_i$ | (n,) | scalar | `np.sum(u)` |\n",
    "| Mean (reduce) | $\\tfrac{1}{n}\\sum_i u_i$ | (n,) | scalar | `np.mean(u)` |\n",
    "| Square root | $\\sqrt{a}$ | scalar | scalar | `np.sqrt(a)` |\n",
    "| L2 norm | $\\lVert u \\rVert_2$ | (n,) | scalar | `np.linalg.norm(u)` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2f536c",
   "metadata": {},
   "source": [
    "## Your task is to implement the closed-form solution for the OLS coefficients $\\beta$ using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OLSClosedForm:\n",
    "    \"\"\"Ordinary Least Squares solved via the closed-form normal equations.\n",
    "\n",
    "    Expects the first column of `X` to be ones to model the intercept.\n",
    "    After calling `fit`, the learned coefficients are stored in `coef_`.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self.coef_ = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Fit the OLS model using the normal equations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Design matrix. The first column should be ones for the intercept.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Response vector.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        numpy.linalg.LinAlgError\n",
    "            If (X^T X) is singular or not numerically invertible.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Computes beta_hat = (X^T X)^{-1} X^T y.\n",
    "        \"\"\"\n",
    "        # ==============================================================================================================\n",
    "        # TODO: Implement closed-form OLS solution for beta_hat using matrix operations.\n",
    "        # Hint: Prefer np.linalg.solve over explicit inverse for numerical stability.\n",
    "        # ==============================================================================================================\n",
    "        raise NotImplementedError(\"TODO: implement OLSClosedForm.fit using normal equations\")\n",
    "        self.coef_ = ...\n",
    "        assert self.coef_.shape == (X.shape[1],), \"beta_hat has wrong shape\"\n",
    "        return None\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict responses for the given design matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : ndarray of shape (n_samples,)\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the model has not been fit yet.\n",
    "        \"\"\"\n",
    "        if self.coef_ is None:\n",
    "            raise ValueError(\"Model not fit yet.\")\n",
    "        # ==============================================================================================================\n",
    "        # TODO: Implement the prediction using matrix operations.\n",
    "        # ==============================================================================================================\n",
    "        raise NotImplementedError(\"TODO: implement OLSClosedForm.predict\")\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7151011c",
   "metadata": {},
   "source": [
    "# Verify our Implementation (you don't need to change the code here, just need to run it)\n",
    "\n",
    "In this section we stress-test our OLS implementation on a moderate synthetic dataset to verify correctness and numerical behavior.\n",
    "- We generate data with $P=5000$ features (4999 random features + 1 intercept) and $N=10000$ samples using the helper, which also adds the intercept.\n",
    "- If you implement `fit` using vectorized NumPy operations, this should run in a few seconds on a typical laptop.\n",
    "- We fit `OLSClosedForm`, compare the recovered coefficients `beta_hat` (through `model.coef_`) to the ground-truth `beta_true`, and report simple error metrics.\n",
    "- We visualize: (1) coefficient parity (`beta_true` vs `beta_hat`) and (2) prediction parity (`y` vs `y_hat`) on training and test sets.\n",
    "- These checks confirm that the implementation matches the closed-form solution and that predictions align with the underlying model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2764981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data and compare OLS coefficients to ground truth\n",
    "rng = np.random.default_rng(7)\n",
    "P, N = 5000, 10000\n",
    "X_tr, X_te, y_tr, y_te, beta_true = generate_synthetic_linear_data(P, N, test_size=0.25, rng=rng)\n",
    "print(f\"X_tr shape: {X_tr.shape}\")\n",
    "print(f\"X_te shape: {X_te.shape}\")\n",
    "print(f\"y_tr shape: {y_tr.shape}\")\n",
    "print(f\"y_te shape: {y_te.shape}\")\n",
    "print(f\"beta_true shape: {beta_true.shape}\")\n",
    "\n",
    "# Fit OLS using the OLSClosedForm class\n",
    "ols = OLSClosedForm()\n",
    "ols.fit(X_tr, y_tr)\n",
    "beta_hat = ols.coef_\n",
    "\n",
    "# Basic metrics\n",
    "abs_diff = np.abs(beta_hat - beta_true)\n",
    "rel_err = np.linalg.norm(beta_hat - beta_true) / (np.linalg.norm(beta_true) + 1e-12)\n",
    "print(\"beta_true:\", np.round(beta_true, 3))\n",
    "print(\"beta_hat:\", np.round(beta_hat, 3))\n",
    "print(\"abs diff:\", np.round(abs_diff, 3))\n",
    "print(f\"relative error: {rel_err:.3e}\")\n",
    "\n",
    "# Predictions\n",
    "y_hat_tr = ols.predict(X_tr)\n",
    "y_hat_te = ols.predict(X_te)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Plot 1: beta_true vs beta_hat\n",
    "ax = axes[0]\n",
    "ax.scatter(beta_true, beta_hat, s=30, alpha=0.8)\n",
    "minv = min(np.min(beta_true), np.min(beta_hat))\n",
    "maxv = max(np.max(beta_true), np.max(beta_hat))\n",
    "ax.plot([minv, maxv], [minv, maxv], 'r--', linewidth=1.5, label=\"y=x\")\n",
    "ax.set_xlabel(r\"$\\beta_{\\mathrm{true}}$\")\n",
    "ax.set_ylabel(r\"$\\hat{\\beta}_{\\mathrm{OLS}}$\")\n",
    "ax.set_title(\"OLS vs ground truth\")\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "# Plot 2: y vs y_hat (Training)\n",
    "ax = axes[1]\n",
    "ax.scatter(y_tr, y_hat_tr, s=10, alpha=0.5, label=\"Train\")\n",
    "minv = min(np.min(y_tr), np.min(y_hat_tr))\n",
    "maxv = max(np.max(y_tr), np.max(y_hat_tr))\n",
    "ax.plot([minv, maxv], [minv, maxv], 'r--', linewidth=1.5, label=\"y=x\")\n",
    "ax.set_xlabel(r\"$y$ (true)\")\n",
    "ax.set_ylabel(r\"$\\hat{y}$ (predicted)\")\n",
    "ax.set_title(\"Predictions vs ground truth (Train)\")\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "# Plot 3: y vs y_hat (Test)\n",
    "ax = axes[2]\n",
    "ax.scatter(y_te, y_hat_te, s=10, alpha=0.5, label=\"Test\")\n",
    "minv = min(np.min(y_te), np.min(y_hat_te))\n",
    "maxv = max(np.max(y_te), np.max(y_hat_te))\n",
    "ax.plot([minv, maxv], [minv, maxv], 'r--', linewidth=1.5, label=\"y=x\")\n",
    "ax.set_xlabel(r\"$y$ (true)\")\n",
    "ax.set_ylabel(r\"$\\hat{y}$ (predicted)\")\n",
    "ax.set_title(\"Predictions vs ground truth (Test)\")\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f09f702",
   "metadata": {},
   "source": [
    "# TODO: Write a few comments on what you observe in your plots. For example, are $\\beta_{\\text{true}}$ and $\\hat{\\beta}_{\\text{OLS}}$ close to each other? How about the predictions $y$ and $\\hat{y}$? How does the prediction performance compare between the training and testing sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e9c69",
   "metadata": {},
   "source": [
    "# Part 2: Implement the Evaluation Metrics\n",
    "\n",
    "So far we have been relying on the visualizations to understand the performance of our model. In this part, we will implement the evaluation metrics to *quantitatively* evaluate the performance of our model.\n",
    "\n",
    "Let $y_{\\text{true}} \\in \\mathbb{R}^n$ be the observed responses and let $y_{\\text{pred}}\\in\\mathbb{R}^n$ be the model predictions. In linear regression with fitted coefficients $\\hat{\\beta}$ and design matrix $X\\in\\mathbb{R}^{n\\times p}$, we have the linear relationship $\\hat{y} \\equiv y_{\\text{pred}} = X\\hat{\\beta}$ (i.e., $\\hat{y}_i = x_i^\\top\\hat{\\beta}$).\n",
    "\n",
    "We will implement two metrics:\n",
    "- RMSE: $\\operatorname{RMSE}(y_{\\text{true}}, y_{\\text{pred}}) = \\sqrt{\\tfrac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}$.\n",
    "- R$^2$: $R^2(y_{\\text{true}}, y_{\\text{pred}}) = 1 - \\dfrac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2}$, where $\\bar{y}=\\tfrac{1}{n}\\sum_i y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c663e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute root mean squared error (RMSE).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : ndarray of shape (n_samples,)\n",
    "        Ground-truth responses.\n",
    "    y_pred : ndarray of shape (n_samples,)\n",
    "        Predicted responses.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The RMSE between `y_true` and `y_pred`.\n",
    "    \"\"\"\n",
    "    # TODO: implement the RMSE using array operations.\n",
    "    raise NotImplementedError(\"TODO: implement RMSE\")\n",
    "\n",
    "def R2(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute the coefficient of determination R^2.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : ndarray of shape (n_samples,)\n",
    "        Ground-truth responses.\n",
    "    y_pred : ndarray of shape (n_samples,)\n",
    "        Predicted responses.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        R^2 in [-inf, 1]. A value of 1 indicates perfect predictions.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    If `y_true` is constant (zero variance), the denominator is 0 and R^2 is\n",
    "    undefined.\n",
    "    \"\"\"\n",
    "    # TODO: implement the R2 using array operations.\n",
    "    raise NotImplementedError(\"TODO: implement R2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13444af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the OLS regressor on training and testing sets\n",
    "train_rmse = RMSE(y_tr, y_hat_tr)\n",
    "train_r2 = R2(y_tr, y_hat_tr)\n",
    "test_rmse = RMSE(y_te, y_hat_te)\n",
    "test_r2 = R2(y_te, y_hat_te)\n",
    "\n",
    "print(\"OLS Regression Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Training R²:   {train_r2:.4f}\")\n",
    "print(f\"Test RMSE:     {test_rmse:.4f}\")\n",
    "print(f\"Test R²:       {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3265fd6",
   "metadata": {},
   "source": [
    "# TODO: Write a few comments on what you observe in the RMSE and $R^2$ values. How does the performance compare between the training and testing sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458105ee",
   "metadata": {},
   "source": [
    "# Part 3: Implement the Ridge Regression (the Regularized OLS Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f120b39a",
   "metadata": {},
   "source": [
    "### Overfitting and why regularization helps\n",
    "\n",
    "Overfitting happens when a model fits noise/idiosyncrasies in the training data rather than the underlying signal. In OLS, this is common when the feature dimension ($p$) is large relative to the sample size ($n$), or when features are highly correlated. Symptoms include near-zero training error but poor generalization: the model achieves high training set performance but poor test set performance.\n",
    "\n",
    "Regularization adds a constraint or penalty that discourages overly complex solutions, trading a small amount of bias for a large variance reduction:\n",
    "- L2 (ridge): shrinks coefficients toward zero, stabilizes the solution when $X^\\top X$ is ill-conditioned, and improves generalization.\n",
    "- L1 (lasso): promotes sparsity (feature selection) by driving some coefficients exactly to zero.\n",
    "\n",
    "The net effect is lower variance, better-conditioned optimization, and typically improved test performance, especially in high-dimensional or noisy settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce017d",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "Ridge regression adds an $\\ell_2$ penalty to stabilize the solution by shrinking coefficients toward zero. Note that because the first column of $X$ is all ones and the first coefficient in $\\beta$ is the intercept, we do not penalize it:\n",
    "\n",
    "$$\n",
    "\\min_{w=(w_1,w_2,\\ldots,w_p)}\\; \\tfrac{1}{2}\\,\\lVert y - Xw \\rVert_2^2 + \\tfrac{\\lambda}{2}\\,\\lVert \\Gamma w \\rVert_2^2 = \\min_{w}\\; \\tfrac{1}{2}\\,\\lVert y - Xw \\rVert_2^2 + \\tfrac{\\lambda}{2}\\,\\lVert [w_2, \\ldots, w_p] \\rVert_2^2,\\quad \\Gamma = \\mathrm{diag}(0,1,\\ldots,1).\n",
    "$$\n",
    "\n",
    "Explicitly, for $p$ columns in $X$ (the first is ones),\n",
    "$$\n",
    "\\Gamma \\,=\\, \\begin{bmatrix}\n",
    "0 & 0 & \\cdots & 0 \\\\\n",
    "0 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & 1\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{p\\times p}.\n",
    "$$\n",
    "\n",
    "The first order condition yields the normal equations\n",
    "$$\n",
    "\\left(X^\\top X + \\lambda\\,\\Gamma\\right)\\,\\hat{\\beta}_{\\text{ridge}\\,\\lambda} = X^\\top y,\\quad\\Rightarrow\\quad \n",
    "\\boxed{\\;\\hat{\\beta}_{\\text{ridge}\\,\\lambda} = \\left(X^\\top X + \\lambda\\,\\Gamma\\right)^{-1} X^\\top y\\;}\n",
    "$$\n",
    "- **Effect of $\\lambda$**: As $\\lambda \\to 0$, ridge approaches OLS; as $\\lambda$ increases, coefficients shrink and variance decreases (with some bias introduced). With a larger $\\lambda$ (i.e., stronger regularization), the coefficients are more biased but the model is more stable and generalizes better.\n",
    "- **Intercept**: We set the left-top entry of $\\Gamma$ to 0 so the intercept is unpenalized.\n",
    "\n",
    "### Optional: Derivation via gradient and first order condition\n",
    "\n",
    "Start from the ridge objective with $\\Gamma=\\mathrm{diag}(0,1,\\dots,1)$ (no penalty on intercept):\n",
    "$$\n",
    "J(w) = \\tfrac{1}{2}\\,\\lVert y - Xw \\rVert_2^2 + \\tfrac{\\lambda}{2}\\,\\lVert \\Gamma w \\rVert_2^2.\n",
    "$$\n",
    "Expand the gradient using $\\nabla_w\\,\\tfrac{1}{2}\\lVert y - Xw\\rVert_2^2 = -X^\\top(y - Xw)$ and $\\nabla_w\\,\\tfrac{1}{2}\\lVert\\Gamma w\\rVert_2^2 = \\Gamma^\\top\\Gamma w$:\n",
    "$$\n",
    "\\nabla_w J(w) = -X^\\top(y - Xw) + \\lambda\\,\\Gamma^\\top\\Gamma\\,w\n",
    "= (X^\\top X + \\lambda\\,\\Gamma)\\,w - X^\\top y.\n",
    "$$\n",
    "Setting $\\nabla_w J(w)=0$ yields the normal equations\n",
    "$$\n",
    "(X^\\top X + \\lambda\\,\\Gamma)\\,\\hat{w} = X^\\top y,\n",
    "$$\n",
    "which (when the matrix is invertible) gives the closed form\n",
    "$$\n",
    "\\hat{w} = (X^\\top X + \\lambda\\,\\Gamma)^{-1} X^\\top y.\n",
    "$$\n",
    "If $(X^\\top X + \\lambda\\,\\Gamma)$ is singular or poorly conditioned, use a numerically stable solver or the pseudoinverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7a4b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeClosedForm:\n",
    "    \"\"\"Ridge regression solved in closed form with an unpenalized intercept.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    coef_ : ndarray of shape (n_features,)\n",
    "        Fitted coefficients after calling `fit`.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self.coef_ = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, lambda_: float = 1.0) -> None:\n",
    "        \"\"\"\n",
    "        Closed-form ridge regression with no penalty on the intercept.\n",
    "\n",
    "        Solves (X^T X + lambda_ * Gamma) w = X^T y, where Gamma = diag(0, 1, ..., 1).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Design matrix. The first column should be ones for the intercept.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Response vector.\n",
    "        lambda_ : float, default=1.0\n",
    "            Nonnegative regularization strength.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        numpy.linalg.LinAlgError\n",
    "            If the system matrix is singular or ill-conditioned.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - The intercept (first coefficient) is unpenalized via Gamma[0, 0] = 0.\n",
    "        - For lambda_ = 0, this reduces to OLS.\n",
    "        \"\"\"\n",
    "        # ==============================================================================================================\n",
    "        # TODO: implement the closed-form ridge regression solution for beta_hat using matrix operations.\n",
    "        # Hint: build Gamma with Gamma[0,0]=0, then solve (X^T X + lambda_*Gamma) w = X^T y.\n",
    "        # ==============================================================================================================\n",
    "        raise NotImplementedError(\"TODO: implement RidgeClosedForm.fit\")\n",
    "        self.coef_ = ...\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict responses for X using the fitted ridge coefficients.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : ndarray of shape (n_samples,)\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the model has not been fit yet.\n",
    "        \"\"\"\n",
    "        if self.coef_ is None:\n",
    "            raise ValueError(\"Model not fit yet.\")\n",
    "        # ==============================================================================================================\n",
    "        # TODO: implement the prediction using matrix operations.\n",
    "        # ==============================================================================================================\n",
    "        raise NotImplementedError(\"TODO: implement RidgeClosedForm.predict\")\n",
    "        return ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee02e29",
   "metadata": {},
   "source": [
    "## Ridge coefficient paths and train/test performance (you do not need to adjust the code here)\n",
    "\n",
    "Fit ridge for a grid of $\\lambda$ (including $\\lambda=0$ for OLS), visualize coefficient paths, and report train/test $R^2$. We deliberately work in a high-dimensional regime (many parameters/features relative to the number of observations), where OLS can be unstable and prone to overfitting.\n",
    "\n",
    "What to expect as $\\lambda$ increases if your implementation is correct:\n",
    "\n",
    "- Coefficients shrink smoothly toward 0 (intercept unpenalized), reflecting stronger regularization.\n",
    "- Training performance typically degrades monotonically (higher RMSE, lower $R^2$) as bias increases.\n",
    "- Test performance often improves from $\\lambda=0$ up to a moderate $\\lambda$ (variance reduction), then degrades again for very large $\\lambda$ (underfitting). Test RMSE/$R^2$ vs $\\lambda$ often looks U-shaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e2c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge coefficient paths exercise\n",
    "rng = np.random.default_rng(123)\n",
    "P, N = 750, 1000\n",
    "X_tr, X_te, y_tr, y_te, beta_true = generate_synthetic_linear_data(P, N, test_size=0.25, rng=rng)\n",
    "\n",
    "# Include lambda=0 (OLS) and positive lambdas for ridge; use symlog scale for plotting zeros\n",
    "lambdas = np.logspace(-4, 1, 30)\n",
    "coefs = []\n",
    "train_rmses = []\n",
    "train_r2s = []\n",
    "test_rmses = []\n",
    "test_r2s = []\n",
    "\n",
    "rid = RidgeClosedForm()\n",
    "for lam in lambdas:\n",
    "    rid.fit(X_tr, y_tr, lambda_=lam)\n",
    "    coefs.append(rid.coef_.copy())\n",
    "\n",
    "    yhat_tr = rid.predict(X_tr)\n",
    "    yhat_te = rid.predict(X_te)\n",
    "    train_rmses.append(RMSE(y_tr, yhat_tr))\n",
    "    train_r2s.append(R2(y_tr, yhat_tr))\n",
    "    test_rmses.append(RMSE(y_te, yhat_te))\n",
    "    test_r2s.append(R2(y_te, yhat_te))\n",
    "\n",
    "# Print metrics for selected lambdas\n",
    "for lam, train_rmse, train_r2, test_rmse, test_r2 in zip(lambdas, train_rmses, train_r2s, test_rmses, test_r2s):\n",
    "    if lam in [0.0, 0.1, 1.0, 10.0]:\n",
    "        print(f\"lambda={lam:>5}: RMSE_train={train_rmse:.3f}  R2_train={train_r2:.3f}  RMSE_test={test_rmse:.3f}  R2_test={test_r2:.3f}\")\n",
    "\n",
    "# Plot coefficient paths, RMSE, and R2\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Coefficient paths (exclude intercept at index 0)\n",
    "cmap = plt.cm.viridis\n",
    "colors = cmap(np.linspace(0, 1, P))\n",
    "feature_indices = range(1, P + 1)\n",
    "for jj, j in enumerate(feature_indices):\n",
    "    axes[0].plot(lambdas, [c[j] for c in coefs], lw=0.8, alpha=0.6, color=colors[jj])\n",
    "axes[0].set_xscale('symlog', linthresh=1e-3)\n",
    "axes[0].set_xlabel(r'Regularization strength $\\lambda$', fontsize=11)\n",
    "axes[0].set_ylabel('Coefficient value (non-intercept)', fontsize=11)\n",
    "axes[0].set_title('Ridge coefficient paths', fontsize=12, fontweight='semibold')\n",
    "axes[0].grid(True, linestyle='--', alpha=0.3, linewidth=0.5)\n",
    "axes[0].spines['top'].set_visible(False)\n",
    "axes[0].spines['right'].set_visible(False)\n",
    "\n",
    "# RMSE vs lambda\n",
    "axes[1].plot(lambdas, train_rmses, marker='o', markersize=4, lw=2, label='Train', color='#2E86AB')\n",
    "axes[1].plot(lambdas, test_rmses, marker='s', markersize=4, lw=2, label='Test', color='#A23B72')\n",
    "axes[1].set_xscale('symlog', linthresh=1e-3)\n",
    "axes[1].set_xlabel(r'Regularization strength $\\lambda$', fontsize=11)\n",
    "axes[1].set_ylabel('RMSE', fontsize=11)\n",
    "axes[1].set_title('Root Mean Squared Error', fontsize=12, fontweight='semibold')\n",
    "axes[1].grid(True, linestyle='--', alpha=0.3, linewidth=0.5)\n",
    "axes[1].legend(frameon=False, fontsize=10)\n",
    "axes[1].spines['top'].set_visible(False)\n",
    "axes[1].spines['right'].set_visible(False)\n",
    "\n",
    "# R2 vs lambda\n",
    "axes[2].plot(lambdas, train_r2s, marker='o', markersize=4, lw=2, label='Train', color='#2E86AB')\n",
    "axes[2].plot(lambdas, test_r2s, marker='s', markersize=4, lw=2, label='Test', color='#A23B72')\n",
    "axes[2].set_xscale('symlog', linthresh=1e-3)\n",
    "axes[2].set_xlabel(r'Regularization strength $\\lambda$', fontsize=11)\n",
    "axes[2].set_ylabel('R²', fontsize=11)\n",
    "axes[2].set_title('Coefficient of Determination', fontsize=12, fontweight='semibold')\n",
    "axes[2].grid(True, linestyle='--', alpha=0.3, linewidth=0.5)\n",
    "axes[2].legend(frameon=False, fontsize=10)\n",
    "axes[2].spines['top'].set_visible(False)\n",
    "axes[2].spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef669b45",
   "metadata": {},
   "source": [
    "# TODO: write a few comments on what you observe as we increase the regularization strength $\\lambda$. For example, you can comment how the magnitude of the coefficients evolve, how the RMSE and $R^2$ values on the training and testing sets change. How does the regularization affect the coefficients and the model's performance? How would you choose the best model configuration when the model is used for actual real-world prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e479ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f939efaa",
   "metadata": {},
   "source": [
    "# You have completed all the parts of this homework. Congratulations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cme193-autumn-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
