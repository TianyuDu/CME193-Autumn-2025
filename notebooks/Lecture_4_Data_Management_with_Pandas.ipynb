{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdHoeqNMdBk0"
      },
      "source": [
        "# CME 193 - Lecture 5: Data Management and Manipulation with Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Announcement\n",
        "\n",
        "- Homework 1 has been released and is due on **Oct. 31, 2025 (Friday), 11:59 PM**. Please email `tianyudu@stanford.edu` if you face any challenges meeting this deadline.\n",
        "- You can apply your late days to this homework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHZ7AT1tdBk3"
      },
      "source": [
        "# Pandas\n",
        "\n",
        "[Pandas](https://pandas.pydata.org/) is a Python library for dealing with tabular data.  The main thing you'll hear people talk about is the DataFrame object (inspired by R), which is designed to hold tabular data.\n",
        "\n",
        "## Difference between a DataFrame and NumPy Array\n",
        "\n",
        "Pandas DataFrames and NumPy arrays both have similarities to Python lists.  \n",
        "* Numpy arrays are designed to contain data of one type (e.g. Int, Float, ...)\n",
        "* DataFrames can contain different types of data (Int, Float, String, ...)\n",
        "    * Usually each column has the same type\n",
        "    \n",
        "    \n",
        "Both arrays and DataFrames are optimized for storage/performance beyond Python lists\n",
        "\n",
        "Pandas is also powerful for working with missing data, working with time series data, for reading and writing your data, for reshaping, grouping, merging your data, ...\n",
        "\n",
        "## Key Features\n",
        "\n",
        "* File I/O - integrations with multiple file formats\n",
        "* Working with missing data (`.dropna()`, `pd.isnull()`)\n",
        "* Normal table operations: merging and joining, groupby functionality, reshaping via stack, and pivot_tables,\n",
        "* Time series-specific functionality:\n",
        "    * date range generation and frequency conversion, moving window statistics/regressions, date shifting and lagging, etc.\n",
        "* Built in Matplotlib integration for basic plotting\n",
        "\n",
        "## Other Strengths\n",
        "\n",
        "* Strong community, support, and documentation\n",
        "* Size mutability: columns can be inserted and deleted from DataFrame and higher dimensional objects\n",
        "* Powerful, flexible group by functionality to perform split-apply-combine operations on data sets, for both aggregating and transforming data\n",
        "* Make it easy to convert ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects Intelligent label-based slicing, fancy indexing, and subsetting of large data sets\n",
        "\n",
        "## Python/Pandas vs. R\n",
        "\n",
        "* R is a language dedicated to statistics. Python is a general-purpose language with statistics modules.\n",
        "* R has more statistical analysis features than Python, and specialized syntaxes.\n",
        "\n",
        "However, when it comes to building complex analysis pipelines that mix statistics with e.g., text mining, or control of a physical experiment, the richness of Python is an invaluable asset.\n",
        "\n",
        "## Objects and Basic Creation\n",
        "\n",
        "| Name | Dimensions | Description  |\n",
        "| ------:| -----------:|----------|\n",
        "| ```pd.Series``` | 1 | 1D labeled homogeneously-typed array |\n",
        "| ```pd.DataFrame```  | 2| General 2D labeled, size-mutable tabular structure |\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "[Here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) is a link to the documentation for DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# References and Further Reading\n",
        "\n",
        "## Additional Pandas Resources\n",
        "- [Pandas user guide](https://pandas.pydata.org/docs/user_guide/index.html)\n",
        "- [Pandas API reference](https://pandas.pydata.org/docs/reference/index.html)\n",
        "- [10 minutes to Pandas](https://pandas.pydata.org/docs/user_guide/10min.html)\n",
        "- [IO tools (read_csv and friends)](https://pandas.pydata.org/docs/user_guide/io.html)\n",
        "- [GroupBy user guide](https://pandas.pydata.org/docs/user_guide/groupby.html)\n",
        "- [Reshaping/Pivot tables](https://pandas.pydata.org/docs/user_guide/reshaping.html)\n",
        "- [Merging/joining](https://pandas.pydata.org/docs/user_guide/merging.html)\n",
        "- [Visualization](https://pandas.pydata.org/docs/user_guide/visualization.html)\n",
        "\n",
        "## For plotting beyond the basics, which we will cover in future lectures, see:\n",
        "- [Matplotlib](https://matplotlib.org/stable/users/index.html)\n",
        "- [Seaborn](https://seaborn.pydata.org/tutorial.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reading the Series display (repr)\n",
        "\n",
        "- Purpose: print a Series to inspect index, values, name, and dtype\n",
        "- Inputs: none (just evaluate/print the Series)\n",
        "- Output: formatted text showing index labels, values, `Name:`, and `dtype:`\n",
        "\n",
        "Explanation (beginner-friendly): This printed view is like looking at a neat list with labels on the left and numbers or text on the right. It helps you quickly confirm you built the Series you expected before doing more work.\n",
        "\n",
        "Reference: [pandas.Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tG0DMucndBk6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd  # import the pandas library as 'pd' (convention)\n",
        "import numpy as np   # import NumPy for numerical arrays\n",
        "\n",
        "# Make printed DataFrames easier to read in the notebook\n",
        "pd.set_option(\"display.max_rows\", 20)        # show at most 20 rows when printing\n",
        "pd.set_option(\"display.max_columns\", 10)     # show at most 10 columns when printing\n",
        "\n",
        "# A helper string we can print between sections for readability\n",
        "endstring = '\\n' + '-'*50 + '\\n'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVuM72QaYLUL"
      },
      "source": [
        "# Pandas Series (1D Array in Pandas)\n",
        "- Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.).\n",
        "- The axis labels are collectively referred to as the **index**.\n",
        "- Basic method to create a series:\n",
        "\n",
        "```python\n",
        "s = pd.Series(value, index=index, name=name)\n",
        "```\n",
        "\n",
        "where\n",
        "\n",
        "- `value` can be many things:\n",
        "    * A 1-D numpy array or regular Python list\n",
        "    * A Python Dictionary (with keys as labels and values as values)\n",
        "    * A scalar (e.g., a single number, create a series with a single value repeated)\n",
        "\n",
        "- The passed `index` is a list of axis labels (which varies on what data is). \n",
        "    * **Default** index is `(0, 1, 2, ..., len(value) - 1)` if not provided.\n",
        "- `value` and `index` must have the same length.\n",
        "- `name` provides a name to the series; this is particularly helpful if you are working with multiple series.\n",
        "\n",
        "You can think of a Series as a vector with optional labels and an optional name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example of creating a Series: ice-cream sales in the past week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2oMvobNdBk7",
        "outputId": "96c3cd6e-b52b-4cfd-a530-bdd15a01c599",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create a Series with daily ice-cream sales; labels on the left are the index\n",
        "first_series = pd.Series(\n",
        "    [1, 2, 3, 8, 16, 32, 64],  # values of the\n",
        "    index=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"],  # index labels\n",
        "    name=\"sale\",  # optional name for this Series\n",
        ")\n",
        "# Print the Python type and a separator for readability\n",
        "print(first_series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the Series to see labels, values, name, and dtype\n",
        "print(first_series)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect a Series\n",
        "\n",
        "- Check: `s.index` (labels), `s.name` (column name if used in DataFrame), `s.dtype`\n",
        "- Optionally: `s.head()`, `s.isna().sum()` to gauge content and missingness\n",
        "\n",
        "Explanation: Validating shape and metadata early avoids subtle alignment bugs when assembling multiple Series into a table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Access the index (labels) and the raw values array\n",
        "print(f\"Index: {first_series.index}\")\n",
        "print(f\"Values: {first_series.values}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Two (Popular) Ways to Access Values in a Series: selecting with `.loc` (label-based selection) and `.iloc` (position-based)\n",
        "Think of `.loc` as “find by name” and `.iloc` as “find by number.” If your rows are labeled by student names or dates, `.loc` lets you ask for them directly. If you just want “the first 10 rows” or “rows 5 through 9,” use `.iloc`.\n",
        "\n",
        "- `.loc[row_labels, col_labels]`: labels, lists of labels, boolean masks (aligned to index), label slices (`a:b` inclusive)\n",
        "- `.iloc[row_sel, col_sel]`: integer positions, position-based slices (`start:stop`, end-exclusive), lists/arrays of ints, boolean arrays aligned by position\n",
        "- Output: Series (single axis) or DataFrame (2D) for both\n",
        "- Rules: `.loc` is label-aware; `.iloc` is strictly positional\n",
        "\n",
        "References: [DataFrame.loc](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html), [DataFrame.iloc](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Use `.iloc(<INTEGER>)` to access values by position"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKliGsRpR-Gs",
        "outputId": "846aa63f-bcaa-441d-9ef1-0f620854cf1d"
      },
      "outputs": [],
      "source": [
        "# Get the 2nd value by position (0-based indexing)\n",
        "first_series.iloc[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Use `.loc(<LABEL>)` to access values by index label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the value labeled \"Tuesday\" (label-based selection)\n",
        "first_series.loc[\"Tuesday\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iterating over the series with `Series.items()`, just like iterating over a dictionary\n",
        "If you’ve looped over a Python dict with `.items()`, this is similar: you get each label (like a key) and its value. For calculations, prefer built-in Series methods instead of loops—they’re faster and less error-prone.\n",
        "\n",
        "- Purpose: iterate over `(index, value)` pairs of a Series\n",
        "- Output: generator yielding tuples `(label, scalar_value)`\n",
        "- Usage: quick inspection, simple side effects (e.g., printing)\n",
        "\n",
        "\n",
        "Reference: [Series.items](https://pandas.pydata.org/docs/reference/api/pandas.Series.items.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEO8czh0dBlH",
        "outputId": "b19c8d1e-5e46-4133-b460-c022f6e54319"
      },
      "outputs": [],
      "source": [
        "# Iterate over (label, value) pairs; useful for quick printing\n",
        "for (index, value) in first_series.items():\n",
        "    print(f\"key: {index} -> value: {value}\")  # show the label and the value\n",
        "    print(\"-\" * 25)                            # divider for readability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## There are much more you can do with a series (e.g., sorting)... But `DataFrame` is much more interesting!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClVn93b3dBlS"
      },
      "source": [
        "# Pandas DataFrame\n",
        "- [**pandas.DataFrame**](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n",
        " is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of `Series` objects, where each `Series` is a column. It is generally the most commonly used pandas object.\n",
        "- You can create a DataFrame from:\n",
        "    - Dict of 1D ndarrays, lists, dicts, or Series\n",
        "    - 2-D numpy array\n",
        "    - A list of dictionaries\n",
        "    - Another Dataframe\n",
        "\n",
        "``` python\n",
        "df = pd.DataFrame(data, index = index, columns = columns)\n",
        "```\n",
        "\n",
        "- `index` / `columns` are lists of row/column labels. If you pass them, you are guaranteeing the index and/or columns of the DataFrame.\n",
        "- If you do not pass anything in, the input will be constructed by \"common sense\" rules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Annotation: `np.array` (homogeneous arrays)\n",
        "\n",
        "- Syntax: `np.array(object, dtype=None)`\n",
        "- Inputs: sequence-like object; optional `dtype`\n",
        "- Output: NumPy ndarray with a single dtype\n",
        "\n",
        "Explanation: NumPy arrays enforce a uniform dtype across elements, unlike DataFrames which can host heterogeneous column dtypes. Keep this in mind when converting between structures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DjmtCeXvmPV"
      },
      "source": [
        "# Creating a DataFrame\n",
        "## Creating a DataFrame of Homogeneous Type Data (e.g., all float numbers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = np.random.randn(10, 4)  # 10 rows, 4 columns of random numbers, need 10*4 numbers.\n",
        "df = pd.DataFrame(\n",
        "    data,\n",
        "    index=[\"indexA\", \"indexB\", \"indexC\", \"indexD\", \"indexE\", \"indexF\", \"indexG\", \"indexH\", \"indexI\", \"indexJ\"],  # label, or names of the rows.\n",
        "    columns=[\"column_W\", \"column_X\", \"column_Y\", \"column_Z\"]  # names of columns.\n",
        ")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Creating a DataFrame from a Dictionary of Series `pd.DataFrame({\"column_name\": series, \"column_name\": series, ...})`\n",
        "One can create a dataframe from a dictionary of series\n",
        "- The index of the resulting DataFrame will be the union of the indices of the various Series. If there are any nested dicts, these will be first converted to Series.\n",
        "- If no columns are passed, the columns will be the sorted list of dict keys.\n",
        "- Series can have different data types.\n",
        "- The series don't need to be the same length (i.e., could have missing values).\n",
        "\n",
        "How Series align by index when combined: alignment by label is a core Pandas feature that prevents accidental row mixing. Mismatched labels surface as `NaN`, signaling data that needs reconciliation.\n",
        "\n",
        "- Behavior: aligns rows by index labels when multiple Series are combined (e.g., into a DataFrame)\n",
        "- Inputs: Series with potentially different indices\n",
        "- Output: aligned result with union of labels; missing matches become `NaN`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjygv3HBdBlS",
        "outputId": "a4203ca6-58f2-47ef-ff2e-908f3e9bf21a"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary of series\n",
        "# Columns are dictionary keys, indices and values obtained from series\n",
        "a_series = pd.Series([10, 20, 30, 40], index=[\"1_Monday\", \"2_Tuesday\", \"3_Wednesday\", \"4_Thursday\"], name=\"an integer series\")\n",
        "b_series = pd.Series([0.5, 0.6, 0.7], index=[\"1_Monday\", \"2_Tuesday\", \"3_Wednesday\"], name=\"a float series\")\n",
        "c_series = pd.Series([\"Ben & Jerry's\", \"Häagen-Dazs\", \"Breyers\", \"Blue Bell\"], index=[\"1_Monday\", \"2_Tuesday\", \"3_Wednesday\", \"4_Thursday\"], name=\"a string series\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a_series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "b_series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "c_series\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Building a DataFrame from dicts/lists (row-wise construction)\n",
        "\n",
        "- Inputs: dict mapping column name -> list/ndarray\n",
        "- Requirements: all arrays must be the same length unless you pass an explicit `index`\n",
        "- Optional: specify `columns` order and `index` labels\n",
        "- Output: DataFrame with given columns and a default RangeIndex if none provided\n",
        "\n",
        "Explanation: This is the most common constructor when loading data already in Python objects. Validate lengths early to avoid silent misalignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "GGHZSwP_AFC9",
        "outputId": "582dc34d-59be-49c0-a8f8-b6cbc905daf9"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(data={\n",
        "    \"ice-cream sales\": a_series,\n",
        "    \"ice-cream price\": b_series,  # <-- note that b_series is shorter than the other two series.\n",
        "    \"ice-cream brand\": c_series\n",
        "})\n",
        "# Note: pandas use NaN to fill in the missing value in series (i.e., no Thursday sale price here).\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyC32Bw3dBlS"
      },
      "source": [
        "## Creating a DataFrame from a Dictionary of Lists/Arrays `pd.DataFrame({\"column_name\": list/array, \"column_name\": list/array, ...})`\n",
        "If creating a dataframe from dictionary of ndarray / lists\n",
        "- The ndarrays / lists must all be the same length, because list/array does not have index, so there is no way to align the data by index.\n",
        "- If an index is passed, it must clearly also be the same length as the arrays. If no index is passed, the result will be range(n), where n is the array length.\n",
        "- Different types of data can still be combined in a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "6pFaDmJ7dBlT",
        "outputId": "c8e33b62-3750-4aa9-bba6-a9c8772705a7"
      },
      "outputs": [],
      "source": [
        "d = {\"float_data\": [1.0, 2.0, 3.0, 4], \"int_data\": [4.0, 3.0, 2.0, 1.0], \"str_data\": [1, 2, 3, 4]}\n",
        "pd.DataFrame(d, index=[\"a\", \"b\", \"c\", \"d\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data from CSV using [`pd.read_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) \n",
        "Reading CSV files with `pd.read_csv`, think of this as “open a spreadsheet file as a table in Python.” You can tell pandas which columns to read (`usecols`), what types to use (`dtype`), and which columns are dates (`parse_dates`). Only set `index_col` when you truly want that column to become the row labels.\n",
        "\n",
        "- Syntax: `pd.read_csv(path, usecols=None, dtype=None, parse_dates=None, index_col=None, nrows=None)`\n",
        "- Inputs: file path/URL; optional column subset and dtypes; date parsing; index column\n",
        "- Output: DataFrame\n",
        "\n",
        "Reference: [pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the CSV file from the local directory into a DataFrame\n",
        "# pd.read_csv parses the file and infers column types (you can specify dtypes if needed)\n",
        "df_students = pd.read_csv(\"./student_grades.csv\")\n",
        "# You can also read from GitHub and other remote resources; this may be slower for large files\n",
        "# Example:\n",
        "# df_students = pd.read_csv(\n",
        "#     \"https://raw.githubusercontent.com/TianyuDu/CME193-Autumn-2025/refs/heads/main/notebooks/student_grades.csv\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_students"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Speaking of `pd.read_csv()`, you can also write to a csv file using `DataFrame.to_csv()`; Pandas supports many different file formats such as Excel, JSON, HDF5, etc. Please see the [Pandas I/O documentation](https://pandas.pydata.org/docs/user_guide/io.html) for a complete list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting quick structure and stats with `describe()` and `info()`\n",
        "\n",
        "- `DataFrame.describe(include=None, percentiles=None)`: numeric summary by default; `include=\"all\"` adds non-numeric; control quantiles via `percentiles`\n",
        "- `DataFrame.info(memory_usage=\"deep\")`: dtypes, non-null counts, memory footprint\n",
        "- Outputs: summary DataFrame and printed schema-like report\n",
        "\n",
        "Explanation: Start with `info()` to validate column types and missingness; follow with `describe()` for distributional insight. For large text columns, `memory_usage=\"deep\"` reveals true footprint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# By default, only summarize numeric columns.\n",
        "df_students.describe(percentiles=[0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show non-null counts, dtypes, and memory usage (deep tries to estimate object column memory)\n",
        "df_students.info(memory_usage=\"deep\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quickly inspecting your data (`head`, `tail`, `sample`, `shape`, `dtypes`)\n",
        "\n",
        "- `df.head(n)`, `df.tail(n)`, `df.sample(n)`: preview rows from top/bottom/random sample\n",
        "- `df.shape` → tuple `(num_rows, num_cols)`; `df.dtypes` → Series of column dtypes\n",
        "- Output: DataFrame/Series previews and quick structural metadata\n",
        "\n",
        "Explanation: Use small peeks to sanity-check parsing and column types without flooding output. Random sampling can surface rare categories or outliers not visible at the head.\n",
        "\n",
        "References: `DataFrame.head` (`https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html`), `DataFrame.tail` (`https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.tail.html`), `DataFrame.sample` (`https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html`), `DataFrame.dtypes` (`https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic inspect of table size\n",
        "print(\"len(df_student):\", len(df_students))            # number of rows\n",
        "print(\"df_student.shape:\", df_students.shape)          # (rows, columns); index is not counted as a column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Peek at the first few rows to get a sense of what's in the DataFrame\n",
        "# head(n) returns the first n rows; common default is 5\n",
        "# This helps verify column names, types, and sample values\n",
        "df_students.head(n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Peek at the last few rows to see the bottom of the table\n",
        "df_students.tail(n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Note: datafame has \"heterogeneous\" data types, which means that the data in different columns can be of different types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show the data type (dtype) of each column; object = text strings\n",
        "print(df_students.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Counting categories with `value_counts()` and measuring uniqueness with `nunique()`\n",
        "\n",
        "- `Series.value_counts(normalize=False, dropna=True)`: counts occurrences per unique value; set `normalize=True` for proportions\n",
        "- `Series.nunique(dropna=True)`: number of distinct values\n",
        "- Output: Series of counts (sorted desc) and an integer count, respectively\n",
        "\n",
        "magine making a tally of how many times each name appears; `value_counts()` does that for you, optionally as percentages. `nunique()` answers “how many different names are there?” If missing values matter to your question, include them with `dropna=False`.\n",
        "\n",
        "References: [Series.value_counts](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html), [Series.nunique](https://pandas.pydata.org/docs/reference/api/pandas.Series.nunique.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Number of distinct student names (cardinality)\n",
        "df_students[\"student_name\"].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The the number of rows for each unique value of the column letter_grade\n",
        "df_students[\"letter_grade\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the number of occurrences of each unique value in the student_name column\n",
        "# This is less interesting here because each student only appears once\n",
        "# value_counts returns a Series with counts sorted descending by default\n",
        "df_students[\"student_name\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg7nVU2fdBlW"
      },
      "source": [
        "# Subsetting the DataFrame: Indexing and Selection\n",
        "Overview of selection methods—use `df[...]` for columns, `.loc` for label-based selection, and `.iloc` for position-based selection.\n",
        "\n",
        "- 3 methods ``` [],  iloc, loc ```\n",
        "\n",
        "| Operation  | Syntax       | Result |\n",
        "|----|----------------------| ---------------------------|\n",
        "| Select a Single Column | `df[col]`   |    Series                      |\n",
        "| Select a Single Column but I really want a DataFrame | `df[[col]]` | DataFrame |\n",
        "| Select Multiple Columns | `df[[col1, col2]]` | DataFrame |\n",
        "| Select Row by Label | `df.loc[label]` | Series  |\n",
        "| Select Row by Integer Location | `df.iloc[idx]` |      Series                    |\n",
        "| Slice rows | `df[5:10]`        |                        DataFrame  |\n",
        "| Select rows by boolean | `df[mask]`   | DataFrame        |\n",
        "\n",
        "- Note all the operations below are valid on series as well restricted to one dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Note: think of a DataFrame as a bunch of Series objects, one Series for each column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP-wzewvdBlW"
      },
      "source": [
        "## Selecting Columns using `df[..] -> Series` (single square bracket) or `df[[...]] -> DataFrame` (double square bracket)\n",
        "- Selecting a single column using `df[\"col\"]` give you a Series\n",
        "- DataFrame: selection single or multiple columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = df_students[\"hw1_grade\"]\n",
        "print(a)\n",
        "print(type(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# suppose you really want to get a DataFrame (for what reason?) with only one column.\n",
        "d = df_students[[\"hw1_grade\"]]\n",
        "print(d)\n",
        "print(type(d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# suppose only want to share the distribution of grades without revealing the student names.\n",
        "c = df_students[[\"hw1_grade\", \"hw2_grade\", \"attendance_pct\"]]\n",
        "print(c)\n",
        "print(type(c))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPkld6IOdBlY"
      },
      "source": [
        "## Selecting Rows\n",
        "\n",
        "Row selection with `.loc` vs `.iloc`\n",
        "\n",
        "- `.loc` uses labels (names) for row selection, provides a list of labels to select rows.\n",
        "- `.iloc` uses integer positions and excludes the stop in slices (`a:b` returns rows a…b-1)\n",
        "\n",
        "References: [DataFrame.loc](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html), [DataFrame.iloc](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_students"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Pfx-O6cm8Jav",
        "outputId": "777c2c4f-2d7a-430f-f779-8d337204140b"
      },
      "outputs": [],
      "source": [
        "# Select rows by label from 0 to 5 inclusive (label-based slice is inclusive)\n",
        "df_students.loc[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select rows by integer position 0..4 (stop is excluded)\n",
        "df_students.iloc[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_students_v2 = df_students.set_index(\"student_name\")\n",
        "df_students_v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use loc to select rows by label, the list must be a subset of the index!\n",
        "# You can also duplicate a row by using the same label multiple times.\n",
        "df_students_v2.loc[[\"Olivia Brown\", \"Tianyu Du\", \"Tianyu Du\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filtering rows with boolean masks\n",
        "\n",
        "- Compose conditions with `&` (and) and `|` (or); wrap each condition in parentheses\n",
        "- Apply mask with `.loc[mask, columns]` to be explicit about rows and columns\n",
        "\n",
        "Explanation (beginner-friendly): Create a “True/False per row” Series that answers a yes/no question (e.g., “is attendance < 90?”). Then use that Series to keep only the rows where the answer is True.\n",
        "\n",
        "References: [Boolean indexing](https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing), [DataFrame.loc](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "Uj-MbCHKdBlZ",
        "outputId": "c4c473e7-2e16-4d1f-bdc8-f676bb8e75ae"
      },
      "outputs": [],
      "source": [
        "# Build a boolean mask: True for rows that meet a condition\n",
        "boolean_mask = [True, False, True, False, True, False, True, False, True, False, True, False, True, False, True, False]\n",
        "\n",
        "# A realisitc example:\n",
        "# boolean_mask = (df_students[\"letter_grade\"] == True)\n",
        "\n",
        "# Use the mask to select rows; here we use .iloc with the boolean array\n",
        "# (equivalently: df_students.loc[boolean_mask, :])\n",
        "df_students[boolean_mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Note: `df[...]` is acting both as a row and column selector, depending on what we put inside `[]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A concise way to filter rows directly with a boolean expression\n",
        "# This keeps rows where letter_grade is True\n",
        "df_students[df_students[\"letter_grade\"] == True]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Another filter example: keep rows where student_name is not a specific value\n",
        "df_students[df_students[\"student_name\"] != \"Tianyu Du\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Writing filters with `DataFrame.query()`\n",
        "\n",
        "- Express conditions as strings (e.g., `\"attendance_pct < 90 and letter_grade == True\"`)\n",
        "- Column names become variables in the expression\n",
        "\n",
        "If you find long chains of `&`/`|` hard to read, `query` lets you write the condition like a sentence. It can make complex filters easier to maintain.\n",
        "\n",
        "Reference: [DataFrame.query](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use query to express filters as a readable string\n",
        "# Here: keep rows where letter_grade is True\n",
        "df_students.query(\"letter_grade == True\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Another query example: keep rows where attendance is below 90%\n",
        "df_students.query(\"attendance_pct < 90\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Filtering based on NAN Values.\n",
        "Handling missing values\n",
        "\n",
        "- `dropna()` removes rows (or columns when `axis=1`) containing missing values; use `subset=[...]` to limit which columns are checked\n",
        "- `fillna(value or method)` replaces missing values with a provided value or method (e.g., forward-fill)\n",
        "\n",
        "Explanation (beginner-friendly): Missing values are blanks—either discard rows that have them or fill them with a sensible default. Choose based on what makes sense for your analysis.\n",
        "\n",
        "References: [DataFrame.dropna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html), [DataFrame.fillna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html).\n",
        "\n",
        "\n",
        "```python\n",
        "# drop all rows with NAN values in all columns\n",
        "df = df.dropna()\n",
        "# drop all rows with NAN values in a specific column\n",
        "df = df.dropna(subset=[\"column_name\"])\n",
        "# drop all rows with NAN values in any column\n",
        "df = df.dropna(axis=0)\n",
        "# drop all columns with NAN values in any row\n",
        "df = df.dropna(axis=1)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Groupby-Aggregation Operations (Optional)\n",
        "- pandas objects can be split on any of their axes. The abstract definition of grouping is to provide a mapping of labels to group names:\n",
        "- Syntax:\n",
        "    - ``` groups = df.groupby([key1, key2]) ```\n",
        "\n",
        "- The group by concept is that we want to apply the same function on subsets of the dataframe, based on some key we use to split the DataFrame into subsets\n",
        "- This idea is referred to as the \"split-apply-combine\" operation:\n",
        "    - Split the data into groups based on some criteria\n",
        "    - Apply a function to each group independently\n",
        "    - Combine the results\n",
        "\n",
        "![image](https://i.stack.imgur.com/sgCn1.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Grouping with `groupby`\n",
        "\n",
        "- `df.groupby(keys)`: split the table into groups based on one or more columns\n",
        "- Apply reducers (like `.mean()`, `.sum()`) or use `.agg({...})` to compute several statistics at once\n",
        "- Use `.transform(...)` when you want to compute per-group values but keep the original row shape (e.g., z-scores within each group)\n",
        "\n",
        "Explanation (beginner-friendly): Think “split-apply-combine.” First split rows into groups (e.g., by major), then apply the same calculation to each group, then combine the results into a new table.\n",
        "\n",
        "References: [GroupBy basics](https://pandas.pydata.org/docs/user_guide/groupby.html), [GroupBy.agg](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.agg.html), [GroupBy.transform](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.transform.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group rows by the 'letter_grade' boolean column (True/False)\n",
        "g = df_students.groupby(\"letter_grade\")\n",
        "# Show the GroupBy object (lazy until you call an aggregation)\n",
        "g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the per-group mean for selected numeric columns\n",
        "print(g[\"hw1_grade\"].mean())      # average HW1 by letter_grade group\n",
        "print(g[\"hw2_grade\"].mean())      # average HW2 by letter_grade group\n",
        "print(g[\"attendance_pct\"].mean()) # average attendance by letter_grade group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using agg function with dictionary to compute multiple aggregations at once\n",
        "print(g.agg({\n",
        "    \"hw1_grade\": \"mean\",\n",
        "    \"hw2_grade\": \"mean\",\n",
        "    \"attendance_pct\": \"mean\"\n",
        "}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute per-group maxima using agg with a dict of column -> function\n",
        "print(\"Using agg function with dict:\")\n",
        "print(g.agg({\n",
        "    \"hw1_grade\": \"max\",\n",
        "    \"hw2_grade\": \"max\",\n",
        "    \"attendance_pct\": \"max\"\n",
        "}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create new columns by modifying existing datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a new column by working with existing columns (recommended, vectorized operation is faster and more readable than loops)\n",
        "Creating derived columns, treat columns like arrays: `df[\"hw_grade\"] = (df[\"hw1_grade\"] + df[\"hw2_grade\"]) / 2`. Vectorized code is faster and more readable than loops.\n",
        "\n",
        "- Use vectorized arithmetic (operate on entire columns) to build new columns quickly\n",
        "- Assign with `df[\"new_col\"] = ...` or use `df.assign(new_col=...)` to return a modified copy\n",
        "\n",
        "\n",
        "References: [DataFrame.assign](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html), [Binary ops](https://pandas.pydata.org/docs/user_guide/basics.html#binary-operator-functions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute an average homework grade and store it in a new column\n",
        "# Vectorized arithmetic operates on whole columns efficiently\n",
        "df_students[\"hw_grade\"] = (df_students[\"hw1_grade\"] + df_students[\"hw2_grade\"]) / 2\n",
        "# Display the updated DataFrame to confirm the new column\n",
        "df_students"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applying row-wise logic\n",
        "If the new column requires a complicated computation, use the `apply` function. Row-wise apply is like a for-loop over the rows. It’s handy for custom rules, but try to phrase your logic in column-wise operations first.\n",
        "\n",
        "- `df.apply(func, axis=1)` runs a Python function on each row and combines the results (axis=1 means row-wise)\n",
        "- Prefer vectorized alternatives (`np.select`, `pd.cut`, arithmetic/comparisons) when possible—they’re faster and clearer\n",
        "\n",
        "References: [DataFrame.apply](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assign letter grades based on conditions\n",
        "# This function receives a row (like a dict of column -> value) and returns a single grade string\n",
        "\n",
        "def assign_grade(row):\n",
        "    # Check whether the student is taking the letter-graded option\n",
        "    if row[\"letter_grade\"] == True:\n",
        "        # F if attendance is below threshold regardless of scores\n",
        "        if row[\"attendance_pct\"] < 90:\n",
        "            return \"F\"\n",
        "        # A for average homework >= 90, else B\n",
        "        if row[\"hw_grade\"] >= 90:\n",
        "            return \"A\"\n",
        "        elif row[\"hw_grade\"] >= 80:\n",
        "            return \"B\"\n",
        "        else:\n",
        "            return \"C\"\n",
        "    else:\n",
        "        # For P/NP: NP if attendance is below threshold; otherwise P\n",
        "        if row[\"attendance_pct\"] < 90:\n",
        "            return \"NP\"\n",
        "        else:\n",
        "            return \"P\"\n",
        "\n",
        "# Apply the function to each row (axis=1) to compute the final letter grade\n",
        "df_students[\"grade\"] = df_students.apply(assign_grade, axis=1)\n",
        "# Show the DataFrame with the new grade column\n",
        "df_students"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basic Pandas Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_students"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Line Plot for the Trend (less relevant in this example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# Set default figure size to be very wide for better visualization\n",
        "plt.rcParams['figure.figsize'] = (16, 4)\n",
        "\n",
        "\n",
        "# Choose numeric columns for plotting examples\n",
        "numeric_cols = [\"hw1_grade\", \"hw2_grade\", \"attendance_pct\"]\n",
        "\n",
        "# 1) Line plot: plot all numeric series over the row index (ordered axis)\n",
        "df_students.set_index(\"student_name\")[numeric_cols].plot(title=\"Student metrics over index\")  # multiple lines\n",
        "plt.show()  # render the figure in the notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bar plot: compare numeric values across categorical x (student index)\n",
        "\n",
        "- Suitable data: categorical or ordinal x with aggregated numeric y (counts, means)\n",
        "- Shows: differences across groups clearly; good for rank order and comparisons\n",
        "- Use when: comparing categories; avoid with too many categories or when x is truly continuous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_students.set_index(\"student_name\")[numeric_cols].plot.bar(title=\"Grades and attendance by student\", alpha=0.85)  # alpha controls opacity\n",
        "plt.show()  # render\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scatter: relationship between two numeric columns\n",
        "\n",
        "- Suitable data: two continuous numeric variables; optional color/size for a third variable\n",
        "- Use when: exploring associations; avoid if points are too dense—consider hexbin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_students.plot.scatter(x=\"hw1_grade\", y=\"hw2_grade\", title=\"HW1 vs HW2\")  # point per student\n",
        "plt.show()  # render\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Histogram: distribution of each numeric column (overlaid)\n",
        "\n",
        "- Suitable data: one continuous numeric variable\n",
        "- Shows: how values are distributed across bins; choose bin count with care\n",
        "- Use when: comparing distributions or checking skew/outliers; avoid for categorical data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_students[\"hw1_grade\"].plot.hist(alpha=0.5)\n",
        "plt.show()  # render\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Area: emphasize cumulative totals/contributions over an ordered x-axis\n",
        "\n",
        "- Suitable data: one or more numeric series over an ordered x (often time)\n",
        "- Shows: cumulative values and how parts contribute to the total\n",
        "- Use when: you want to emphasize totals and composition over time; avoid with many overlapping categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_students[numeric_cols].plot.area(title=\"Area chart of metrics\", alpha=0.3)  # stacked by default\n",
        "plt.show()  # render\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pie: part-to-whole proportions (few categories); first bucketize HW1 into letter bins\n",
        "\n",
        "- Suitable data: one categorical variable with a small number of categories (≈3–6) and their proportions\n",
        "- Shows: relative contributions of categories to a single whole\n",
        "- Use when: you have few, clearly distinct parts; avoid for precise comparison—bar charts are often better"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_students"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_students[\"grade\"].value_counts().sort_index().plot.pie(autopct=\"%1.0f%%\", ylabel=\"\", title=\"Grade distribution\")\n",
        "plt.show()  # render\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Box plot: summarize distributions (median, quartiles, outliers) per variable\n",
        "\n",
        "- Suitable data: one continuous numeric variable; optionally split by a categorical variable to compare groups\n",
        "- Shows: median, quartiles (IQR), whiskers, and outliers; robust to skew\n",
        "- Use when: you want a compact summary of distributions across groups; avoid when sample sizes are tiny\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_students[numeric_cols].plot.box(title=\"Boxplot of numeric columns\")\n",
        "plt.show()  # render"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDuma8ijdBla"
      },
      "source": [
        "# Merging Multiple Dataframes with `DataFrame.merge` (Optional)\n",
        "\n",
        "- Basic idea: join two tables by matching values in key columns (like SQL joins)\n",
        "- `how=` picks the join type (`\"inner\"`, `\"left\"`, `\"right\"`, `\"outer\"`)\n",
        "- Use `on=` for same-named keys, or `left_on=`/`right_on=` if names differ\n",
        "- Add `indicator=True` to see where rows came from; use `validate=\"m:1\"` or `\"1:1\"` to catch unexpected duplicates\n",
        "\n",
        "Exanoke: Imagine two spreadsheets: one with grades, one with majors. Merge combines them by lining up rows with the same student name.\n",
        "\n",
        "Reference: [DataFrame.merge](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Most Popular) Left joins (`how=\"left\"`) keep all rows from the left DataFrame and drop unmatched right rows—useful for enriching a primary fact table.\n",
        "\n",
        "1. Let's call the left table the primary table, and the right table the supplemental table.\n",
        "2. For each row in the primary table, get the value of the matching key column (e.g., `student_name`, and the name is `Tianyu Du`).\n",
        "3. Look up the matching row in the supplemental table (i.e., look for the rows with `student_name` being `Tianyu Du` in the supplemental right table).\n",
        "4. Append the data from the supplemental table to the row in the primary table.\n",
        "5. If not found, values are left NAN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_students  # the main table we are working on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "roster = pd.read_csv(\"student_roster.csv\")        # right table: roster metadata\n",
        "roster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Note: Tianyu Du from the left (main) table is not in the right (supplemental) table (kept).\n",
        "### Zoe Park is in the supplemental table but not in the main table (dropped)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged = df_students.merge(roster, on=\"student_name\", how=\"left\")\n",
        "merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for example: we can compute the average homework grade for each section.\n",
        "merged.groupby(\"section\").agg({\"hw1_grade\": \"mean\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### There are other ways to merge two tables (i.e., right join, outer join, inner join, etc.), we don't have time to cover all of them here. But I personally recommend you to read this Medium article: https://medium.com/data-science/how-to-merge-pandas-dataframes-35afe8b1497co\n",
        "\n",
        "![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vq8e0dAr0Xsfw0bJRz4FRg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A quick note on Dask\n",
        "Dask DataFrame mirrors much of the pandas API and can scale workflows beyond memory and across cores/clusters. See the [Dask DataFrame docs](https://docs.dask.org/en/stable/dataframe.html).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Rg7nVU2fdBlW"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "cme193-autumn-2025",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
