{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4366c6fe",
   "metadata": {},
   "source": [
    "# Lecture 6 — Introduction to Statistical Learning\n",
    "\n",
    "In this lecture, we will walk through a complete data science research pipeline using scikit-learn. This lecture aims to show you the basic steps of data science with two distinct objectives: regression and binary classification. We will only cover basic linear models in this lecture but the same pipeline can be applied to more complex models that we will cover next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7865659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install scikit-learn if you haven't done so.\n",
    "# ! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694c83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78be50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the computational environment.\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# fix the random seed for reproducibility.\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Create outputs dir\n",
    "Path(\"../outputs\").mkdir(parents=True, exist_ok=True)\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341dfb39",
   "metadata": {},
   "source": [
    "## Notations and High-Level Picture\n",
    "\n",
    "### Input Data\n",
    "- Tabular features: $X \\in \\mathbb{R}^{n \\times p}$\n",
    "  - $n$: number of samples (rows)\n",
    "  - $p$: number of features (columns)\n",
    "\n",
    "### Target/Objective\n",
    "  - Regression: $y \\in \\mathbb{R}^{n}$\n",
    "  - Classification (binary): $y \\in \\{0,1\\}^{n}$\n",
    "  - Build a model $f: X \\to y$ to predict the target $\\hat{y}$ from the features $X$.\n",
    "  - The model $f$ is a function that maps the features $X$ to the target $y$.\n",
    "  - We want to find the best model $f$ that minimizes the prediction error (i.e., the difference between $\\hat{y}$ and $y$).\n",
    "\n",
    "### Split\n",
    "  - Randomly split the data into training and test sets: $(X_{\\text{train}}, y_{\\text{train}}), (X_{\\text{test}}, y_{\\text{test}})$\n",
    "  - Train a model using the training set and evaluate on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d72ef47",
   "metadata": {},
   "source": [
    "## Retrieve the Data for Binary Classification: [Breast Cancer Wisconsin (Diagnostic) Dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset)\n",
    "\n",
    "**Data Set Characteristics:**\n",
    "\n",
    "- **Number of Instances:** 569\n",
    "- **Number of Attributes:** 30 numeric, predictive attributes and the class\n",
    "\n",
    "**Attribute Information:**\n",
    "\n",
    "- radius (mean of distances from center to points on the perimeter)\n",
    "- texture (standard deviation of gray-scale values)\n",
    "- perimeter\n",
    "- area\n",
    "- smoothness (local variation in radius lengths)\n",
    "- compactness (perimeter² / area - 1.0)\n",
    "- concavity (severity of concave portions of the contour)\n",
    "- concave points (number of concave portions of the contour)\n",
    "- symmetry\n",
    "- fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "The mean, standard error, and \"worst\" or largest (mean of the three worst/largest values) of these features were computed for each image, resulting in 30 features. For instance, field 0 is Mean Radius, field 10 is Radius SE, field 20 is Worst Radius.\n",
    "\n",
    "**Classes:**\n",
    "\n",
    "- WDBC-Malignant\n",
    "- WDBC-Benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sklearn.datasets.load_breast_cancer(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b12a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X here is a pandas DataFrame.\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daed6a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is a pandas Series.\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96549a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts().plot.pie(autopct='%1.1f%%', startangle=90)\n",
    "plt.ylabel('')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa6a93",
   "metadata": {},
   "source": [
    "# Data Transformation/Normalization/Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, 5, figsize=(15, 12))\n",
    "X.hist(ax=axes, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a701d1b0",
   "metadata": {},
   "source": [
    "## 1. Normalization with MinMaxScaler\n",
    "> Normalization rescales each feature to a **fixed range**, commonly $[0, 1]$ or $[-1, 1]$.\n",
    "\n",
    "**Formula (min–max normalization):**\n",
    "$$\n",
    "x'_i = \\frac{x_i - x_{\\min}}{x_{\\max} - x_{\\min}}\n",
    "$$\n",
    "\n",
    "where  \n",
    "- $x_{\\min}$ and $x_{\\max}$ are the minimum and maximum values of the feature.\n",
    "\n",
    "**Why normalize:**\n",
    "- Keeps features within a bounded range, useful for models sensitive to scale (e.g., neural networks, k-NN).  \n",
    "- Improves convergence during gradient-based optimization.  \n",
    "- Maintains interpretability when combining heterogeneous units or feature types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a625259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalize the data column by column\n",
    "X_normalized = X.copy()\n",
    "\n",
    "# create the min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for col in X.columns:\n",
    "    # use fit-transform to normalize the data\n",
    "    X_normalized[col] = scaler.fit_transform(X[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4bee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side histograms\n",
    "fig, axes = plt.subplots(6, 10, figsize=(20, 9), dpi=300)\n",
    "\n",
    "# Original data on the left (first 5 columns)\n",
    "for i, col in enumerate(X.columns):\n",
    "    row = i // 5\n",
    "    col_idx = i % 5\n",
    "    X[col].hist(ax=axes[row, col_idx], bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[row, col_idx].set_title(f'{col}\\n(Original)', fontsize=8)\n",
    "    axes[row, col_idx].tick_params(labelsize=6)\n",
    "\n",
    "# Normalized data on the right (second 5 columns)\n",
    "for i, col in enumerate(X_normalized.columns):\n",
    "    row = i // 5\n",
    "    col_idx = (i % 5) + 5\n",
    "    X_normalized[col].hist(ax=axes[row, col_idx], bins=20, edgecolor='black', alpha=0.7, color='coral')\n",
    "    axes[row, col_idx].set_title(f'{col}\\n(Normalized)', fontsize=8)\n",
    "    axes[row, col_idx].tick_params(labelsize=6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a285034",
   "metadata": {},
   "source": [
    "## 2. Standardization\n",
    "> Standardization rescales each feature so that it has **mean = 0** and **standard deviation = 1**.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "z_i = \\frac{x_i - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "where  \n",
    "- $x_i$ is the original value,  \n",
    "- $\\mu$ is the mean of the feature,  \n",
    "- $\\sigma$ is the standard deviation.\n",
    "\n",
    "**Why standardize:**\n",
    "- Ensures all features contribute equally in distance- or gradient-based models (e.g., linear/logistic regression, SVM, PCA).  \n",
    "- Prevents large-scale variables from dominating smaller-scale ones.  \n",
    "- Less sensitive to outliers than normalization.\n",
    "- Makes model training more stable and interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe7729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the data column by column\n",
    "X_standardized = X.copy()\n",
    "\n",
    "# create the standard scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for col in X.columns:\n",
    "    # use fit-transform to standardize the data\n",
    "    X_standardized[col] = scaler.fit_transform(X[[col]])\n",
    "\n",
    "# Create side-by-side histograms\n",
    "fig, axes = plt.subplots(6, 10, figsize=(20, 9), dpi=300)\n",
    "\n",
    "# Original data on the left (first 5 columns)\n",
    "for i, col in enumerate(X.columns):\n",
    "    row = i // 5\n",
    "    col_idx = i % 5\n",
    "    X[col].hist(ax=axes[row, col_idx], bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[row, col_idx].set_title(f'{col}\\n(Original)', fontsize=8)\n",
    "    axes[row, col_idx].tick_params(labelsize=6)\n",
    "\n",
    "# Standardized data on the right (second 5 columns)\n",
    "for i, col in enumerate(X_standardized.columns):\n",
    "    row = i // 5\n",
    "    col_idx = (i % 5) + 5\n",
    "    X_standardized[col].hist(ax=axes[row, col_idx], bins=20, edgecolor='black', alpha=0.7, color='mediumseagreen')\n",
    "    axes[row, col_idx].set_title(f'{col}\\n(Standardized)', fontsize=8)\n",
    "    axes[row, col_idx].tick_params(labelsize=6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da801d02",
   "metadata": {},
   "source": [
    "## 3. Log Transformation\n",
    "> Log transformation compresses large values and expands small ones, reducing skewness in right-tailed distributions.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "x'_i = \\log(x_i + c)\n",
    "$$\n",
    "\n",
    "where\n",
    "- $x_i$ is the original value,\n",
    "- $c$ is a constant (often $c = 1$) to handle zero or negative values.\n",
    "\n",
    "**Why apply log transformation:**\n",
    "- Reduces the impact of outliers and right-skewed distributions.\n",
    "- Makes multiplicative relationships approximately additive.\n",
    "- Stabilizes variance and improves linear model assumptions.\n",
    "- Useful for highly skewed data such as income, citation counts, or population sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0533b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to the original data\n",
    "X_log = X.copy()\n",
    "for col in X_log.columns:\n",
    "    X_log[col] = np.log(X_log[col] + 1)  # Add 1 to handle zeros\n",
    "\n",
    "# Create side-by-side histograms\n",
    "fig, axes = plt.subplots(6, 10, figsize=(20, 9), dpi=300)\n",
    "\n",
    "# Original data on the left (first 5 columns)\n",
    "for i, col in enumerate(X.columns):\n",
    "    row = i // 5\n",
    "    col_idx = i % 5\n",
    "    X[col].hist(ax=axes[row, col_idx], bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[row, col_idx].set_title(f'{col}\\n(Original)', fontsize=8)\n",
    "    axes[row, col_idx].tick_params(labelsize=6)\n",
    "\n",
    "# Log-transformed data on the right (second 5 columns)\n",
    "for i, col in enumerate(X_log.columns):\n",
    "    row = i // 5\n",
    "    col_idx = (i % 5) + 5\n",
    "    X_log[col].hist(ax=axes[row, col_idx], bins=20, edgecolor='black', alpha=0.7, color='green')\n",
    "    axes[row, col_idx].set_title(f'{col}\\n(Log-transformed)', fontsize=8)\n",
    "    axes[row, col_idx].tick_params(labelsize=6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4b309a",
   "metadata": {},
   "source": [
    "## 4. Summary of Data Transformation Techniques\n",
    "| Transformation      | Typical Use Case                                   | Range / Scale     | Key Benefit                           |\n",
    "|----------------------|----------------------------------------------------|-------------------|----------------------------------------|\n",
    "| Standardization      | Features with different means/variances           | Mean 0, SD 1      | Equal weighting across features and less sensitive to outliers        |\n",
    "| Normalization        | Scale-sensitive models (e.g., NN, k-NN)           | [0, 1] or [-1, 1] | Bounded magnitude, stable gradients    |\n",
    "| Log Transformation   | Skewed, positive-valued data                      | Unbounded         | Reduces skewness, stabilizes variance  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f170fe",
   "metadata": {},
   "source": [
    "# Data Splitting with `sklearn.model_selection.train_test_split`\n",
    "- Data splitting is essential to ensure that our model’s performance reflects genuine predictive ability rather than overfitting to a particular dataset.\n",
    "- By dividing the data into training, validation, and test sets, we can train the model on one subset, tune hyperparameters on another, and finally evaluate it on unseen data.\n",
    "- This separation allows for an unbiased assessment of generalization performance and helps prevent overly optimistic estimates that might arise if the same data were used for both fitting and evaluation.\n",
    "\n",
    "In the data science pipeline, we want to split the data into three parts: the training set, the validation set, and the test set.\n",
    "\n",
    "- Training set (typically 70% of the data) – The portion of the data used to fit the model. The model learns patterns, parameters, or relationships from this subset.\n",
    "- Validation set (typically 15% of the data) – A separate subset used to tune hyperparameters and prevent overfitting. It helps select the best model configuration without touching the final test data.\n",
    "- Test set (typically 15% of the data) – The held-out portion of data used only for final evaluation. It provides an unbiased estimate of generalization performance on unseen data after all model choices are finalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b7e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_and_test, y_train, y_val_and_test = sklearn.model_selection.train_test_split(\n",
    "    X, y, test_size=0.5, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = sklearn.model_selection.train_test_split(\n",
    "    X_val_and_test, y_val_and_test, test_size=0.5, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {len(X_train)} samples, {y_train.sum()} positive ({y_train.mean():.2%})\")\n",
    "print(f\"Validation set: {X_val.shape}, {len(X_val)} samples, {y_val.sum()} positive ({y_val.mean():.2%})\")\n",
    "print(f\"Test set: {X_test.shape}, {len(X_test)} samples, {y_test.sum()} positive ({y_test.mean():.2%})\")\n",
    "\n",
    "# Visualize the class distribution in the three splits\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Training set pie chart\n",
    "n_train_neg = len(y_train) - y_train.sum()\n",
    "n_train_pos = y_train.sum()\n",
    "axes[0].pie([n_train_neg, n_train_pos],\n",
    "            labels=[f'Negative (n={n_train_neg})', f'Positive (n={n_train_pos})'],\n",
    "            autopct='%1.1f%%',\n",
    "            startangle=90,\n",
    "            colors=['#ff9999', '#66b3ff'])\n",
    "axes[0].set_title(f'Training Set\\n(n={len(y_train)})')\n",
    "\n",
    "# Validation set pie chart\n",
    "n_val_neg = len(y_val) - y_val.sum()\n",
    "n_val_pos = y_val.sum()\n",
    "axes[1].pie([n_val_neg, n_val_pos],\n",
    "            labels=[f'Negative (n={n_val_neg})', f'Positive (n={n_val_pos})'],\n",
    "            autopct='%1.1f%%',\n",
    "            startangle=90,\n",
    "            colors=['#ff9999', '#66b3ff'])\n",
    "axes[1].set_title(f'Validation Set\\n(n={len(y_val)})')\n",
    "\n",
    "# Test set pie chart\n",
    "n_test_neg = len(y_test) - y_test.sum()\n",
    "n_test_pos = y_test.sum()\n",
    "axes[2].pie([n_test_neg, n_test_pos],\n",
    "            labels=[f'Negative (n={n_test_neg})', f'Positive (n={n_test_pos})'],\n",
    "            autopct='%1.1f%%',\n",
    "            startangle=90,\n",
    "            colors=['#ff9999', '#66b3ff'])\n",
    "axes[2].set_title(f'Test Set\\n(n={len(y_test)})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a39be",
   "metadata": {},
   "source": [
    "#### Notes on `stratify`: `stratify` is used to ensure that the class distribution in the training, validation, and test sets is the same as in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f064b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_and_test, y_train, y_val_and_test = sklearn.model_selection.train_test_split(\n",
    "    X, y, test_size=0.5, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = sklearn.model_selection.train_test_split(\n",
    "    X_val_and_test, y_val_and_test, test_size=0.5, stratify=y_val_and_test, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {len(X_train)} samples, {y_train.sum()} positive ({y_train.mean():.2%})\")\n",
    "print(f\"Validation set: {X_val.shape}, {len(X_val)} samples, {y_val.sum()} positive ({y_val.mean():.2%})\")\n",
    "print(f\"Test set: {X_test.shape}, {len(X_test)} samples, {y_test.sum()} positive ({y_test.mean():.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fdf310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the class distribution in the three splits\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Training set pie chart\n",
    "n_train_neg = len(y_train) - y_train.sum()\n",
    "n_train_pos = y_train.sum()\n",
    "axes[0].pie([n_train_neg, n_train_pos],\n",
    "            labels=[f'Negative (n={n_train_neg})', f'Positive (n={n_train_pos})'],\n",
    "            autopct='%1.1f%%',\n",
    "            startangle=90,\n",
    "            colors=['#ff9999', '#66b3ff'])\n",
    "axes[0].set_title(f'Training Set\\n(n={len(y_train)})')\n",
    "\n",
    "# Validation set pie chart\n",
    "n_val_neg = len(y_val) - y_val.sum()\n",
    "n_val_pos = y_val.sum()\n",
    "axes[1].pie([n_val_neg, n_val_pos],\n",
    "            labels=[f'Negative (n={n_val_neg})', f'Positive (n={n_val_pos})'],\n",
    "            autopct='%1.1f%%',\n",
    "            startangle=90,\n",
    "            colors=['#ff9999', '#66b3ff'])\n",
    "axes[1].set_title(f'Validation Set\\n(n={len(y_val)})')\n",
    "\n",
    "# Test set pie chart\n",
    "n_test_neg = len(y_test) - y_test.sum()\n",
    "n_test_pos = y_test.sum()\n",
    "axes[2].pie([n_test_neg, n_test_pos],\n",
    "            labels=[f'Negative (n={n_test_neg})', f'Positive (n={n_test_pos})'],\n",
    "            autopct='%1.1f%%',\n",
    "            startangle=90,\n",
    "            colors=['#ff9999', '#66b3ff'])\n",
    "axes[2].set_title(f'Test Set\\n(n={len(y_test)})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88747ce4",
   "metadata": {},
   "source": [
    "## Apply the standardization transformation for the training, validation, and test sets.\n",
    "- Note that we are fitting for the training set and then applying the same transformation to the validation and test sets.\n",
    "- Avoid data leakage: Using validation/test data to compute mean/std “peeks” at their distribution, leaking information into preprocessing and inflating metrics.\n",
    "- Unbiased evaluation: Metrics should reflect how the model performs on unseen data; scaling with only training stats keeps the test truly unseen.\n",
    "- Deployment match: At inference you won’t have future data to recompute scaling; you apply the fixed scaler learned from historical (training) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42220e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the standardization transformation here for example.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# note that we are fitting for the training set and then applying the same transformation to the validation and test sets.\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Original training set mean: {X_train.mean().mean():.4f}, std: {X_train.std().mean():.4f}\")\n",
    "print(f\"Scaled training set mean: {X_train_scaled.mean():.4f}, std: {X_train_scaled.std():.4f}\")\n",
    "\n",
    "print(f\"Original validation set mean: {X_val.mean().mean():.4f}, std: {X_val.std().mean():.4f}\")\n",
    "print(f\"Scaled validation set mean: {X_val_scaled.mean():.4f}, std: {X_val_scaled.std():.4f}\")\n",
    "\n",
    "print(f\"Original test set mean: {X_test.mean().mean():.4f}, std: {X_test.std().mean():.4f}\")\n",
    "print(f\"Scaled test set mean: {X_test_scaled.mean():.4f}, std: {X_test_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d838aa8a",
   "metadata": {},
   "source": [
    "# Estimate our First Model (Logistic Regression)\n",
    "## 1. Create the `LogisticRegression` model object\n",
    "- The logistic regression model is solved using an iterative optimization algorithm instead of using a closed-form solution.\n",
    "- So we would need to specify the maximum number of iterations using `max_iter`. \n",
    "- We want to set a `max_iter` value sufficiently large (e.g., 1000 here, but depending on your actual dataset) so that the model parameters could converge within this window.\n",
    "- There is an internal \"early-stopping\" design, the model estimation will stop early if it detects convergence.\n",
    "- The `random_state` argument helps you fix the randomness in the algorithm (e.g., parameter initialization, data ordering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9bce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a logistic regression model\n",
    "model = sklearn.linear_model.LogisticRegression(max_iter=1000, random_state=RANDOM_STATE, verbose=2)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae202cd",
   "metadata": {},
   "source": [
    "Here is a list of parameters that you can tune for the `LogisticRegression` model.\n",
    "| Parameter | Description |\n",
    "|------------|--------------|\n",
    "| **penalty** | Specifies the norm used in the penalization. Options: `'l1'`, `'l2'`, `'elasticnet'`, or `'none'`. Default is `'l2'`. |\n",
    "| **dual** | Whether to use the dual or primal formulation. `True` is only supported for `'liblinear'` solver and useful when `n_samples < n_features`. |\n",
    "| **tol** | Tolerance for stopping criteria; smaller values lead to more precise convergence. |\n",
    "| **C** | Inverse of regularization strength. Smaller values mean stronger regularization. |\n",
    "| **fit_intercept** | If `True`, adds an intercept term (bias) to the decision function. |\n",
    "| **intercept_scaling** | Used only when `solver='liblinear'` and `fit_intercept=True`; scales the synthetic feature added to the intercept. |\n",
    "| **class_weight** | Assigns weights to classes to handle imbalance. Options: `None`, `'balanced'`, or a dictionary mapping classes to weights. |\n",
    "| **random_state** | Controls random number generation for reproducibility (e.g., in solver initialization or data shuffling). |\n",
    "| **solver** | Optimization algorithm to use: `'lbfgs'`, `'liblinear'`, `'saga'`, `'newton-cg'`, or `'sag'`. `'lbfgs'` works well for most problems. |\n",
    "| **max_iter** | Maximum number of iterations taken for the solvers to converge. Increase if you get convergence warnings. |\n",
    "| **multi_class** | Defines multiclass strategy: `'auto'`, `'ovr'` (one-vs-rest), or `'multinomial'`. `'auto'` chooses based on the solver. |\n",
    "| **verbose** | Controls verbosity of solver output (integer). |\n",
    "| **warm_start** | If `True`, reuse the previous solution to speed up convergence for similar data. |\n",
    "| **n_jobs** | Number of CPU cores used during computation. `None` = 1 core, `-1` = all cores. |\n",
    "| **l1_ratio** | Used only when `penalty='elasticnet'`; mixes L1 and L2 regularization (0 = L2 only, 1 = L1 only). |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daafae2",
   "metadata": {},
   "source": [
    "## 2. Call the `fit()` method on the logistic regression to estimate model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167536dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a301d",
   "metadata": {},
   "source": [
    "## 3.1 Make predictions on the validation set with `predict()`, generates the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the validation set\n",
    "y_val_pred = model.predict(X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb4cb3c",
   "metadata": {},
   "source": [
    "## 3.2 Make predictions on the validation set with `predict_proba()`, generates the probability of the positive and negative classes. It will give you predicted probabilities for all classes if multi-class classification is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are taking the second column of the output, which is the probability of the positive class (1).\n",
    "y_val_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "y_val_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98364a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predicted class labels vs predicted probabilities\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Histogram of predicted probabilities by predicted class\n",
    "axes[0].hist(y_val_pred_proba[y_val_pred == 0], bins=30, alpha=0.6, label='Predicted Class 0', color='blue')\n",
    "axes[0].hist(y_val_pred_proba[y_val_pred == 1], bins=30, alpha=0.6, label='Predicted Class 1', color='red')\n",
    "axes[0].axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold (0.5)')\n",
    "axes[0].set_xlabel('Predicted Probability')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Predicted Probabilities by Predicted Class')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Scatter plot of predicted probabilities colored by actual class\n",
    "scatter_0 = axes[1].scatter(range(len(y_val_pred_proba[y_val == 0])),\n",
    "                            y_val_pred_proba[y_val == 0],\n",
    "                            alpha=0.6, label='Actual Class 0', color='blue', s=30)\n",
    "scatter_1 = axes[1].scatter(range(len(y_val_pred_proba[y_val == 0]),\n",
    "                                  len(y_val_pred_proba[y_val == 0]) + len(y_val_pred_proba[y_val == 1])),\n",
    "                            y_val_pred_proba[y_val == 1],\n",
    "                            alpha=0.6, label='Actual Class 1', color='red', s=30)\n",
    "axes[1].axhline(y=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold (0.5)')\n",
    "axes[1].set_xlabel('Sample Index')\n",
    "axes[1].set_ylabel('Predicted Probability')\n",
    "axes[1].set_title('Predicted Probabilities by Actual Class')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07377398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do the same on the training set.\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_train_pred_proba = model.predict_proba(X_train_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d385b58a",
   "metadata": {},
   "source": [
    "## 4. Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11346e",
   "metadata": {},
   "source": [
    "### **Accuracy**\n",
    "Accuracy measures the proportion of correctly predicted instances among all instances.  \n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "It is suitable when classes are balanced and misclassification costs are equal.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Basic usage\n",
    "y_true = [0, 1, 1, 0]\n",
    "y_pred = [0, 1, 0, 0]\n",
    "print(accuracy_score(y_true, y_pred))  # 0.75\n",
    "\n",
    "# With sample weights\n",
    "weights = [1.0, 2.0, 1.0, 1.0]\n",
    "print(accuracy_score(y_true, y_pred, sample_weight=weights))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e0944",
   "metadata": {},
   "source": [
    "**API reference:** [sklearn.metrics.accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ebf633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracy on validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Calculate accuracy on training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928bdb5d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Confusion Matrix**\n",
    "A confusion matrix summarizes prediction results by comparing actual vs. predicted classes.\n",
    "\n",
    "|                | **Predicted Positive** | **Predicted Negative** |\n",
    "|----------------|------------------------|------------------------|\n",
    "| **Actual Positive** | True Positive (TP)       | False Negative (FN)      |\n",
    "| **Actual Negative** | False Positive (FP)      | True Negative (TN)       |\n",
    "\n",
    "From the confusion matrix, we can derive:\n",
    "- **Precision:** $ \\displaystyle \\frac{TP}{TP + FP} $\n",
    "- **Recall (Sensitivity):** $ \\displaystyle \\frac{TP}{TP + FN} $\n",
    "- **Specificity:** $ \\displaystyle \\frac{TN}{TN + FP} $\n",
    "- **Accuracy:** $ \\displaystyle \\frac{TP + TN}{TP + TN + FP + FN} $\n",
    "\n",
    "Examples:\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = [0, 0, 1, 1]\n",
    "y_pred = [0, 1, 0, 1]\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Explicit label order\n",
    "print(confusion_matrix(y_true, y_pred, labels=[1, 0]))\n",
    "\n",
    "# Normalized by true labels (rows)\n",
    "print(confusion_matrix(y_true, y_pred, normalize='true'))\n",
    "```\n",
    "\n",
    "**API reference:** [sklearn.metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcec41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute confusion matrix for validation set\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "print(\"Confusion Matrix (Validation Set):\")\n",
    "print(cm)\n",
    "\n",
    "# Extract metrics from confusion matrix\n",
    "tn = cm[0, 0]\n",
    "fp = cm[0, 1]\n",
    "fn = cm[1, 0]\n",
    "tp = cm[1, 1]\n",
    "print(f\"\\nTrue Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "\n",
    "# Calculate derived metrics\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "print(f\"\\nPrecision: {precision:.4f}\")\n",
    "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a448c91d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **F1 Score**\n",
    "The F1 score is the harmonic mean of precision and recall, balancing both metrics.  \n",
    "It is particularly useful when dealing with imbalanced datasets.\n",
    "\n",
    "$$\n",
    "\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{2TP}{2TP + FP + FN}\n",
    "$$\n",
    "\n",
    "- **Range:** $0$ (worst) → $1$ (best)\n",
    "- **Interpretation:** High F1 indicates both low false positives and false negatives.\n",
    "\n",
    "\n",
    "Examples:\n",
    "```python\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Binary F1\n",
    "y_true = [0, 0, 1, 1]\n",
    "y_pred = [0, 1, 1, 1]\n",
    "print(f1_score(y_true, y_pred))  # default 'binary'\n",
    "\n",
    "# Multiclass F1 with averaging\n",
    "y_true = [0, 1, 2, 2]\n",
    "y_pred = [0, 2, 1, 2]\n",
    "print(f1_score(y_true, y_pred, average='macro'))\n",
    "print(f1_score(y_true, y_pred, average='weighted'))\n",
    "```\n",
    "\n",
    "**API reference:** [sklearn.metrics.f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea6414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_val, y_val_pred)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Manual calculation to verify\n",
    "f1_manual = 2 * (precision * recall) / (precision + recall)\n",
    "print(f\"F1 Score (manual): {f1_manual:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7618c35",
   "metadata": {},
   "source": [
    "## Probability ($\\hat{p}$-Based) Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d951df5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Cross-Entropy Loss**\n",
    "Cross-entropy loss (or log loss) quantifies the difference between the true labels $y$ and the predicted probabilities $\\hat{y}$.  \n",
    "\n",
    "For **binary classification**:\n",
    "$$\n",
    "L = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "For **multi-class classification**:\n",
    "$$\n",
    "L = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})\n",
    "$$\n",
    "\n",
    "Examples:\n",
    "```python\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Binary: y_true (0/1) and predicted probabilities for class 1\n",
    "y_true = [0, 1, 1, 0]\n",
    "y_prob = [0.1, 0.9, 0.4, 0.2]\n",
    "print(log_loss(y_true, y_prob))\n",
    "\n",
    "# Multiclass: probability matrix with columns per class\n",
    "y_true = [0, 2, 1]\n",
    "y_proba = [[0.7, 0.2, 0.1],\n",
    "           [0.1, 0.2, 0.7],\n",
    "           [0.2, 0.7, 0.1]]\n",
    "print(log_loss(y_true, y_proba, labels=[0, 1, 2]))\n",
    "```\n",
    "\n",
    "**API reference:** [sklearn.metrics.log_loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Calculate cross-entropy loss\n",
    "ce_loss = log_loss(y_val, y_val_pred_proba)\n",
    "print(f\"Cross-Entropy Loss: {ce_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac245aa1",
   "metadata": {},
   "source": [
    "\n",
    "It penalizes confident wrong predictions heavily and is widely used in neural network training.\n",
    "\n",
    "---\n",
    "\n",
    "### **AUC (Area Under the ROC Curve)**\n",
    "AUC measures the ability of a model to rank positive samples higher than negative ones.  \n",
    "It corresponds to the area under the ROC curve (True Positive Rate vs. False Positive Rate).\n",
    "\n",
    "- **Interpretation:** Probability that a randomly chosen positive instance is ranked above a randomly chosen negative instance.\n",
    "- **Range:** $0.5$ (random guessing) → $1.0$ (perfect classifier)\n",
    "\n",
    "**API references:**\n",
    "- [sklearn.metrics.roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    "- [sklearn.metrics.roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3840bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate AUC\n",
    "auc_score = roc_auc_score(y_val, y_val_pred_proba)\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_val_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.4f})', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.5)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a8145",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **(Bonus) Calibration**\n",
    "Calibration evaluates how well predicted probabilities align with actual outcome frequencies.  \n",
    "A well-calibrated model’s output probabilities reflect true likelihoods (e.g., predictions of $\\hat{p}=0.8$ are correct about 80% of the time).\n",
    "\n",
    "Common tools:\n",
    "- **Reliability Diagram:** Plots predicted probabilities vs. empirical accuracies.\n",
    "- **Expected Calibration Error (ECE):**\n",
    "$$\n",
    "\\text{ECE} = \\sum_{m=1}^{M} \\frac{n_m}{N} \\left| \\text{acc}(B_m) - \\text{conf}(B_m) \\right|\n",
    "$$\n",
    "where $B_m$ is the $m$-th bin, $\\text{acc}(B_m)$ is the accuracy within that bin, and $\\text{conf}(B_m)$ is the average predicted confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fbe088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "import numpy as np\n",
    "\n",
    "# Compute calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_val, y_val_pred_proba, n_bins=10, strategy='uniform')\n",
    "\n",
    "# Plot calibration curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(prob_pred, prob_true, marker='o', linewidth=2, label='Logistic Regression')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Calibration Curve (Reliability Diagram)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate Expected Calibration Error (ECE)\n",
    "def expected_calibration_error(y_true, y_pred_proba, n_bins=10):\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_pred_proba, n_bins=n_bins, strategy='uniform')\n",
    "    bin_counts = np.histogram(y_pred_proba, bins=n_bins, range=(0, 1))[0]\n",
    "    bin_counts = bin_counts[bin_counts > 0]  # Only consider non-empty bins\n",
    "\n",
    "    # Align prob_true and prob_pred with non-empty bins\n",
    "    if len(prob_true) != len(bin_counts):\n",
    "        # Recalculate to ensure alignment\n",
    "        bins = np.linspace(0, 1, n_bins + 1)\n",
    "        bin_indices = np.digitize(y_pred_proba, bins) - 1\n",
    "        bin_indices = np.clip(bin_indices, 0, n_bins - 1)\n",
    "\n",
    "        ece = 0.0\n",
    "        for i in range(n_bins):\n",
    "            mask = bin_indices == i\n",
    "            if mask.sum() > 0:\n",
    "                bin_acc = y_true[mask].mean()\n",
    "                bin_conf = y_pred_proba[mask].mean()\n",
    "                ece += (mask.sum() / len(y_true)) * np.abs(bin_acc - bin_conf)\n",
    "        return ece\n",
    "    else:\n",
    "        ece = np.sum(bin_counts / len(y_true) * np.abs(prob_true - prob_pred))\n",
    "        return ece\n",
    "\n",
    "ece = expected_calibration_error(y_val, y_val_pred_proba, n_bins=10)\n",
    "print(f\"Expected Calibration Error (ECE): {ece:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04da1dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "accuracy = sklearn.metrics.accuracy_score(y_val, y_val_pred)\n",
    "roc_auc = sklearn.metrics.roc_auc_score(y_val, y_val_pred_proba)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Validation ROC AUC: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ea72d3",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with Cross-Validation\n",
    "\n",
    "![Cross-Validation Illustration](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
    "\n",
    "\n",
    "- Goal: select model settings that generalize beyond the training set, not just fit it better.\n",
    "- Use cross-validation on the training data only; never peek at the test set.\n",
    "- Keep preprocessing inside a `Pipeline` to avoid leakage during CV.\n",
    "- Choose a scoring function aligned to the objective (e.g., `roc_auc` or `log_loss` for probabilistic classifiers; `accuracy` when classes are balanced).\n",
    "- Search strategies:\n",
    "  - `GridSearchCV`: exhaustive over small, well-chosen grids.\n",
    "  - `RandomizedSearchCV`: sample large spaces efficiently; good early-stage.\n",
    "- Practical tips: set `refit=True` to retrain the best config on the full training split; inspect `cv_results_` for variance across folds; fix `random_state` for reproducibility; constrain search space to your compute budget."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e2c2d6",
   "metadata": {},
   "source": [
    "Put together the training and validation sets because we are now using a dynamic train-validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef0bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{X_train_scaled.shape=:}, {y_train.shape=:}, {X_val_scaled.shape=:}, {y_val.shape=:}\")\n",
    "X_train_val_scaled = np.concatenate([X_train_scaled, X_val_scaled], axis=0)\n",
    "y_train_val = np.concatenate([y_train, y_val], axis=0)\n",
    "print(f\"{X_train_val_scaled.shape=:}, {y_train_val.shape=:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ccd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Define the model.\n",
    "model = sklearn.linear_model.LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "\n",
    "# Define hyperparameter grid, we will search over these parameters over a specific range.\n",
    "param_grid = {\n",
    "    \"C\": [0.001, 0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0, 500.0],\n",
    "    # \"penalty\": [\"l1\", \"l2\"],\n",
    "    # \"solver\": [\"lbfgs\", \"liblinear\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756fc600",
   "metadata": {},
   "source": [
    "---\n",
    "### Pseudocode: Grid Search with K-fold Cross-Validation\n",
    "\n",
    "```python\n",
    "inputs:\n",
    "  base_model\n",
    "  param_grid              # dict of hyperparameter lists\n",
    "  X_train, y_train\n",
    "  K                       # number of CV folds\n",
    "  scoring_fn              # e.g., roc_auc, accuracy, neg_rmse\n",
    "\n",
    "best_score := -inf\n",
    "best_params := None\n",
    "\n",
    "for params in cartesian_product(param_grid):\n",
    "  fold_scores := []\n",
    "  for (train_idx, val_idx) in KFold(n_splits=K, shuffle=True, random_state=42):\n",
    "    X_tr := X_train[train_idx]; y_tr := y_train[train_idx]\n",
    "    X_val := X_train[val_idx];  y_val := y_train[val_idx]\n",
    "\n",
    "    # Build full pipeline to avoid leakage\n",
    "    model := clone(base_model).set_params(**params)\n",
    "\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    # Use probabilities if the metric expects them; else labels\n",
    "    y_pred := if scoring_fn expects probabilities then model.predict_proba(X_val)[:, 1]\n",
    "              else model.predict(X_val)\n",
    "\n",
    "    score := scoring_fn(y_val, y_pred)\n",
    "    append(fold_scores, score)\n",
    "\n",
    "  mean_score := mean(fold_scores)\n",
    "  if mean_score > best_score:\n",
    "    best_score := mean_score\n",
    "    best_params := params\n",
    "\n",
    "# Refit best configuration on all training data\n",
    "best_model := Pipeline([(\"scaler\", StandardScaler()),\n",
    "                        (\"model\", clone(base_model).set_params(**best_params))])\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "return best_model, best_params, best_score\n",
    "```\n",
    "\n",
    "Notes:\n",
    "- Keep preprocessing inside the CV pipeline; never touch the test set during tuning.\n",
    "- Use `refit=True` to automatically retrain the best config on the full training split.\n",
    "- Inspect `cv_results_` to understand variance across folds and confidence in the selected params.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    model,  # the model we want to tune.\n",
    "    param_grid=param_grid,  # the parameters we want to search over.\n",
    "    cv=5,  # the number of folds in cross-validation (5-fold CV in the visualization above.)\n",
    "    scoring=\"roc_auc\",  # the scoring function, what model is a good model?\n",
    "    n_jobs=-1,  # the number of CPU cores to use, set to -1 to use all cores in parallel.\n",
    "    verbose=1,  # the verbosity level.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30e33ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search on scaled training data\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79378b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize grid search results\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Extract C values for plotting\n",
    "results_df['param_C'] = results_df['param_C']\n",
    "\n",
    "# Plot: Line plot showing performance across C values\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "ax.plot(results_df['param_C'], results_df['mean_test_score'],\n",
    "        marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Regularization Parameter (C)', fontsize=12)\n",
    "ax.set_ylabel('Mean ROC AUC (CV)', fontsize=12)\n",
    "ax.set_title('Hyperparameter Tuning: C vs ROC AUC', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9550cf15",
   "metadata": {},
   "source": [
    "## Alternative Method: RandomizedSearchCV\n",
    "This is particularly useful when the parameter space is large and you only have a limited compute budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c4ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "# Define a broader parameter distribution for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'C': loguniform(1e-3, 1e3),  # Sample C from a log-uniform distribution\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    sklearn.linear_model.LogisticRegression(max_iter=1000, random_state=42),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,  # Number of parameter settings sampled\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the random search\n",
    "print(\"Starting randomized hyperparameter search...\")\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nBest parameters from RandomizedSearchCV: {random_search.best_params_}\")\n",
    "print(f\"Best cross-validation ROC AUC: {random_search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292b20c",
   "metadata": {},
   "source": [
    "# Finally, examine the best model and its performance on the test set with `classification_report`\n",
    "\n",
    "The `classification_report` function provides a comprehensive summary of classification performance metrics including precision, recall, F1-score, and support for each class. It's particularly useful for understanding model performance across different classes and identifying potential issues like class imbalance or poor performance on specific categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ef8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe src=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\" width=\"100%\" height=\"500\"></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c031f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classification_report if not already imported\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Extract best model and parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(f\"Best cross-validation ROC AUC: {best_score:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = best_model.predict(X_test_scaled)\n",
    "y_proba_test = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "test_roc_auc = roc_auc_score(y_test, y_proba_test)\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"ROC AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e173d23",
   "metadata": {},
   "source": [
    "# A Regression Example on the California Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X_housing = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "y_housing = housing.target\n",
    "\n",
    "print(\"California Housing Dataset\")\n",
    "print(f\"Number of samples: {X_housing.shape[0]}\")\n",
    "print(f\"Number of features: {X_housing.shape[1]}\")\n",
    "print(f\"\\nFeatures: {list(X_housing.columns)}\")\n",
    "print(f\"\\nTarget: Median house value (in $100,000s)\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(X_housing.head())\n",
    "print(\"\\nTarget statistics:\")\n",
    "print(f\"Mean: ${y_housing.mean():.2f} (x100k)\")\n",
    "print(f\"Std: ${y_housing.std():.2f} (x100k)\")\n",
    "print(f\"Min: ${y_housing.min():.2f} (x100k)\")\n",
    "print(f\"Max: ${y_housing.max():.2f} (x100k)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c753ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/validation/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=RANDOM_STATE\n",
    ")  # 0.25 * 0.8 = 0.2 validation\n",
    "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc924538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baselines: LinearRegression and untuned RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pipe_lin = sklearn.pipeline.Pipeline([\n",
    "    (\"scaler\", sklearn.preprocessing.StandardScaler()),\n",
    "    (\"model\", sklearn.linear_model.LinearRegression()),\n",
    "])\n",
    "pipe_lin.fit(X_train, y_train)\n",
    "\n",
    "rf_base = RandomForestRegressor(random_state=RANDOM_STATE)\n",
    "rf_base.fit(X_train, y_train)\n",
    "\n",
    "for name, model in {\"Linear\": pipe_lin, \"RF_base\": rf_base}.items():\n",
    "    ypred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, ypred))\n",
    "    mae = mean_absolute_error(y_val, ypred)\n",
    "    r2 = r2_score(y_val, ypred)\n",
    "    print(f\"{name} | RMSE={rmse:.3f} | MAE={mae:.3f} | R2={r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348da46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized search on RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rf = RandomForestRegressor(random_state=RANDOM_STATE)\n",
    "param_dist_rf = {\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500],\n",
    "    \"max_depth\": [None, 10, 20, 30, 40],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_gs = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=1,  # adjust the number of parameter settings to sample depending on the compute budget.\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    refit=True,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "print(\"Starting RF randomized search ...\")\n",
    "rf_gs.fit(X_train, y_train)\n",
    "print(\"Best params:\", rf_gs.best_params_)\n",
    "print(\"Best CV RMSE:\", -rf_gs.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned RF on validation and test sets\n",
    "best_rf_model = rf_gs.best_estimator_\n",
    "\n",
    "# Validation\n",
    "y_val_pred_best = best_rf_model.predict(X_val)\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred_best))\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred_best)\n",
    "r2_val = r2_score(y_val, y_val_pred_best)\n",
    "print(f\"Validation | RMSE={rmse_val:.3f} | MAE={mae_val:.3f} | R2={r2_val:.3f}\")\n",
    "\n",
    "# Test\n",
    "y_test_pred_best = best_rf_model.predict(X_test)\n",
    "rmse_te = np.sqrt(mean_squared_error(y_test, y_test_pred_best))\n",
    "mae_te = mean_absolute_error(y_test, y_test_pred_best)\n",
    "r2_te = r2_score(y_test, y_test_pred_best)\n",
    "print(f\"Test | RMSE={rmse_te:.3f} | MAE={mae_te:.3f} | R2={r2_te:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed2d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance plot\n",
    "importances = best_rf_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = np.array(X_housing.columns)[indices]\n",
    "vals = importances[indices]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(features[::-1], vals[::-1])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"RandomForestRegressor Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3fad06",
   "metadata": {},
   "source": [
    "# Don't forget the attandance code!\n",
    "\n",
    "# <img src=\"images/CME193_Lecture_6_Attendance_Form.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b6451d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cme193-autumn-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
