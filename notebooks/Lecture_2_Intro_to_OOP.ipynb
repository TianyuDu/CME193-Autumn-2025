{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from typing import Protocol, runtime_checkable, Iterable, Optional, Union, List, Tuple\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Presenter notes — Inheritance & Polymorphism\n",
        "- **Interface**: `BaseRegressor` communicates expected behavior (`fit`, `predict`).\n",
        "- **Substitutability**: Adapters show how different implementations can share an API.\n",
        "- **Why adapters?**: Our existing classes differ slightly from the uniform interface; adapters are a lightweight bridge.\n",
        "- **Client code**: `evaluate_model` treats any `BaseRegressor` the same; implementation details hidden.\n",
        "- **Takeaway**: OOP enables architecture where components can be swapped with minimal friction.\n",
        "@runtime_checkable\n",
        "class BaseRegressor(Protocol):\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) -> \"BaseRegressor\":\n",
        "        ...\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        ...\n",
        "\n",
        "# Adapters: Provide a uniform interface for our existing classes\n",
        "class UnivariateOLSAdapter:\n",
        "    def __init__(self, fit_intercept: bool = True):\n",
        "        self._impl = UnivariateOLS(fit_intercept=fit_intercept)\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) -> \"UnivariateOLSAdapter\":\n",
        "        self._impl.fit(X, y)\n",
        "        return self\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        return np.asarray(self._impl.predict(X))\n",
        "\n",
        "class UnivariateOnlineOLSAdapter:\n",
        "    def __init__(self, fit_intercept: bool = True):\n",
        "        self._impl = UnivariateOnlineOLS(fit_intercept=fit_intercept)\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) -> \"UnivariateOnlineOLSAdapter\":\n",
        "        for x_i, y_i in zip(np.asarray(X).reshape(-1), np.asarray(y).reshape(-1)):\n",
        "            self._impl += (float(x_i), float(y_i))\n",
        "        return self\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        return np.asarray(self._impl.predict(X))\n",
        "\n",
        "# Client code that is agnostic to the specific implementation\n",
        "def evaluate_model(model: BaseRegressor, X: np.ndarray, y: np.ndarray) -> float:\n",
        "    model.fit(X, y)\n",
        "    yhat = model.predict(X)\n",
        "    u = y - yhat\n",
        "    v = y - y.mean()\n",
        "    den = float(np.dot(v, v))\n",
        "    return 1.0 - float(np.dot(u, u)) / den if den != 0.0 else 0.0\n",
        "\n",
        "# Demo\n",
        "X_demo = X  # from earlier synthetic data cell\n",
        "y_demo = y\n",
        "\n",
        "batch_model = UnivariateOLSAdapter(True)\n",
        "online_model = UnivariateOnlineOLSAdapter(True)\n",
        "\n",
        "print(\"R^2 (batch adapter):\", round(evaluate_model(batch_model, X_demo, y_demo), 4))\n",
        "print(\"R^2 (online adapter):\", round(evaluate_model(online_model, X_demo, y_demo), 4))## Stretch — Inheritance and Polymorphism (Extended)\n",
        "\n",
        "- **Goal**: Show how a common interface enables interchangeable implementations.\n",
        "- **Approach**: Define a minimal `BaseRegressor` with `fit` and `predict`.\n",
        "- **Children**: `UnivariateOLS` (batch) and `UnivariateOnlineOLS` (streaming) implement the same API.\n",
        "- **Polymorphism**: Write helper functions that work with any `BaseRegressor`.\n",
        "\n",
        "Presenter note: Emphasize API shape over implementation details; show that client code doesn’t care whether it’s batch or streaming.### Presenter notes — Optional: Online OLS and operator overloading\n",
        "- **Motivation**: Process data streams or large datasets incrementally; avoid storing all points.\n",
        "- **State**: Maintain running sums (n, Σx, Σy, Σx², Σxy); compute parameters lazily when needed.\n",
        "- **Design choices**:\n",
        "  - `add(x, y)` mutates internal state; invalidates cached parameters.\n",
        "  - `__iadd__` enables `m += (x, y)`; `__add__` returns a new object (non-mutating).\n",
        "  - Properties `slope`/`intercept` trigger computation only when requested.\n",
        "- **Teaching tip**: Ingest a few points interactively; compare with batch OLS on the same data.\n",
        "- **OOP angle**: Operator overloading demonstrates polymorphism with built-in syntax; shows careful state invalidation/caching.\n",
        "### Presenter notes — Block 5: Dunder methods `__str__`, `__repr__`\n",
        "- **`__repr__`**: Unambiguous representation for debugging; include config and learned state.\n",
        "- **`__str__`**: Friendly printing; switch message depending on fitted vs not fitted.\n",
        "- **Teaching tip**: Print before and after fitting to emphasize object lifecycle.\n",
        "- **OOP angle**: Integrates with Python tooling (print, REPL, logging).### Presenter notes — Block 4: `score`\n",
        "- **Metric**: R² = 1 − SS_res / SS_tot (with intercept), else denominator uses ||y||².\n",
        "- **Behavior**: Calls `predict`, computes residuals, then R²; handles zero denominator by returning 0.\n",
        "- **Teaching tip**: Show clean vs noisy data R² to calibrate expectations.\n",
        "- **OOP angle**: Demonstrates method composition and reuse (`predict` within `score`).### Presenter notes — Block 3: `predict`\n",
        "- **Purpose**: Use learned parameters to generate outputs for new inputs.\n",
        "- **Precondition**: Require model to be fit; otherwise raise informative error.\n",
        "- **Design**: Return scalars or arrays; rely on NumPy broadcasting for ergonomics.\n",
        "- **Teaching tip**: Compare predictions for intercept vs no-intercept models.\n",
        "- **OOP angle**: Reads `self` state; no mutation if purely predictive.### Presenter notes — Block 2: `fit`\n",
        "- **Purpose**: Compute parameters (a, b) from data; update object state.\n",
        "- **Key lines**:\n",
        "  - Input sanitation: ensure 1-D arrays, equal lengths; guard against shape mismatch.\n",
        "  - Closed form: with intercept uses `n, Σx, Σy, Σx², Σxy`; without intercept uses `Sxy / Sxx`.\n",
        "  - Edge cases: denominator zero when all x identical (with intercept) or all x zero (no intercept).\n",
        "- **Teaching tip**: Print `(m.slope_, m.intercept_)` on a perfect line to build intuition.\n",
        "- **OOP angle**: Method modifies `self` (learned state persists across method calls).### Presenter notes — Block 1: Class and `__init__`\n",
        "- **Encapsulation**: Group configuration (`fit_intercept`) and learned parameters (`intercept_`, `slope_`).\n",
        "- **Constructor**: Convert to `bool` defensively; initialize learned params to `None`.\n",
        "- **Teaching tip**: Show `model.fit_intercept` and discuss object state vs local variables.\n",
        "- **Gotcha**: Distinguish configuration (unchanging after construction) vs learned state (mutates after `fit`).### Presenter notes — Synthetic data + OLS plot\n",
        "- **Purpose**: Make OLS tangible; show the goal visually before classes/methods.\n",
        "- **Key callouts**:\n",
        "  - Data: `X ~ Uniform[-3, 3]`, noise `ε ~ N(0, 0.8)`; true line `y = 2.5 + 1.8 x + ε`.\n",
        "  - Closed-form solution uses sums Σx, Σy, Σx², Σxy; ensures we connect math → code.\n",
        "  - The red line is the fitted model `y ≈ â + b̂ x`.\n",
        "- **Transitions**: “We’ll now encapsulate this logic in a class to illustrate OOP.”# Synthetic data and initial OLS fit (closed-form)\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "\n",
        "rng = np.random.default_rng(193)\n",
        "num_points = 80\n",
        "X = rng.uniform(-3.0, 3.0, size=num_points)\n",
        "true_intercept = 2.5\n",
        "true_slope = 1.8\n",
        "noise = rng.normal(0.0, 0.8, size=num_points)\n",
        "y = true_intercept + true_slope * X + noise\n",
        "\n",
        "# Closed-form OLS with intercept\n",
        "Sx = X.sum()\n",
        "Sy = y.sum()\n",
        "Sxx = float(np.dot(X, X))\n",
        "Sxy = float(np.dot(X, y))\n",
        "den = num_points * Sxx - Sx * Sx\n",
        "b_hat = (num_points * Sxy - Sx * Sy) / den\n",
        "a_hat = (Sy - b_hat * Sx) / num_points\n",
        "\n",
        "x_line = np.linspace(X.min() - 0.5, X.max() + 0.5, 100)\n",
        "y_line = a_hat + b_hat * x_line\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(X, y, s=18, alpha=0.7, label=\"data\")\n",
        "plt.plot(x_line, y_line, color=\"crimson\", linewidth=2.0,\n",
        "         label=f\"OLS fit: y ≈ {a_hat:.2f} + {b_hat:.2f} x\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Synthetic data and OLS fit (closed-form)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()## OLS vs Online OLS — Problem Outline\n",
        "\n",
        "- Ordinary Least Squares (batch OLS):\n",
        "  - **Goal**: Fit y ≈ a + b x by minimizing sum of squared residuals.\n",
        "  - **Inputs**: Full dataset available in memory.\n",
        "  - **Output**: Closed-form parameters a (intercept), b (slope).\n",
        "- Online (Streaming) OLS:\n",
        "  - **Goal**: Update estimates as (x, y) arrive sequentially, without storing all data.\n",
        "  - **State**: Maintain running statistics (n, Σx, Σy, Σx², Σxy) and compute a, b on demand.\n",
        "  - **Benefit**: Works with data streams and memory-constrained settings.\n",
        "\n",
        "Presenter note: Contrast batch vs streaming. Frame Online OLS as the same objective, different computation/constraints.# CME 193 — 50‑Minute Intro to OOP in Python (via Univariate OLS)\n",
        "\n",
        "This lecture targets `https://web.stanford.edu/class/cme193/index.html` and is designed for slide-style delivery with concise presenter notes.\n",
        "\n",
        "## Agenda (≈50 minutes)\n",
        "- What is Univariate OLS? Primer and goals\n",
        "- OLS vs Online (Streaming) OLS overview\n",
        "- Block 1: Defining a class and `__init__` (encapsulation)\n",
        "- Block 2: Instance methods (`fit`)\n",
        "- Block 3: Using internal state (`predict`)\n",
        "- Block 4: Quality metric (`score`), docstrings, type hints\n",
        "- Block 5: Magic/dunder methods: `__str__` / `__repr__`\n",
        "- Optional: Streaming OLS + operator overloading (`__iadd__`, `__add__`)\n",
        "- Stretch: Inheritance and polymorphism\n",
        "\n",
        "Presenter note: Emphasize OOP principles through a tangible example (OLS). Keep math light; focus on how object state and methods interact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# CME 193: Introduction to Scientific Python\n",
        "## Week 2: Object-Oriented Programming (Oct. 1, 2025)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agenda\n",
        "\n",
        "In this mini‑session, you’ll learn core OOP ideas in Python by building a tiny linear regression model and progressively adding features.\n",
        "\n",
        "- What is Univariate Ordinary Least Squares (OLS)? Quick statistical primer.\n",
        "\n",
        "- Defining a class and `__init__` (encapsulation).\n",
        "\n",
        "- Instance methods (`fit`).\n",
        "\n",
        "- Using internal state (`predict`).\n",
        "\n",
        "- Quality metric (`score`), docstrings, type hints.\n",
        "\n",
        "- Magic/dunder methods: `__str__` / `__repr__`\n",
        "\n",
        "- Optional: Streaming OLS + operator overloading (`__iadd__`, `__add__`)\n",
        "\n",
        "- Stretch: Inheritance sketch (if time permits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Primer: What is Univariate Ordinary Least Squares (OLS)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simulated Data Generating Process\n",
        "\n",
        "We'll create synthetic data following a simple linear relationship with noise:\n",
        "\n",
        "$$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n",
        "\n",
        "Where:\n",
        "- $x$ = predictor variable, uniformly spaced from 0 to 10 (50 points)\n",
        "- $\\beta_0$ = true intercept (10.0)\n",
        "- $\\beta_1$ = true slope (2.5)\n",
        "- $\\epsilon$ ~ $\\mathcal{N}$(0, 1.5) = random noise\n",
        "\n",
        "Our goal is to estimate $\\beta_0$ and $\\beta_1$ from the observed data points using OLS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fix the random seed for reproducibility.\n",
        "np.random.seed(42)\n",
        "# Number of N points.\n",
        "n_points = 50\n",
        "# Generate x values from 0 to 10.\n",
        "x = np.linspace(0, 10, n_points)\n",
        "# Define the true slope and intercept.\n",
        "true_slope = 2.5\n",
        "true_intercept = 10.0\n",
        "# Generate noise.\n",
        "noise = np.random.normal(0, 1.5, n_points)\n",
        "# Generate y values.\n",
        "y = true_intercept + true_slope * x + noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Professional visualization of synthetic data\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6), dpi=300)\n",
        "\n",
        "# Plot 1: Raw data points with enhanced styling\n",
        "axes[0].scatter(x, y, alpha=0.8, color='#2E86AB', s=60, edgecolors='white', linewidth=0.5)\n",
        "axes[0].set_xlabel('Predictor Variable (x)', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Response Variable (y)', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Synthetic Dataset\\n(n=50 observations)', fontsize=14, fontweight='bold', pad=20)\n",
        "axes[0].grid(True, alpha=0.4, linestyle='-', linewidth=0.5)\n",
        "axes[0].tick_params(axis='both', which='major', labelsize=10)\n",
        "\n",
        "# Plot 2: Data with true relationship line\n",
        "axes[1].scatter(x, y, alpha=0.8, color='#2E86AB', s=60, edgecolors='white',\n",
        "                linewidth=0.5, label='Observed Data', zorder=3)\n",
        "y_true = true_intercept + true_slope * x\n",
        "axes[1].plot(x, y_true, color='#A23B72', linewidth=3, linestyle='--',\n",
        "             label=f'True Model: y = {true_intercept:.1f} + {true_slope:.1f}x', zorder=2)\n",
        "axes[1].set_xlabel('Predictor Variable (x)', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('Response Variable (y)', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title('Data with Underlying Linear Relationship', fontsize=14, fontweight='bold', pad=20)\n",
        "axes[1].legend(loc='upper left', frameon=True, fancybox=True, shadow=True, fontsize=11)\n",
        "axes[1].grid(True, alpha=0.4, linestyle='-', linewidth=0.5)\n",
        "axes[1].tick_params(axis='both', which='major', labelsize=10)\n",
        "\n",
        "# Enhance overall figure appearance\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.subplots_adjust(top=0.85)\n",
        "fig.suptitle('Univariate Linear Regression: Data Generation Process',\n",
        "             fontsize=16, fontweight='bold', y=0.98)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Professional data summary table\n",
        "print(\"=\" * 60)\n",
        "print(\"DATA GENERATION SUMMARY\".center(60))\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Parameter':<25} {'Value':<15} {'Description'}\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Sample Size (n)':<25} {n_points:<15} {'Number of observations'}\")\n",
        "print(f\"{'X Range':<25} {f'[{x.min():.1f}, {x.max():.1f}]':<15} {'Predictor variable range'}\")\n",
        "print(f\"{'Y Range':<25} {f'[{y.min():.1f}, {y.max():.1f}]':<15} {'Response variable range'}\")\n",
        "print(f\"{'True Intercept (β₀)':<25} {true_intercept:<15} {'Population parameter'}\")\n",
        "print(f\"{'True Slope (β₁)':<25} {true_slope:<15} {'Population parameter'}\")\n",
        "print(f\"{'Noise Std Dev (σ)':<25} {f'{noise.std():.2f}':<15} {'Error term variability'}\")\n",
        "print(f\"{'R² (theoretical)':<25} {f'{1 - (noise.var() / y.var()):.3f}':<15} {'Expected explained variance'}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ordinary Least Squares: Mathematical Foundation\n",
        "\n",
        "### **Linear Model Assumption:**\n",
        "- We assume a linear relationship between predictor $x$ and response $y$: \n",
        "  $$y = \\beta_0 + \\beta_1 x + \\varepsilon$$\n",
        "  where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ represents random error.\n",
        "\n",
        "### **Optimization Objective:**\n",
        "- Ordinary Least Squares (OLS) estimates parameters $(\\beta_0, \\beta_1)$ by minimizing the sum of squared residuals:\n",
        "  $$\\min_{\\beta_0, \\beta_1} \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2$$\n",
        "\n",
        "### **Analytical Solution:**\n",
        "- Define convenient notation for sums:\n",
        "  - $S_x = \\sum_{i=1}^{n} x_i$\n",
        "  - $S_y = \\sum_{i=1}^{n} y_i$\n",
        "  - $S_{xy} = \\sum_{i=1}^{n} x_i y_i$\n",
        "  - $S_{xx} = \\sum_{i=1}^{n} x_i^2$\n",
        "\n",
        "- The closed-form OLS estimators are:\n",
        "  $$\\hat{\\beta_1} = \\frac{n S_{xy} - S_x S_y}{n S_{xx} - S_x^2}$$\n",
        "  \n",
        "  $$\\hat{\\beta_0} = \\frac{S_y - \\hat{\\beta_1} S_x}{n}$$\n",
        "\n",
        "- For regression without intercept (through origin, useful when we have a lot of variables):\n",
        "  $$\\hat{\\beta_1}^{\\text{(no-intercept)}} = \\frac{S_{xy}}{S_{xx}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Code this up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute OLS coefficients manually (for illustration)\n",
        "n = len(x)\n",
        "Sxy = np.sum(x * y)\n",
        "Sxx = np.sum(x**2)\n",
        "Sx = np.sum(x)\n",
        "Sy = np.sum(y)\n",
        "slope = (n * Sxy - Sx * Sy) / (n * Sxx - Sx**2)\n",
        "intercept = (Sy - slope * Sx) / n\n",
        "\n",
        "slope_without_intercept = Sxy / Sxx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate fitted line for visualization\n",
        "x_line = np.linspace(x.min(), x.max(), 100)\n",
        "y_line = intercept + slope * x_line\n",
        "y_line_no_intercept = slope_without_intercept * x_line\n",
        "\n",
        "# Create comprehensive visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(x, y, alpha=0.7, color='steelblue', s=50, label='Observed data')\n",
        "plt.plot(x_line, y_line, 'r-', linewidth=2.5,\n",
        "         label=f'OLS fit: y = {intercept:.3f} + {slope:.3f}x')\n",
        "\n",
        "# Add true regression line for comparison\n",
        "y_true_line = true_intercept + true_slope * x_line\n",
        "plt.plot(x_line, y_true_line, 'g--', linewidth=2, alpha=0.8,\n",
        "         label=f'True model: y = {true_intercept:.3f} + {true_slope:.3f}x')\n",
        "\n",
        "# Add slope without intercept line\n",
        "plt.plot(x_line, y_line_no_intercept, 'm:', linewidth=2,\n",
        "         label=f'No intercept: y = {slope_without_intercept:.3f}x')\n",
        "\n",
        "plt.xlabel('Predictor Variable (x)', fontsize=12)\n",
        "plt.ylabel('Response Variable (y)', fontsize=12)\n",
        "plt.title('Univariate Ordinary Least Squares Regression', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display parameter comparison\n",
        "print(\"Parameter Estimation Results:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"True parameters:      β₀ = {true_intercept:6.3f}, β₁ = {true_slope:6.3f}\")\n",
        "print(f\"OLS estimates:        β̂₀ = {intercept:6.3f}, β̂₁ = {slope:6.3f}\")\n",
        "print(f\"No-intercept slope:   β̂₁ = {slope_without_intercept:6.3f}\")\n",
        "print(f\"Estimation errors:    Δβ₀ = {intercept - true_intercept:6.3f}, Δβ₁ = {slope - true_slope:6.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now you can use the OLS class to fit the model!\n",
        "\n",
        "But this could be cumbersome to code this up every time.\n",
        "\n",
        "Can we encapsulate this into an (univariate) OLS object that we could re-use this easily?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Block 1 — Defining a Class and `__init__` (Encapsulation)\n",
        "\n",
        "- A class bundles data (attributes) and behavior (methods)\n",
        "\n",
        "- `__init__` initializes object state when you create an instance\n",
        "\n",
        "- Instance attributes live on `self` and belong to each object\n",
        "\n",
        "- We’ll start a minimal `UnivariateOLS` model that stores configuration\n",
        "\n",
        "Exercise: Run the code and inspect `model.fit_intercept`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_data = x.tolist()\n",
        "y_data = y.tolist()\n",
        "print(f\"{x_data=:}\")\n",
        "print(f\"{y_data=:}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UnivariateOLS:  # we start with the simplest model, with class ModelName.\n",
        "    def __init__(self, x_data: List[float], y_data: List[float], fit_intercept: bool):\n",
        "        # This is the constructor (if you are familiar with C++).\n",
        "        # Initialize the model with the fit_intercept parameter, whether we want to fit the intercept.\n",
        "        # The self points to the instance of this class.\n",
        "        self.fit_intercept = bool(fit_intercept)\n",
        "        # Store the training data (X, y) to the instance.\n",
        "        self.x_data = deepcopy(x_data)\n",
        "        self.y_data = deepcopy(y_data)\n",
        "        # Store the intercept and slope fitted, initialized to None because we haven't fitted the model yet!\n",
        "        self.intercept = None\n",
        "        self.slope = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calling the UnivariateOLS(...) constructor will (i) create an instance of the class, and (ii) call the `__init__` method inside the class with the provided arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = UnivariateOLS(x_data=x, y_data=y, fit_intercept=True)\n",
        "# This isn't very informative, it shows the class name and the memory address of the object.\n",
        "# we will fix this later by defining the `__repr__` method.\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We have passed in a few data and a boolean flag to the model.\n",
        "print(f\"{model.fit_intercept=:}\")\n",
        "print(f\"{model.x_data=:}\")\n",
        "print(f\"{model.y_data=:}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# However, we haven't fitted the model yet, so the intercept and slope are None.\n",
        "print(f\"{model.intercept=:}\")\n",
        "print(f\"{model.slope=:}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Block 2 — Instance Method `fit` (Behavior on State)\n",
        "\n",
        "- Let's now define our first method (apart from the constructor) in this `UnivariateOLS` class.\n",
        "\n",
        "- Instance methods always take `self` as the first parameter, so that it can refer to other information stored in the instance (e.g., `self.x_data` and `self.y_data`).\n",
        "\n",
        "- Methods can read/modify attributes stored on `self`.\n",
        "\n",
        "- We implement `fit(X, y)` using the closed form solution to OLS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UnivariateOLS:  # we start with the simplest model, with class ModelName.\n",
        "    def __init__(self, x_data: List[float], y_data: List[float], fit_intercept: bool):\n",
        "        # This is the constructor (if you are familiar with C++).\n",
        "        # Initialize the model with the fit_intercept parameter, whether we want to fit the intercept.\n",
        "        # The self points to the instance of this class.\n",
        "        self.fit_intercept = bool(fit_intercept)\n",
        "        # Store the training data (X, y) to the instance.\n",
        "        self.x_data = deepcopy(x_data)\n",
        "        self.y_data = deepcopy(y_data)\n",
        "        # Store the intercept and slope fitted, initialized to None because we haven't fitted the model yet!\n",
        "        self.intercept = None\n",
        "        self.slope = None\n",
        "\n",
        "    # ==================================================================================================================\n",
        "    # Everything above is the same as the previous version.\n",
        "    # ==================================================================================================================\n",
        "    def fit(self):\n",
        "        # The method inside the class can READ other attributes stored on this instance via the `self`.\n",
        "        # For example, `self.x_data` and `self.y_data` are the training data.\n",
        "        # Again, it is always good to check for potential data errors before you do the computation.\n",
        "        if len(self.x_data) != len(self.y_data):\n",
        "            raise ValueError(\"X and y must have same length\")\n",
        "\n",
        "        # get the length of the training data.\n",
        "        n = len(self.x_data)\n",
        "\n",
        "        # Get the sum of the training x and trainig y, we need them to compute the slope and intercept.\n",
        "        Sx = sum(self.x_data)\n",
        "        Sy = sum(self.y_data)\n",
        "\n",
        "        # Also the sum of suqare of x and the sum of x and y.\n",
        "        Sxx = 0\n",
        "        # you can loop over lists using for item in list.\n",
        "        for x in self.x_data:\n",
        "            Sxx += x * x\n",
        "\n",
        "        # Also the sum of x and y.\n",
        "        Sxy = 0\n",
        "        # you can loop over lists using index i.\n",
        "        for i in range(n):  # (0, 1, 2, ..., n-1).\n",
        "            Sxy += self.x_data[i] * self.y_data[i]\n",
        "\n",
        "        # You can choose to report some information, which can be helpful for debugging.\n",
        "        print(f\"Training data: n={n}, Sx={Sx}, Sy={Sy}, Sxx={Sxx}, Sxy={Sxy}\")\n",
        "\n",
        "        # Here we got two options!\n",
        "        if self.fit_intercept:\n",
        "            # The denominator of the slope.\n",
        "            den = n * Sxx - Sx * Sx\n",
        "            # If the denominator is zero, it means all x are identical, which is not a good thing.\n",
        "            # We rise the error here.\n",
        "            if den == 0:\n",
        "                raise ZeroDivisionError(\"Denominator is zero (all x identical?).\")\n",
        "            # The slope.\n",
        "            self.slope = float((n * Sxy - Sx * Sy) / den)\n",
        "            # The intercept.\n",
        "            self.intercept = float((Sy - self.slope * Sx) / n)\n",
        "            print(f\"Model intercept and slope have been fitted successfully.\")\n",
        "        else:\n",
        "            if Sxx == 0:\n",
        "                raise ZeroDivisionError(\"Sxx is zero (all x are zero?).\")\n",
        "            self.slope = float(Sxy / Sxx)\n",
        "            # we do not touch the intercept here, it remains None.\n",
        "            print(f\"Model slope has been fitted successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's now fit the first model (with intercept).\n",
        "model_with_intercept = UnivariateOLS(x_data=x, y_data=y, fit_intercept=True)\n",
        "model_with_intercept.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One can access the attributes of the instance via the dot notation.\n",
        "print(f\"{model_with_intercept.slope=:}\")\n",
        "print(f\"{model_with_intercept.intercept=:}\")\n",
        "\n",
        "# These are pretty close to the ground truth!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's now fit the second model (without intercept).\n",
        "model_without_intercept = UnivariateOLS(x_data=x, y_data=y, fit_intercept=False)\n",
        "model_without_intercept.fit()\n",
        "print(f\"{model_without_intercept.slope=:}\")\n",
        "print(f\"{model_without_intercept.intercept=:}\")  # the intercept remains None because we did not fit the intercept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's visualize both models with the original data\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the original data points\n",
        "plt.scatter(x, y, alpha=0.6, color='blue', label='Data points')\n",
        "\n",
        "# Create x values for plotting the fitted lines\n",
        "x_plot = np.linspace(min(x), max(x), 100)\n",
        "\n",
        "# Plot the model with intercept\n",
        "y_with_intercept = model_with_intercept.slope * x_plot + model_with_intercept.intercept\n",
        "plt.plot(x_plot, y_with_intercept, 'red', linewidth=2,\n",
        "         label=f'With intercept: y = {model_with_intercept.slope:.3f}x + {model_with_intercept.intercept:.3f}')\n",
        "\n",
        "# Plot the model without intercept\n",
        "y_without_intercept = model_without_intercept.slope * x_plot\n",
        "plt.plot(x_plot, y_without_intercept, 'green', linewidth=2,\n",
        "         label=f'Without intercept: y = {model_without_intercept.slope:.3f}x')\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Univariate OLS: Fitted Lines Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Block 3 — Using `self.intercept` and `self.slope` in Out-of-Sample `predict`\n",
        "\n",
        "- After `fit`, parameters live on the object (`self.intercept`, `self.slope`)\n",
        "\n",
        "- `predict(X)` uses learned parameters to generate outputs\n",
        "\n",
        "- We can also use `self.intercept` and `self.slope` in the `fit` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UnivariateOLS:  # we start with the simplest model, with class ModelName.\n",
        "    def __init__(self, x_data: List[float], y_data: List[float], fit_intercept: bool):\n",
        "        # This is the constructor (if you are familiar with C++).\n",
        "        # Initialize the model with the fit_intercept parameter, whether we want to fit the intercept.\n",
        "        # The self points to the instance of this class.\n",
        "        self.fit_intercept = bool(fit_intercept)\n",
        "        # Store the training data (X, y) to the instance.\n",
        "        self.x_data = deepcopy(x_data)\n",
        "        self.y_data = deepcopy(y_data)\n",
        "        # Store the intercept and slope fitted, initialized to None because we haven't fitted the model yet!\n",
        "        self.intercept = None\n",
        "        self.slope = None\n",
        "\n",
        "    # the new fit method.\n",
        "    def fit(self):\n",
        "        # The method inside the class can READ other attributes stored on this instance via the `self`.\n",
        "        # For example, `self.x_data` and `self.y_data` are the training data.\n",
        "        # Again, it is always good to check for potential data errors before you do the computation.\n",
        "        if len(self.x_data) != len(self.y_data):\n",
        "            raise ValueError(\"X and y must have same length\")\n",
        "\n",
        "        # get the length of the training data.\n",
        "        n = len(self.x_data)\n",
        "\n",
        "        # Get the sum of the training x and trainig y, we need them to compute the slope and intercept.\n",
        "        Sx = sum(self.x_data)\n",
        "        Sy = sum(self.y_data)\n",
        "\n",
        "        # Also the sum of suqare of x and the sum of x and y.\n",
        "        Sxx = 0\n",
        "        # you can loop over lists using for item in list.\n",
        "        for x in self.x_data:\n",
        "            Sxx += x * x\n",
        "\n",
        "        # Also the sum of x and y.\n",
        "        Sxy = 0\n",
        "        # you can loop over lists using index i.\n",
        "        for i in range(n):  # (0, 1, 2, ..., n-1).\n",
        "            Sxy += self.x_data[i] * self.y_data[i]\n",
        "\n",
        "        # You can choose to report some information, which can be helpful for debugging.\n",
        "        print(f\"Training data: n={n}, Sx={Sx}, Sy={Sy}, Sxx={Sxx}, Sxy={Sxy}\")\n",
        "\n",
        "        # Here we got two options!\n",
        "        if self.fit_intercept:\n",
        "            # The denominator of the slope.\n",
        "            den = n * Sxx - Sx * Sx\n",
        "            # If the denominator is zero, it means all x are identical, which is not a good thing.\n",
        "            # We rise the error here.\n",
        "            if den == 0:\n",
        "                raise ZeroDivisionError(\"Denominator is zero (all x identical?).\")\n",
        "            # The slope.\n",
        "            self.slope = float((n * Sxy - Sx * Sy) / den)\n",
        "            # The intercept.\n",
        "            self.intercept = float((Sy - self.slope * Sx) / n)\n",
        "            print(f\"Model intercept and slope have been fitted successfully.\")\n",
        "        else:\n",
        "            if Sxx == 0:\n",
        "                raise ZeroDivisionError(\"Sxx is zero (all x are zero?).\")\n",
        "            self.slope = float(Sxy / Sxx)\n",
        "            # we do not touch the intercept here, it remains None.\n",
        "            print(f\"Model slope has been fitted successfully.\")\n",
        "\n",
        "    # ==================================================================================================================\n",
        "    # Everything above is the same as the previous version.\n",
        "    # ==================================================================================================================\n",
        "    def predict(self, new_x_data: List[float]) -> List[float]:\n",
        "        # there are again two options!\n",
        "        if self.fit_intercept:\n",
        "            # check whether both intercept and slope are fitted.\n",
        "            if self.intercept is None or self.slope is None:\n",
        "                raise ValueError(\"Model not fit.\")\n",
        "\n",
        "            # otherwise, we can predict the new y using the new x.\n",
        "            return [self.intercept + self.slope * x for x in new_x_data]\n",
        "        else:\n",
        "            # Otherwise, we only require the slope to be fitted.\n",
        "            if self.slope is None:\n",
        "                raise ValueError(\"Model not fit.\")\n",
        "\n",
        "            # otherwise, we can predict the new y using the new x.\n",
        "            return [self.slope * x for x in new_x_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_new = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "model = UnivariateOLS(x_data=x, y_data=y, fit_intercept=True)\n",
        "model.fit()\n",
        "y_new_pred = model.predict(x_new)\n",
        "print(f\"The predicted y values for the new x values are: {y_new_pred=:}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the prediction\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(x, y, alpha=0.7, label='Training data', color='blue', s=50)\n",
        "plt.scatter(x_new, y_new_pred, alpha=0.8, label='Predictions', color='red', s=80, marker='^')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Univariate OLS: Training Data and Predictions')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## We have completed our first OOP object!\n",
        "\n",
        "- It loads the training data\n",
        "- Fits the model using the training data\n",
        "- Can make predictions on new data\n",
        "- The `UnivariateOLS` class already handles the complete data science!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Block 4 — One Additional Metric: RMSE\n",
        "- Add a method that uses object state and inputs to compute a metric, let's compute the Root Mean Squared Error (RMSE).\n",
        "\n",
        "$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n",
        "\n",
        "where $y_i$ is the true value, $\\hat{y}_i$ is the predicted value, and $n$ is the number of data points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stop here: are we going to make a copy of the UnivariateOLS class and add a new method? No we can define a new class that inherits from the UnivariateOLS class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Class Inheritance\n",
        "\n",
        "- Create a new class that inherits from the UnivariateOLS class.\n",
        "\n",
        "- Add a new method that computes the Root Mean Squared Error (RMSE).\n",
        "\n",
        "- The new class inherits the (i) data management `__init__`, (ii) model fitting `fit`, and (iii) prediction `predict` methods from the UnivariateOLS class.\n",
        "\n",
        "- This is pretty helpful if you want to build upon an existing class: for example, if you want to build an enhanced version of the random forest model from sklearn, you don't need to start from scratch by copying and pasting the code.\n",
        "\n",
        "- Instead, you can inherit from the existing class and add the new functionality or override the existing functionality.\n",
        "\n",
        "- Inheritance is a powerful feature of OOP that allows you **focus on the changes** rather than the entire codebase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UnivariateOLSwithRMSE(UnivariateOLS):  # <-- Inherits from the UnivariateOLS class, put the \"parent class\" in the parentheses.\n",
        "    # Multiple parent classes are possible, but we will not cover that in this course.\n",
        "    def __init__(self, x_data: List[float], y_data: List[float], fit_intercept: bool=True) -> None:\n",
        "        # The updated __init__ method calls the parent class's __init__ method so we don't\n",
        "        # You can access the parent class's attributes and methods using the super() function.\n",
        "        # We call the parent class's __init__ method to initialize the attributes of the parent class.\n",
        "        super().__init__(x_data, y_data, fit_intercept)\n",
        "\n",
        "        # after calling the parent class's __init__ method, we now have self.slope and self.intercept even though we did not define them in the UnivariateOLSwithRMSE.__init__ method.\n",
        "        print(f\"After calling the parent class's __init__ method, the slope is {self.slope} and the intercept is {self.intercept}.\")\n",
        "\n",
        "    def compute_rmse(self, x_data: List[float], y_data: List[float]) -> float:\n",
        "        # This is the new functionality that we want to add.\n",
        "        # It computes the RMSE of the model\n",
        "        squared_errors = []\n",
        "        # Note: we can use the parent class's method (i.e., self.predict) in this class even though we did not define it here.\n",
        "        predictions = self.predict(x_data)\n",
        "\n",
        "        # Note: there are much faster and more elegant ways to compute the RMSE using for example numpy (next quarter).\n",
        "        # But let's stick to the basics for now.\n",
        "        # Loop through each data point to calculate squared errors\n",
        "        for i in range(len(y_data)):\n",
        "            # Calculate the squared difference between actual and predicted values\n",
        "            squared_error = (y_data[i] - predictions[i]) ** 2\n",
        "            squared_errors.append(squared_error)\n",
        "\n",
        "        # Calculate the mean of all squared errors\n",
        "        mean_squared_error = sum(squared_errors) / len(squared_errors)\n",
        "\n",
        "        # Return the square root of the mean squared error (RMSE)\n",
        "        return np.sqrt(mean_squared_error)\n",
        "\n",
        "    # We can also override the parent class's methods, we can print out the training set RMSE after fitting the model.\n",
        "    def fit(self) -> None:\n",
        "        # We call the parent class's fit method to fit the model as usual.\n",
        "        super().fit()\n",
        "        # This new method also reports the training set RMSE.\n",
        "        print(f\"The training set RMSE is {self.compute_rmse(self.x_data, self.y_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = UnivariateOLSwithRMSE(x_data=x, y_data=y, fit_intercept=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Suppose that we have a new set of observations.\n",
        "x_new = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "y_new = [12.3, 14.1, 15.8, 17.2, 25.5, 23.1, 25.8, 29.2, 30.7, 36.1]\n",
        "y_new_pred = model.predict(x_new)\n",
        "print(f\"The predicted y values for the new x values are: {y_new_pred=:}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the original data, new data, and predictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot original training data\n",
        "plt.scatter(x, y, color='blue', alpha=0.7, label='Original training data')\n",
        "\n",
        "# Plot new actual data points\n",
        "plt.scatter(x_new, y_new, color='red', alpha=0.7, label='New actual data')\n",
        "\n",
        "# Plot predictions for new data\n",
        "plt.scatter(x_new, y_new_pred, color='green', alpha=0.7, label='Predictions for new data')\n",
        "\n",
        "# Add a line connecting predictions to show the trend\n",
        "plt.plot(x_new, y_new_pred, color='green', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Original Data, New Data, and Predictions')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We can compute how good the model fits on the new set of data.\n",
        "model.compute_rmse(x_new, y_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Block 5 — Special Methods\n",
        "\n",
        "- Special methods let objects integrate with Python syntax/printing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# As mentioned, we can use the `print` function to print any object.\n",
        "# But since this is a customized class we just built, the `print` function will not be able to print it in an informative way.\n",
        "# It only tells the class name and the memory address of the object.\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "str(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "repr(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Difference between `__str__` and `__repr__`\n",
        "\n",
        "#### Intended Audience\n",
        "- `__str__` \n",
        "    - Designed for **end users**. Its goal is to return a string that is readable and friendly — something you’d want to show in logs or a user interface.\n",
        "    - Called by the built-in `str()` function and by `print(obj)`.\n",
        "- `__repr__`\n",
        "    - Designed for **developers / debugging**. It should return an unambiguous string representation of the object — ideally one that could be used to recreate the object if passed to `eval()` (when practical).\n",
        "    - Called by the built-in `repr()` function and used by the interactive interpreter when you type an object at the prompt.\n",
        "\n",
        "#### Fallback Behavior\n",
        "- If you define only `__repr__` but not `__str__`, Python will use `__repr__` when you call `str(obj)` or `print(obj)`.\n",
        "- If you define only `__str__`, the interpreter will still use the default `__repr__` for the console/debug display.\n",
        "\n",
        "\n",
        "#### Conclusion\n",
        "- In this course, we are both the developer and the end user. I will be definig `__repr__` here because it updates the `__str__` method as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What information do you think to be helpful to report?\n",
        "\n",
        "A few helpful pieces of information to include:\n",
        "- What the model is for (e.g., \"OLS model with intercept\" or \"OLS model without intercept\")?\n",
        "- Whether the model has been fitted or not?\n",
        "- Some summary statistics for the training data (e.g., number of data points, mean of x, mean of y, etc.)\n",
        "- The fitted parameters (slope and intercept) if available.\n",
        "- A human-readable equation representation perhaps?\n",
        "- For debugging: the class name and memory address (keep it for debugging purposes)\n",
        "\n",
        "# Sample printing results:\n",
        "```\n",
        "# Before fitting:\n",
        "UnivariateOLS(not fitted)\n",
        "UnivariateOLS(fit_intercept=True, slope_=None, intercept_=None)\n",
        "\n",
        "After fitting:\n",
        "y ≈ 2.000 + 1.500 x\n",
        "UnivariateOLS(fit_intercept=True, slope_=1.5, intercept_=2.0)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We don't need to rewrite everything, instead, we inherit from the `UnivariateOLS` class and override the `__str__` and `__repr__` methods.\n",
        "\n",
        "class UnivariateOLSwithSummary(UnivariateOLS):\n",
        "    def __repr__(self) -> str:\n",
        "        representation_string = f\"UnivariateOLSwithSummary\"\n",
        "        if self.fit_intercept:\n",
        "            representation_string += \"\\n - Estimated with intercept\"\n",
        "        else:\n",
        "            representation_string += \"\\n - Estimated without intercept (i.e., fitted through the origin)\"\n",
        "\n",
        "        # Add summary statistics of the data.\n",
        "        representation_string += f\"\\n - Data summary: {len(self.x_data)} data points, mean of x={sum(self.x_data)/len(self.x_data):.3f}, mean of y={sum(self.y_data)/len(self.y_data):.3f}.\"\n",
        "        # Add range of x and y.\n",
        "        representation_string += f\"\\n - Data range: x in [{min(self.x_data):.3f}, {max(self.x_data):.3f}], y in [{min(self.y_data):.3f}, {max(self.y_data):.3f}].\"\n",
        "        # You can even add an ascii art plot of the data (less useful lol...)\n",
        "        representation_string += f\"\\n - Data plot for the trainig data: \\n\"\n",
        "        # Create a simple ASCII plot\n",
        "        plot_width = 40\n",
        "        plot_height = 8\n",
        "\n",
        "        # Get data ranges for scaling\n",
        "        x_min, x_max = min(self.x_data), max(self.x_data)\n",
        "        y_min, y_max = min(self.y_data), max(self.y_data)\n",
        "\n",
        "        # Create empty plot canvas\n",
        "        canvas = [[' ' for _ in range(plot_width)] for _ in range(plot_height)]\n",
        "        # Plot the first 100 data points.\n",
        "        for i in range(min(100, len(self.x_data))):\n",
        "            # Scale coordinates to canvas size\n",
        "            if x_max != x_min:\n",
        "                x_pos = int((self.x_data[i] - x_min) / (x_max - x_min) * (plot_width - 1))\n",
        "            else:\n",
        "                x_pos = plot_width // 2\n",
        "\n",
        "            if y_max != y_min:\n",
        "                y_pos = plot_height - 1 - int((self.y_data[i] - y_min) / (y_max - y_min) * (plot_height - 1))\n",
        "            else:\n",
        "                y_pos = plot_height // 2\n",
        "\n",
        "            # Place point on canvas\n",
        "            if 0 <= x_pos < plot_width and 0 <= y_pos < plot_height:\n",
        "                canvas[y_pos][x_pos] = '*'\n",
        "\n",
        "        # Convert canvas to string with axes\n",
        "        for i, row in enumerate(canvas):\n",
        "            # Add y-axis labels on the left\n",
        "            if i == 0:\n",
        "                y_label = f\"{y_max:.1f}\"\n",
        "            elif i == plot_height - 1:\n",
        "                y_label = f\"{y_min:.1f}\"\n",
        "            else:\n",
        "                y_label = \" \" * 4\n",
        "\n",
        "            representation_string += f\"   {y_label:>4}|\" + \"\".join(row) + \"\\n\"\n",
        "\n",
        "        # Add x-axis\n",
        "        representation_string += \"   \" + \" \" * 4 + \"+\" + \"-\" * plot_width + \"\\n\"\n",
        "        representation_string += f\"   {' ' * 4} {x_min:.1f}\" + \" \" * (plot_width - 8) + f\"{x_max:.1f}\\n\"\n",
        "\n",
        "        # Add the fitted parameters if the model has been fitted.\n",
        "        if self.slope is None:\n",
        "            # the model has not been fitted yet\n",
        "            representation_string += \"\\n - Not fitted, call `fit` method to fit the model before using it.\"\n",
        "        else:\n",
        "            # the model has been fitted\n",
        "            if self.fit_intercept:\n",
        "                representation_string += f\"\\n - Fitted (slope={self.slope:.3f}, intercept={self.intercept:.3f}).\"\n",
        "            else:\n",
        "                representation_string += f\"\\n - Fitted (slope={self.slope:.3f}, fitted through the origin). \"\n",
        "\n",
        "        # finally, the __repr__ method returns the string representation of the model.\n",
        "        return representation_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = UnivariateOLSwithSummary(x_data=x_data, y_data=y_data, fit_intercept=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "str(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "repr(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The model changes after fitting.\n",
        "model.fit()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Optional — Streaming/Online OLS with `__iadd__` and `__add__` Special Methods\n",
        "\n",
        "- Motivation: Imagine you're running a real-time analytics system where new data points arrive continuously throughout the day. Instead of re-fitting your entire model every time a new observation comes in, you want to update your regression incrementally. This is especially important when dealing with streaming data from sensors, financial markets, or user interactions.\n",
        "\n",
        "- Overload `__iadd__` (for `+=`) and `__add__` (for `+`) to add new data points to update the model.\n",
        "\n",
        "- This demonstrates: custom behavior for built-in operators.\n",
        "\n",
        "- When you write `a + b`, Python does not hard-code addition, instead, it calls the special (dunder) method: `a.__add__(b)`.\n",
        "\n",
        "- When you write `a += b`, Python calls `a.__iadd__(b)`.\n",
        "\n",
        "- The pattern here is, the addition method (`__add__` and `__iadd__`) of a given objectis called with a single arugment that is **compatiable to be added to the object** and returns the modified object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's define the addition here to be: adding a new pair of training data to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UnivariateOnlineOLS(UnivariateOLSwithSummary):\n",
        "    def __iadd__(self, new_data: Tuple[float, float]) -> \"UnivariateOnlineOLS\":\n",
        "        self.x_data.append(new_data[0])\n",
        "        self.y_data.append(new_data[1])\n",
        "        # refit the model with the new data.\n",
        "        self.fit()\n",
        "        # return the modified object.\n",
        "        return self\n",
        "\n",
        "    def __add__(self, new_data: Tuple[float, float]) -> \"UnivariateOnlineOLS\":\n",
        "        # we can reuse the __iadd__ method to add the new data.\n",
        "        return self.__iadd__(new_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's see how it works..\n",
        "model = UnivariateOnlineOLS(x_data=x_data, y_data=y_data, fit_intercept=True)\n",
        "model.fit()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add a new pair of data points to the model.\n",
        "# See how the summay statistics and the fitted line change.\n",
        "model = model + (15, 5)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model += (20, 3)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_data_pairs = [(15, 5), (20, 3), (25, 2), (30, 1), (35, 0.5), (40, 0.25), (45, 0.1), (50, 0.05), (55, 0.025), (60, 0.01)]\n",
        "\n",
        "model = UnivariateOnlineOLS(x_data=x_data, y_data=y_data, fit_intercept=True)\n",
        "model.fit()\n",
        "# add them one by one and see how the model changes.\n",
        "fitted_slopes = [model.slope]\n",
        "fitted_intercepts = [model.intercept]\n",
        "for pair in new_data_pairs:\n",
        "    model += pair\n",
        "    fitted_slopes.append(model.slope)\n",
        "    fitted_intercepts.append(model.intercept)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot the evolution of fitted lines as new data points are added\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "\n",
        "# Create initial model to get x_data range\n",
        "temp_model = UnivariateOnlineOLS(x_data=x_data, y_data=y_data, fit_intercept=True)\n",
        "temp_model.fit()\n",
        "\n",
        "# Plot original data points\n",
        "ax.scatter(x_data, y_data, color='blue', alpha=0.6, s=30, label='Original data')\n",
        "\n",
        "# Plot all added data points\n",
        "all_added_x = [pair[0] for pair in new_data_pairs]\n",
        "all_added_y = [pair[1] for pair in new_data_pairs]\n",
        "ax.scatter(all_added_x, all_added_y, color='red', alpha=0.8, s=50, marker='s', label='Added data')\n",
        "\n",
        "# Get overall x range for all data\n",
        "all_x = list(x_data) + all_added_x\n",
        "x_min, x_max = min(all_x), max(all_x)\n",
        "x_range = np.linspace(x_min, x_max, 100)\n",
        "\n",
        "# Create color map from yellow to dark red\n",
        "colors = plt.cm.YlOrRd(np.linspace(0.3, 1.0, len(fitted_slopes)))\n",
        "\n",
        "# Plot each fitted line with different colors (yellow to dark red)\n",
        "for i in range(len(fitted_slopes)):\n",
        "    y_pred = fitted_slopes[i] * x_range + fitted_intercepts[i]\n",
        "    data_points = len(x_data) + i\n",
        "    ax.plot(x_range, y_pred, color=colors[i], linewidth=2,\n",
        "            label=f'Step {i}: {data_points} points (slope={fitted_slopes[i]:.3f})')\n",
        "\n",
        "ax.set_title('Evolution of Fitted Lines as New Data Points are Added', fontsize=14)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References & Next Steps\n",
        "\n",
        "**For deeper learning:**\n",
        "- 📖 **Real Python OOP Guide**: Comprehensive tutorial on Python OOP concepts\n",
        "  `https://realpython.com/python3-object-oriented-programming/`\n",
        "\n",
        "- 💻 **Interactive Examples**: Companion Colab notebook (from last quarter's OOP lecture) with hands-on exercises\n",
        "  `https://colab.research.google.com/drive/1lElGzDa_uOUNB2YeZ8h92QCCuCRR2esm?usp=sharing`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cme193-autumn-2025",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
