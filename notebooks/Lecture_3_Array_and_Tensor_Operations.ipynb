{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CME 193 ‚Äì Lecture 3: Array & Tensor Operations in Python\n",
        "\n",
        "> **TIME: 3:35 PM - Start of class**\n",
        "\n",
        "In this notebook we will learn **how to perform different operations on arrays/tensors with native Python, NumPy and PyTorch.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Installing PyTorch\n",
        "Use the following command to install PyTorch to the current virtual environment used by the notebook. You only need to run this once, you can comment it out after the installation is complete.\n",
        "\n",
        "The `torch` package requires the `numpy` package as a \"dependency\", installing `torch` will also install `numpy` automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# comment out the following line if you have already installed PyTorch.\n",
        "# ! pip install torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify Installation\n",
        "\n",
        "> **TIME: 3:35‚Äì3:38 PM**\n",
        "\n",
        "Run this in a Python terminal or Jupyter cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NumPy version: 2.3.3\n",
            "PyTorch version: 2.8.0\n",
            "CUDA available: False\n",
            "MPS available: True\n",
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "# Import NumPy for numerical array operations\n",
        "import numpy as np\n",
        "# Import PyTorch for tensor operations and deep learning\n",
        "import torch\n",
        "# Import pprint for pretty-printing nested data structures\n",
        "from pprint import pprint\n",
        "\n",
        "# Display the installed version of NumPy\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "# Display the installed version of PyTorch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "# Check if CUDA (GPU support) is available on this system\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "# Check if MPS (Metal Performance Shaders for Apple Silicon M1/M2/M3/M4 Macs) is available\n",
        "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
        "\n",
        "# automatically chose the best device for your system\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìö Tensor Terminology\n",
        "\n",
        "> **TIME: 3:38‚Äì3:43 PM**\n",
        "\n",
        "**Important naming conventions:**\n",
        "\n",
        "<!-- ![tensor-terminology](https://github.com/tianyudu/CME193-Autumn-2025/blob/main/notebooks/images/tensors.png?raw=true) -->\n",
        "![tensor-terminology](./images/tensors.png?raw=true)\n",
        "\n",
        "- **Vector**: 1-dimensional tensor/array (just like a list in Python)\n",
        "\n",
        "- **Matrix**: 2-dimensional tensor/array (just like a *nested* list in Python)\n",
        "\n",
        "- **Tensor**: General term, but often refers to 3+ dimensions, so vectors and matrices are special cases of tensors\n",
        "\n",
        "- In NumPy, everything is called an \"array\" or \"ndarray\"\n",
        "\n",
        "- In PyTorch, everything is called a \"tensor\" because arrays, matrices, and higher-dimensional tensors are all tensors\n",
        "\n",
        "- Note: the Tensor data type in Pytorch is similar to the \"tensor\" in Physics, but not exactly the same.\n",
        "\n",
        "This terminology is important when reading documentation and research papers!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example of Creating Vectors and Matrices with Native Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3]\n",
            "[[1, 2, 3], [4, 5, 6]]\n",
            "[[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]\n",
            "1\n",
            "2\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "vector_example = [1, 2, 3]\n",
        "matrix_example = [[1, 2, 3], [4, 5, 6]]\n",
        "tensor_example = [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]\n",
        "\n",
        "pprint(vector_example)\n",
        "pprint(matrix_example)\n",
        "pprint(tensor_example)\n",
        "\n",
        "print(vector_example[0])\n",
        "print(matrix_example[0][1])\n",
        "print(tensor_example[0][1][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "list indices must be integers or slices, not tuple",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmatrix_example\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# <-- this won't work with native Python nested lists!\u001b[39;00m\n",
            "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not tuple"
          ]
        }
      ],
      "source": [
        "print(matrix_example[0, 1])  # <-- this won't work with native Python nested lists!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üêå Why Not Just Use Python Lists?\n",
        "\n",
        "> **TIME: 3:43‚Äì3:48 PM**\n",
        "\n",
        "\n",
        "Before we dive into NumPy and PyTorch, let's see why we need them at all!\n",
        "\n",
        "Let's try to square 1 million numbers in different ways and see how fast they are.\n",
        "\n",
        "- **Vectorization/Parallelism**: Utilize multiple cores of the CPU.\n",
        "- **GPU acceleration**: PyTorch can offload operations to thousands of parallel GPU cores (thousands to tens of thousands cores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "PERFORMANCE TEST: Squaring 1,000,000 numbers\n",
            "======================================================================\n",
            "\n",
            "1Ô∏è‚É£  Python list comprehension:\n",
            "44.6 ms ¬± 4.16 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n",
            "\n",
            "2Ô∏è‚É£  NumPy vectorized:\n",
            "1.12 ms ¬± 57.6 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n",
            "\n",
            "3Ô∏è‚É£  PyTorch vectorized:\n",
            "The slowest run took 64.56 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "11.4 ms ¬± 22.4 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n",
            "\n",
            "======================================================================\n",
            "üöÄ NumPy and PyTorch are typically 20-100x faster!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Simple task: element-wise square of 1 million numbers\n",
        "print(\"=\" * 70)\n",
        "print(\"PERFORMANCE TEST: Squaring 1,000,000 numbers\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "n = 1_000_000\n",
        "\n",
        "# Python lists (manual loop)\n",
        "print(\"\\n1Ô∏è‚É£  Python list comprehension:\")\n",
        "%timeit [x**2 for x in range(n)]\n",
        "\n",
        "# NumPy (vectorized)\n",
        "print(\"\\n2Ô∏è‚É£  NumPy vectorized:\")\n",
        "%timeit np.arange(n)**2\n",
        "\n",
        "# NumPy (manual loop)\n",
        "# print(\"\\n2Ô∏è‚É£b NumPy with manual loop:\")\n",
        "# def numpy_loop_square(n):\n",
        "#     arr = np.arange(n)\n",
        "#     result = np.empty(n)\n",
        "#     for i in range(n):\n",
        "#         result[i] = arr[i]**2\n",
        "#     return result\n",
        "# %timeit numpy_loop_square(n)\n",
        "\n",
        "# PyTorch (vectorized)\n",
        "print(\"\\n3Ô∏è‚É£  PyTorch vectorized:\")\n",
        "%timeit torch.arange(n).to(device)**2\n",
        "\n",
        "# PyTorch (manual loop)\n",
        "# print(\"\\n3Ô∏è‚É£b PyTorch with manual loop:\")\n",
        "# def pytorch_loop_square(n):\n",
        "#     arr = torch.arange(n)\n",
        "#     result = torch.empty(n)\n",
        "#     for i in range(n):\n",
        "#         result[i] = arr[i]**2\n",
        "#     return result\n",
        "# %timeit pytorch_loop_square(n)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ NumPy and PyTorch are typically 10-50x faster!\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Why Two Packages? They have different areas of specialization, here is a comparison:\n",
        "\n",
        "> **TIME: 3:48‚Äì3:50 PM**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| NumPy | PyTorch |\n",
        "|---|---|\n",
        "| **What is it?**<br>NumPy (Numerical Python) is a foundational library for scientific computing in Python. It provides efficient operations on large, multi-dimensional arrays and matrices, plus a broad suite of math functions. | **What is it?**<br>PyTorch is an open-source machine learning library widely used for deep learning. It emphasizes flexibility and speed when building and training neural networks. |\n",
        "| **Key features**<br>‚Ä¢ Efficient n-dimensional array (ndarray) operations<br>‚Ä¢ Rich math: linear algebra, FFT, random sampling, etc.<br>‚Ä¢ Integrates smoothly with SciPy, Pandas, Matplotlib<br>‚Ä¢ High performance from C under the hood | **Key features**<br>‚Ä¢ Dynamic computation graphs for flexible modeling and debugging<br>‚Ä¢ Tensors (NumPy-like) with GPU acceleration<br>‚Ä¢ Autograd for automatic differentiation<br>‚Ä¢ Rich ecosystem for CV, NLP, and more |\n",
        "| **Core purpose**<br>General numerical & scientific computing: array manipulation, math functions, and data analysis foundations. | **Core purpose**<br>Deep learning focus: building and training neural networks (while still supporting general tensor ops). |\n",
        "| **Data structures**<br>Primary type: **ndarray** (N-dimensional array) supporting many dtypes and vectorized ops. | **Data structures**<br>Primary type: **Tensor**, similar to ndarray but designed to work with GPUs and autograd. |\n",
        "| **Computation**<br>Primarily CPU-based; fast but no built-in GPU acceleration. | **Computation**<br>Built for GPU acceleration; tensors can be moved to GPUs to speed up large ML workloads. |\n",
        "| **Automatic differentiation**<br>No built-in autodiff; gradients must be computed manually or via external tools. | **Automatic differentiation**<br>**autograd** automatically computes gradients to simplify backprop and optimization. |\n",
        "| **Typical use cases**<br>‚Ä¢ General scientific computing & data analysis<br>‚Ä¢ Array/matrix math and linear algebra<br>‚Ä¢ Data preprocessing before ML/DL | **Typical use cases**<br>‚Ä¢ Building & training deep learning models<br>‚Ä¢ Tasks benefiting from dynamic graphs (e.g., RNNs)<br>‚Ä¢ Flexible research & rapid prototyping in AI |\n",
        "| **Ecosystem & integration**<br>Works hand-in-glove with SciPy/Pandas/Matplotlib to form a full data science stack. | **Ecosystem & integration**<br>Part of a larger DL ecosystem (e.g., Torchvision; integrates with libraries like Transformers). Plays well with NumPy, with easy array‚Üîtensor conversion. |\n",
        "\n",
        "**Reference**: [https://www.linkedin.com/pulse/numpy-vs-pytorch-nadir-riyani-z6cdf/](https://www.linkedin.com/pulse/numpy-vs-pytorch-nadir-riyani-z6cdf/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Creating arrays & tensors\n",
        "\n",
        "> **TIME: 3:50‚Äì4:00 PM**\n",
        "\n",
        "### 1.1 Creating Numpy Arrays and Torch Tensors from Python lists with `np.array(<PYTHON LIST>)` and `torch.tensor(<PYTHON LIST>)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Creating from Python lists ===\n",
            "Python list: [1, 2, 3]\n",
            "NumPy array: [1 2 3]\n",
            "PyTorch tensor: tensor([1, 2, 3])\n"
          ]
        }
      ],
      "source": [
        "# From Python lists\n",
        "print(\"=== Creating from Python lists ===\")\n",
        "python_list = [1, 2, 3]\n",
        "print(\"Python list:\", python_list)\n",
        "np_arr = np.array(python_list)\n",
        "print(\"NumPy array:\", np_arr)\n",
        "pt_arr = torch.tensor(python_list)\n",
        "print(\"PyTorch tensor:\", pt_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before we start, how do we know what we've created? Inspecting Array/Tensor Shapes/Sizes\n",
        "\n",
        "### Shape and Number of Elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NumPy array shape: (10,)\n",
            "PyTorch tensor shape: torch.Size([10])\n",
            "NumPy total elements: 10\n",
            "PyTorch total elements: 10\n"
          ]
        }
      ],
      "source": [
        "# Get the shape (dimensions)\n",
        "np_arr = np.arange(10)\n",
        "pt_arr = torch.arange(10)\n",
        "print(\"NumPy array shape:\", np_arr.shape)      # Returns a tuple, e.g., (10,)\n",
        "print(\"PyTorch tensor shape:\", pt_arr.shape)   # Returns torch.Size([...])\n",
        "\n",
        "# Get the total number of elements\n",
        "print(\"NumPy total elements:\", np_arr.size)      # For NumPy: .size\n",
        "print(\"PyTorch total elements:\", pt_arr.numel())  # For PyTorch: .numel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "| Feature | NumPy | PyTorch |\n",
        "|---------|-------|---------|\n",
        "| **Get shape** | `.shape` returns tuple, e.g., `(3,)` | `.shape` returns `torch.Size([3])` |\n",
        "| **Total elements** | `.size` (attribute) | `.numel()` (method) |\n",
        "| **`len()` behavior** | Returns size of first dimension (rows for 2D+) | Returns size of first dimension (rows for 2D+) |\n",
        "\n",
        "\n",
        "### What does `len()` do?\n",
        "\n",
        "‚ö†Ô∏è **Important distinction:**\n",
        "- **For 1D arrays/tensors**: `len()` returns the number of elements\n",
        "- **For multi-dimensional arrays/tensors**: `len()` returns the size of the **first dimension** (number of rows), NOT the total!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n",
            "3\n",
            "6\n"
          ]
        }
      ],
      "source": [
        "vector = np.array([1, 2, 3])\n",
        "matrix = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "print(len(vector))   # Output: 3 (number of elements)\n",
        "print(len(matrix))   # Output: 3 (rows, NOT 6!)\n",
        "print(matrix.size)   # Output: 6 (total elements)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Best practice:** Use `.size` (NumPy) or `.numel()` (PyTorch) for total element count, do not use `len()`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Creating Zero Matrices: `np.zeros(shape)` and `torch.zeros(shape)`\n",
        "\n",
        "What's **shape**? By shape we mean a tuple or list of integers that describe the size of the array/tensor. For example, `(2, 3)` means the shape of the array/tensor is 2x3 matrix, and `(2, 3, 4)` means the shape of the array/tensor is 2x3x4 tensor, and this tensor has 2x3x4=24 elements.\n",
        "\n",
        "**Syntax:**\n",
        "- NumPy: `np.zeros((rows, cols))`\n",
        "- PyTorch: `torch.zeros((rows, cols))`\n",
        "\n",
        "**Mathematical notation:**\n",
        "\n",
        "$$\n",
        "\\mathbf{0}_{2 \\times 3} = \\begin{bmatrix}\n",
        "0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Zeros (2x3) ===\n",
            "Python list:\n",
            "  [0.0, 0.0, 0.0]\n",
            "  [0.0, 0.0, 0.0]\n",
            "NumPy:\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "PyTorch:\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "# Zeros\n",
        "print(\"\\n=== Zeros (2x3) ===\")\n",
        "# Python list\n",
        "py_zeros = [[0.0 for i in range(3)] for j in range(2)]  # 2x3 matrix\n",
        "print(\"Python list:\")\n",
        "for row in py_zeros:\n",
        "    print(\" \", row)\n",
        "# NumPy & PyTorch\n",
        "np_zeros = np.zeros((2,3))\n",
        "pt_zeros = torch.zeros((2,3))\n",
        "print(\"NumPy:\")\n",
        "print(np_zeros)\n",
        "print(\"PyTorch:\")\n",
        "print(pt_zeros)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Ones Matrices: `np.ones(shape)` and `torch.ones(shape)`\n",
        "\n",
        "**Syntax:**\n",
        "- NumPy: `np.ones((rows, cols))`\n",
        "- PyTorch: `torch.ones((rows, cols))`\n",
        "\n",
        "**Mathematical notation:**\n",
        "\n",
        "$$\n",
        "\\mathbf{1}_{3 \\times 2} = \\begin{bmatrix}\n",
        "1 & 1 \\\\\n",
        "1 & 1 \\\\\n",
        "1 & 1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ones\n",
        "print(\"\\n=== Ones (3x2) ===\")\n",
        "py_ones = [[1.0 for _ in range(2)] for _ in range(3)]  # 3x2 matrix\n",
        "print(\"Python list:\", py_ones)\n",
        "np_ones = np.ones((3,2))\n",
        "pt_ones = torch.ones((3,2))\n",
        "print(\"NumPy:\")\n",
        "print(np_ones)\n",
        "print(\"PyTorch:\")\n",
        "print(pt_ones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Sequences: `np.arange(n)` and `torch.arange(n)`\n",
        "\n",
        "**Syntax:**\n",
        "- Python: `list(range(n))`\n",
        "- NumPy: `np.arange(n)`\n",
        "- PyTorch: `torch.arange(n)`\n",
        "\n",
        "**Mathematical notation:**\n",
        "\n",
        "$$\n",
        "\\text{range}(10) = \\begin{bmatrix}\n",
        "0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Range/arange\n",
        "print(\"\\n=== Range 0..9 ===\")\n",
        "py_range = list(range(10))\n",
        "np_range = np.arange(10)\n",
        "pt_range = torch.arange(10)\n",
        "print(\"Python range:\", py_range)\n",
        "print(\"NumPy arange:\", np_range)\n",
        "print(\"PyTorch arange:\", pt_range)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Sequences with Step: `np.arange(start, stop, step)`\n",
        "\n",
        "**Syntax:**\n",
        "- Python: `list(range(start, stop, step))`\n",
        "- NumPy: `np.arange(start, stop, step)`\n",
        "- PyTorch: `torch.arange(start, stop, step)`\n",
        "\n",
        "**Mathematical notation:**\n",
        "\n",
        "$$\n",
        "\\text{range}(2, 10, \\text{step}=2) = \\begin{bmatrix}\n",
        "2 & 4 & 6 & 8 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Range with step\n",
        "print(\"\\n=== Range 2..10 step 2 ===\")\n",
        "py_step = list(range(2, 10, 2))\n",
        "np_step = np.arange(2, 10, 2)  # start, stop, step\n",
        "pt_step = torch.arange(2, 10, 2)\n",
        "print(\"Python range:\", py_step)\n",
        "print(\"NumPy arange:\", np_step)\n",
        "print(\"PyTorch arange:\", pt_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Evenly Spaced Values: `np.linspace(start, stop, num)` and `torch.linspace(start, stop, num)`\n",
        "\n",
        "**Syntax:**\n",
        "- NumPy: `np.linspace(start, stop, num)` - Creates `num` evenly spaced values from `start` to `stop` (inclusive)\n",
        "- PyTorch: `torch.linspace(start, stop, num)`\n",
        "\n",
        "**Mathematical notation:**\n",
        "\n",
        "$$\n",
        "\\text{linspace}(0, 1, 5) = \\begin{bmatrix}\n",
        "0.0 & 0.25 & 0.5 & 0.75 & 1.0 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Difference from `range`:** `linspace` includes the endpoint and spaces values evenly, while `arange` uses step sizes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Linspace (PyTorch has this too!)\n",
        "print(\"\\n=== Linspace 0 to 1 (5 points) ===\")\n",
        "# Python (manual calculation)\n",
        "py_lin = [0.0 + i * (1.0 - 0.0) / (5 - 1) for i in range(5)]\n",
        "np_lin = np.linspace(0, 1, 5)  # 5 evenly spaced values from 0 to 1\n",
        "pt_lin = torch.linspace(0, 1, 5)\n",
        "print(\"Python list:\", [f\"{x:.4f}\" for x in py_lin])\n",
        "print(\"NumPy linspace:\", np_lin)\n",
        "print(\"PyTorch linspace:\", pt_lin)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Uninitialized Arrays/Tensors: `np.empty(shape)` and `torch.empty(shape)` üóëÔ∏è\n",
        "\n",
        "**Syntax:**\n",
        "- NumPy: `np.empty((rows, cols))`\n",
        "- PyTorch: `torch.empty((rows, cols))`\n",
        "\n",
        "**Mathematical notation:**\n",
        "\n",
        "$$\n",
        "\\mathbf{?}_{2 \\times 3} = \\begin{bmatrix}\n",
        "? & ? & ? \\\\\n",
        "? & ? & ? \\\\\n",
        "\\end{bmatrix}\n",
        "\\quad \\text{(random garbage from memory!)}\n",
        "$$\n",
        "\n",
        "‚ö†Ô∏è **Warning**: These create arrays/tensors without initializing values - you'll see whatever was in memory! It could be zeros, but you should not expect it to be zeros.\n",
        "\n",
        "**Why do we need uninitialized arrays/tensors?**\n",
        "- **Performance**: It's faster to create an uninitialized array as a placeholder and fill it later\n",
        "- **Use case**: When you know you'll immediately overwrite all values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NumPy empty (uninitialized):\n",
            "[[4.94e-323 1.04e-322 1.58e-322]\n",
            " [6.42e-323 1.19e-322 1.73e-322]]\n",
            "Type: float64\n",
            "\n",
            "PyTorch empty (uninitialized):\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "Type: torch.float32\n",
            "\n",
            "NumPy zeros timing:\n",
            "489 Œºs ¬± 59.2 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n",
            "\n",
            "NumPy empty timing:\n",
            "22.3 Œºs ¬± 414 ns per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)\n",
            "\n",
            "PyTorch zeros timing:\n",
            "222 Œºs ¬± 37.3 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n",
            "\n",
            "PyTorch empty timing:\n",
            "13.4 Œºs ¬± 321 ns per loop (mean ¬± std. dev. of 7 runs, 100,000 loops each)\n"
          ]
        }
      ],
      "source": [
        "# Creating uninitialized arrays (fastest but unsafe!)\n",
        "# NumPy: empty()\n",
        "np_empty = np.empty((2, 3))  # Whatever was in memory\n",
        "print(\"NumPy empty (uninitialized):\")\n",
        "print(np_empty)\n",
        "print(\"Type:\", np_empty.dtype)\n",
        "\n",
        "# PyTorch: empty()\n",
        "pt_empty = torch.empty((2, 3))\n",
        "print(\"\\nPyTorch empty (uninitialized):\")\n",
        "print(pt_empty)\n",
        "print(\"Type:\", pt_empty.dtype)\n",
        "\n",
        "# Why use empty()? Performance when you'll overwrite all values immediately\n",
        "\n",
        "# Timing comparison\n",
        "size = (1000, 1000)\n",
        "\n",
        "# NumPy zeros\n",
        "print(\"\\nNumPy zeros timing:\")\n",
        "%timeit np.zeros(size)\n",
        "\n",
        "# NumPy empty\n",
        "print(\"\\nNumPy empty timing:\")\n",
        "%timeit np.empty(size)\n",
        "\n",
        "# PyTorch zeros\n",
        "print(\"\\nPyTorch zeros timing:\")\n",
        "%timeit torch.zeros(size)\n",
        "\n",
        "# PyTorch empty\n",
        "print(\"\\nPyTorch empty timing:\")\n",
        "%timeit torch.empty(size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Types (dtypes) (Please read after the lecture)\n",
        "\n",
        "Both libraries support multiple data types. **PyTorch defaults to float32, NumPy to float64.**\n",
        "\n",
        "### Why Care About Data Types?\n",
        "\n",
        "- **Memory efficiency**: `float16` uses half the memory of `float32`, which uses half of `float64`\n",
        "- **Performance**: Smaller data types can be faster (less data movement)\n",
        "- **GPU constraints**: GPUs often perform better with `float32` or `float16`\n",
        "- **Precision tradeoffs**: Higher precision (float64) for scientific computing, lower (float16/32) for deep learning\n",
        "- **Compatibility**: Ensure data types match between operations to avoid automatic conversions\n",
        "\n",
        "### Memory Requirements Formula\n",
        "\n",
        "$$\n",
        "\\text{Memory (bytes)} = \\text{num\\_elements} \\times \\text{bytes\\_per\\_element}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{For a matrix } M_{m \\times n} \\text{ with dtype float32: } \\text{Memory} = m \\times n \\times 4 \\text{ bytes}\n",
        "$$\n",
        "\n",
        "**Example:** A 1000√ó1000 `float32` matrix uses: $1000 \\times 1000 \\times 4 = 4,000,000$ bytes = 3.81 MB\n",
        "\n",
        "### Data Type Summary Table\n",
        "\n",
        "| Data Type | NumPy | PyTorch | Bytes | Memory (1M elements) | Range/Notes |\n",
        "|-----------|-------|---------|-------|---------------------|-------------|\n",
        "| **Integers** | | | | | |\n",
        "| 8-bit int | `np.int8` | `torch.int8` | 1 | 1 MB | -128 to 127 |\n",
        "| 32-bit int | `np.int32` | `torch.int32` | 4 | 4 MB | ¬±2.1 billion |\n",
        "| 64-bit int | `np.int64` | `torch.int64` or `.long` | 8 | 8 MB | ¬±9.2 quintillion |\n",
        "| **Floats** | | | | | |\n",
        "| Half precision | `np.float16` | `torch.float16` or `.half` | 2 | 2 MB | ~3 decimal digits |\n",
        "| Single precision | `np.float32` | `torch.float32` or `.float` | 4 | 4 MB | ~7 decimal digits |\n",
        "| Double precision | `np.float64` | `torch.float64` or `.double` | 8 | 8 MB | ~15 decimal digits |\n",
        "| **Boolean** | `np.bool_` | `torch.bool` | 1 | 1 MB | True/False |\n",
        "\n",
        "### General Syntax\n",
        "\n",
        "**NumPy:**\n",
        "```python\n",
        "arr = np.array([1, 2, 3], dtype=np.float32)\n",
        "arr = np.zeros((3, 4), dtype=np.int64)\n",
        "```\n",
        "\n",
        "**PyTorch:**\n",
        "```python\n",
        "tensor = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
        "tensor = torch.zeros((3, 4), dtype=torch.int64)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Examples of creating arrays/tensors with different data types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Default dtypes:\n",
            "NumPy: float64\n",
            "PyTorch: torch.float32\n"
          ]
        }
      ],
      "source": [
        "# Default data types\n",
        "np_default = np.array([1.0, 2.0, 3.0])\n",
        "pt_default = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(\"Default dtypes:\")\n",
        "print(f\"NumPy: {np_default.dtype}\")  # float64 by default\n",
        "print(f\"PyTorch: {pt_default.dtype}\")  # float32 by default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Integer Types ===\n",
            "NumPy int8: int8, bytes per element: 1\n",
            "NumPy int32: int32, bytes per element: 4\n",
            "NumPy int64: int64, bytes per element: 8\n"
          ]
        }
      ],
      "source": [
        "# Common data types\n",
        "print(\"\\n=== Integer Types ===\")\n",
        "# NumPy\n",
        "np_int8 = np.array([1, 2, 3], dtype=np.int8)\n",
        "np_int32 = np.array([1, 2, 3], dtype=np.int32)\n",
        "np_int64 = np.array([1, 2, 3], dtype=np.int64)\n",
        "print(f\"NumPy int8: {np_int8.dtype}, bytes per element: {np_int8.itemsize}\")\n",
        "print(f\"NumPy int32: {np_int32.dtype}, bytes per element: {np_int32.itemsize}\")\n",
        "print(f\"NumPy int64: {np_int64.dtype}, bytes per element: {np_int64.itemsize}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PyTorch int8: torch.int8\n",
            "PyTorch int32: torch.int32\n",
            "PyTorch int64: torch.int64\n"
          ]
        }
      ],
      "source": [
        "# PyTorch\n",
        "pt_int8 = torch.tensor([1, 2, 3], dtype=torch.int8)\n",
        "pt_int32 = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
        "pt_int64 = torch.tensor([1, 2, 3], dtype=torch.int64)\n",
        "print(f\"\\nPyTorch int8: {pt_int8.dtype}\")\n",
        "print(f\"PyTorch int32: {pt_int32.dtype}\")\n",
        "print(f\"PyTorch int64: {pt_int64.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Float Types ===\n",
            "NumPy float16: float16, bytes: 2\n",
            "NumPy float32: float32, bytes: 4\n",
            "NumPy float64: float64, bytes: 8\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Float Types ===\")\n",
        "# NumPy\n",
        "np_float16 = np.array([1.0, 2.0, 3.0], dtype=np.float16)  # half precision\n",
        "np_float32 = np.array([1.0, 2.0, 3.0], dtype=np.float32)  # single precision\n",
        "np_float64 = np.array([1.0, 2.0, 3.0], dtype=np.float64)  # double precision\n",
        "print(f\"NumPy float16: {np_float16.dtype}, bytes: {np_float16.itemsize}\")\n",
        "print(f\"NumPy float32: {np_float32.dtype}, bytes: {np_float32.itemsize}\")\n",
        "print(f\"NumPy float64: {np_float64.dtype}, bytes: {np_float64.itemsize}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PyTorch\n",
        "pt_float16 = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float16)  # or torch.half\n",
        "pt_float32 = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)  # or torch.float\n",
        "pt_float64 = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float64)  # or torch.double\n",
        "print(f\"\\nPyTorch float16: {pt_float16.dtype}\")\n",
        "print(f\"PyTorch float32: {pt_float32.dtype}\")\n",
        "print(f\"PyTorch float64: {pt_float64.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Boolean Types ===\n",
            "NumPy bool: bool\n",
            "PyTorch bool: torch.bool\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Boolean Types ===\")\n",
        "np_bool = np.array([True, False, True])\n",
        "pt_bool = torch.tensor([True, False, True])\n",
        "print(f\"NumPy bool: {np_bool.dtype}\")\n",
        "print(f\"PyTorch bool: {pt_bool.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Type Conversion\n",
        "\n",
        "### General Syntax\n",
        "\n",
        "**NumPy:**\n",
        "```python\n",
        "arr_float = arr_int.astype(np.float32)  # General method\n",
        "```\n",
        "\n",
        "**PyTorch (multiple ways):**\n",
        "```python\n",
        "tensor_float = tensor_int.float()              # Convenience method -> float32\n",
        "tensor_double = tensor_int.double()            # Convenience method -> float64\n",
        "tensor_half = tensor_int.half()                # Convenience method -> float16\n",
        "tensor_int32 = tensor_float.int()              # Convenience method -> int32\n",
        "tensor_custom = tensor_int.to(torch.float16)   # General method\n",
        "```\n",
        "\n",
        "### Why Care About Type Conversion?\n",
        "\n",
        "- **GPU requirements**: Many GPU operations require `float32` or `float16`\n",
        "- **Memory optimization**: Convert to smaller types when high precision isn't needed\n",
        "- **Compatibility**: Some operations require matching data types\n",
        "- **Loss prevention**: Integer division truncates; convert to float first if you need decimals\n",
        "- **Performance**: Operations on mixed types trigger automatic conversions (slower)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Examples of Type Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Type Conversion ===\n",
            "NumPy: int32 -> float32\n"
          ]
        }
      ],
      "source": [
        "# Type conversion\n",
        "print(\"\\n=== Type Conversion ===\")\n",
        "np_arr = np.array([1, 2, 3], dtype=np.int32)\n",
        "np_float = np_arr.astype(np.float32)\n",
        "print(f\"NumPy: {np_arr.dtype} -> {np_float.dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: torch.int32 -> torch.float32, torch.float64, torch.float16\n"
          ]
        }
      ],
      "source": [
        "pt_arr = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
        "pt_float = pt_arr.float()  # or .to(torch.float32)\n",
        "pt_double = pt_arr.double()  # or .to(torch.float64)\n",
        "pt_half = pt_arr.half()  # or .to(torch.float16)\n",
        "print(f\"PyTorch: {pt_arr.dtype} -> {pt_float.dtype}, {pt_double.dtype}, {pt_half.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Original: tensor([1.5000, 2.7000, 3.9000])\n",
            "To int: tensor([1, 2, 3], dtype=torch.int32)\n",
            "To long: tensor([1, 2, 3])\n"
          ]
        }
      ],
      "source": [
        "# PyTorch also has convenient conversion methods\n",
        "pt_arr = torch.tensor([1.5, 2.7, 3.9])\n",
        "print(f\"\\nOriginal: {pt_arr}\")\n",
        "print(f\"To int: {pt_arr.int()}\")  # truncates\n",
        "print(f\"To long: {pt_arr.long()}\")  # int64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Converting between NumPy ‚ü∑ PyTorch using `torch.from_numpy()` and `tensor.numpy()` (Please read after the lecture)\n",
        "\n",
        "PyTorch shares memory with NumPy when possible (zero-copy conversion)!\n",
        "\n",
        "### What Does \"Share Memory\" Mean?\n",
        "\n",
        "**Shared memory** means both the NumPy array and PyTorch tensor point to the **same underlying data** in RAM:\n",
        "\n",
        "- ‚úÖ **Advantage**: No data copying ‚Üí instant conversion, saves memory\n",
        "- ‚ö†Ô∏è **Caution**: Modifying one affects the other!\n",
        "\n",
        "```python\n",
        "np_arr = np.array([1, 2, 3])\n",
        "pt_tensor = torch.from_numpy(np_arr)  # Shares memory!\n",
        "\n",
        "np_arr[0] = 999\n",
        "print(pt_tensor)  # Output: tensor([999, 2, 3]) - changed too!\n",
        "```\n",
        "\n",
        "**When memory is shared:**\n",
        "- `torch.from_numpy(np_array)` ‚Üí Shares memory\n",
        "- `tensor.numpy()` ‚Üí Shares memory (if tensor is on CPU and contiguous)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NumPy array:\n",
            "[[0 1 2]\n",
            " [3 4 5]]\n",
            "\n",
            "PyTorch tensor (shares memory):\n",
            "tensor([[0, 1, 2],\n",
            "        [3, 4, 5]])\n"
          ]
        }
      ],
      "source": [
        "np_arr = np.arange(6).reshape(2,3)\n",
        "pt_arr = torch.from_numpy(np_arr)\n",
        "print(\"NumPy array:\")\n",
        "print(np_arr)\n",
        "print(\"\\nPyTorch tensor (shares memory):\")\n",
        "print(pt_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "After modifying np_arr[0,0] = 99:\n",
            "tensor([[99,  1,  2],\n",
            "        [ 3,  4,  5]])\n",
            "\n",
            "Converted back to NumPy:\n",
            "[[99  1  2]\n",
            " [ 3  4  5]]\n"
          ]
        }
      ],
      "source": [
        "# Changing one changes the other\n",
        "np_arr[0,0] = 99\n",
        "print(\"\\nAfter modifying np_arr[0,0] = 99:\")\n",
        "print(pt_arr)\n",
        "\n",
        "# Back to NumPy (shares memory when tensor is on CPU!)\n",
        "back_to_np = pt_arr.numpy()\n",
        "print(\"\\nConverted back to NumPy:\")\n",
        "print(back_to_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[99  1  2]\n",
            " [ 3  4  5]]\n",
            "[[100   2   3]\n",
            " [  4   5   6]]\n"
          ]
        }
      ],
      "source": [
        "print(back_to_np)\n",
        "pt_arr += 1\n",
        "print(back_to_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**When memory is NOT shared:**\n",
        "- `torch.tensor(python_list)` ‚Üí Creates new copy\n",
        "- `tensor.cpu().numpy()` when tensor was on GPU ‚Üí Must copy from GPU to CPU\n",
        "- `np_array.copy()` or `tensor.clone()` ‚Üí Explicit copies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original NumPy array: [999   2   3]\n",
            "PyTorch tensor (independent copy): tensor([1, 2, 3])\n"
          ]
        }
      ],
      "source": [
        "# Example: Creating a copy (memory NOT shared)\n",
        "np_original = np.array([1, 2, 3])\n",
        "pt_copy = torch.tensor(np_original)  # Creates a new copy.\n",
        "\n",
        "np_original[0] = 999\n",
        "print(\"Original NumPy array:\", np_original)\n",
        "print(\"PyTorch tensor (independent copy):\", pt_copy)  # Still [1, 2, 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Explicit copies are independent:\n",
            "NumPy copy: [[100   2   3]\n",
            " [  4   5   6]]\n",
            "PyTorch clone: tensor([[100,   2,   3],\n",
            "        [  4,   5,   6]])\n",
            "NumPy copy: [[100   2   3]\n",
            " [  4   5   6]]\n",
            "PyTorch clone: tensor([[100,   2,   3],\n",
            "        [  4,   5,   6]])\n"
          ]
        }
      ],
      "source": [
        "# Explicit copies\n",
        "np_explicit_copy = np_arr.copy()\n",
        "pt_explicit_copy = pt_arr.clone()\n",
        "print(\"\\nExplicit copies are independent:\")\n",
        "print(\"NumPy copy:\", np_explicit_copy)\n",
        "print(\"PyTorch clone:\", pt_explicit_copy)\n",
        "np_arr += 100  # make modification to the original array won't affect the copy.\n",
        "print(\"NumPy copy:\", np_explicit_copy)\n",
        "print(\"PyTorch clone:\", pt_explicit_copy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Reshaping Arrays and Tensors\n",
        "\n",
        "> **TIME: 4:00‚Äì4:05 PM**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original shapes: (12,) torch.Size([12])\n",
            "\n",
            "NumPy reshape(3,4):\n",
            "[[ 0  1  2  3]\n",
            " [ 4  5  6  7]\n",
            " [ 8  9 10 11]]\n",
            "\n",
            "PyTorch reshape(3,4):\n",
            "tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n"
          ]
        }
      ],
      "source": [
        "np_arr = np.arange(12)\n",
        "pt_arr = torch.arange(12)\n",
        "print(\"Original shapes:\", np_arr.shape, pt_arr.shape)\n",
        "\n",
        "# NumPy: reshape\n",
        "np_reshaped = np_arr.reshape(3,4)\n",
        "print(\"\\nNumPy reshape(3,4):\")\n",
        "print(np_reshaped)\n",
        "\n",
        "# PyTorch also has reshape!\n",
        "pt_reshaped = pt_arr.reshape(3,4)\n",
        "print(\"\\nPyTorch reshape(3,4):\")\n",
        "print(pt_reshaped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Infer the missing dimension with `-1`.\n",
        "\n",
        "Suppose we have an imaging dataset with shape `(num_images, height, width)`, and we want to convert it to a 2D array with shape `(num_images, height * width)` for modeling (e.g., using a linear regression model).\n",
        "\n",
        "One way to do this is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def flatten_image_data(input_data):\n",
        "    num_images = input_data.shape[0]  # first get the number of images.\n",
        "    height = input_data.shape[1]  # then get the height.\n",
        "    width = input_data.shape[2]  # then get the width.\n",
        "    return input_data.reshape(num_images, height * width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "array([[-0.83175747,  1.70909045, -0.95976222, ...,  0.64370245,\n",
            "         1.25609875,  0.0857383 ],\n",
            "       [ 0.83158161, -1.2127492 ,  1.02904441, ...,  1.75632332,\n",
            "         1.09064765,  0.63171535],\n",
            "       [ 0.42167475,  1.84065853,  0.35019243, ...,  0.97769641,\n",
            "         2.19496482,  0.88373801],\n",
            "       ...,\n",
            "       [-0.92189106, -0.01332148,  0.02257782, ..., -0.94432448,\n",
            "        -0.76903861,  0.9000175 ],\n",
            "       [ 0.44457042,  0.38450181, -0.51066133, ...,  0.34844134,\n",
            "         0.39742803, -1.49165511],\n",
            "       [-0.37761877, -0.87661186,  1.51565247, ..., -1.09130349,\n",
            "        -0.8484453 ,  0.31653122]], shape=(100, 65536))\n",
            "(100, 65536)\n"
          ]
        }
      ],
      "source": [
        "example_image_data = np.random.randn(100, 256, 256)\n",
        "pprint(flatten_image_data(example_image_data))\n",
        "print(flatten_image_data(example_image_data).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferred dimension with -1:\n",
            "[[-0.83175747  1.70909045 -0.95976222 ...  0.64370245  1.25609875\n",
            "   0.0857383 ]\n",
            " [ 0.83158161 -1.2127492   1.02904441 ...  1.75632332  1.09064765\n",
            "   0.63171535]\n",
            " [ 0.42167475  1.84065853  0.35019243 ...  0.97769641  2.19496482\n",
            "   0.88373801]\n",
            " ...\n",
            " [-0.92189106 -0.01332148  0.02257782 ... -0.94432448 -0.76903861\n",
            "   0.9000175 ]\n",
            " [ 0.44457042  0.38450181 -0.51066133 ...  0.34844134  0.39742803\n",
            "  -1.49165511]\n",
            " [-0.37761877 -0.87661186  1.51565247 ... -1.09130349 -0.8484453\n",
            "   0.31653122]]\n",
            "(100, 65536)\n"
          ]
        }
      ],
      "source": [
        "# Using -1 to infer dimension, if we know the first dimension (num_images), and we are going to reduce the 3-D array to 2-D array, the second dimension of the resulting 2-D array is simply total number of entries / first dimension. PyTorch and NumPy will figure this out for you!\n",
        "print(\"\\nInferred dimension with -1:\")\n",
        "print(example_image_data.reshape(len(example_image_data), -1))  # -1 means \"figure this out\"\n",
        "print(example_image_data.reshape(len(example_image_data), -1).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Indexing & Slicing\n",
        "\n",
        "> **TIME: 4:05‚Äì4:12 PM**\n",
        "\n",
        "\n",
        "Works the same in both libraries!\n",
        "\n",
        "\n",
        "### General Syntax\n",
        "\n",
        "| Operation | NumPy | PyTorch | Documentation |\n",
        "|-----------|-------|---------|---------------|\n",
        "| **Basic Indexing** | `arr[i]` | `tensor[i]` | [NumPy](https://numpy.org/doc/stable/user/basics.indexing.html) \\| [PyTorch](https://pytorch.org/docs/stable/tensor_view.html) |\n",
        "| **Slicing** | `arr[start:stop:step]` | `tensor[start:stop:step]` | Same as above |\n",
        "| **2D Indexing** | `arr[row, col]` | `tensor[row, col]` | Same as above |\n",
        "| **2D Slicing** | `arr[r1:r2, c1:c2]` | `tensor[r1:r2, c1:c2]` | Same as above |\n",
        "| **All rows, specific column** | `arr[:, col]` | `tensor[:, col]` | Same as above |\n",
        "| **Negative indexing** | `arr[-1]` (last element) | `tensor[-1]` (last element) | Same as above |\n",
        "| **Reverse array** | `arr[::-1]` | `torch.flip(tensor, [dim])` | [flip](https://pytorch.org/docs/stable/generated/torch.flip.html) |\n",
        "| **Boolean/Fancy indexing** | `arr[mask]` or `arr[indices]` | `tensor[mask]` or `tensor[indices]` | [NumPy advanced](https://numpy.org/doc/stable/user/basics.indexing.html#advanced-indexing) |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "arr[5]: 5 tensor(5)\n",
            "arr[3:7]: [3 4 5 6] tensor([3, 4, 5, 6])\n",
            "arr[::2]: [0 2 4 6 8] tensor([0, 2, 4, 6, 8])\n",
            "arr[::-1]: [9 8 7 6 5 4 3 2 1 0] tensor([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])\n",
            "\n",
            "2D array:\n",
            "[[ 0  1  2  3  4  5]\n",
            " [10 11 12 13 14 15]\n",
            " [20 21 22 23 24 25]\n",
            " [30 31 32 33 34 35]\n",
            " [40 41 42 43 44 45]\n",
            " [50 51 52 53 54 55]]\n",
            "\n",
            "Second row: [10 11 12 13 14 15]\n",
            "Element [2,3]: 23\n",
            "\n",
            "Slice [2:5, 1:4]:\n",
            "[[21 22 23]\n",
            " [31 32 33]\n",
            " [41 42 43]]\n"
          ]
        }
      ],
      "source": [
        "np_arr = np.arange(10)\n",
        "pt_arr = torch.arange(10)\n",
        "\n",
        "print(\"arr[5]:\", np_arr[5], pt_arr[5])\n",
        "print(\"arr[3:7]:\", np_arr[3:7], pt_arr[3:7])\n",
        "print(\"arr[::2]:\", np_arr[::2], pt_arr[::2])  # every other element\n",
        "print(\"arr[::-1]:\", np_arr[::-1], torch.flip(pt_arr, [0]))  # reverse (PyTorch uses flip)\n",
        "\n",
        "# 2D indexing\n",
        "np_2d = 10 * np.arange(6).reshape(-1, 1) + np.arange(6)\n",
        "pt_2d = 10 * torch.arange(6).reshape(-1, 1) + torch.arange(6)\n",
        "print(\"\\n2D array:\")\n",
        "print(np_2d)\n",
        "print(\"\\nSecond row:\", np_2d[1])\n",
        "print(\"Element [2,3]:\", np_2d[2, 3])\n",
        "print(\"\\nSlice [2:5, 1:4]:\")\n",
        "print(np_2d[2:5, 1:4])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fancy Indexing\n",
        "\n",
        "Use arrays/tensors or boolean masks as indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Integer array indexing\n",
        "np_arr = np.arange(10)\n",
        "pt_arr = torch.arange(10)\n",
        "\n",
        "idx = [2, 7, 9]  # Can use Python list as the index.\n",
        "print(\"Select indices [2,7,9]:\")\n",
        "print(np_arr[idx])\n",
        "print(pt_arr[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Elements > 5:\n",
            "[6 7 8 9]\n",
            "tensor([6, 7, 8, 9])\n"
          ]
        }
      ],
      "source": [
        "# Boolean indexing\n",
        "np_mask = np_arr > 5\n",
        "pt_mask = pt_arr > 5\n",
        "print(\"\\nElements > 5:\")\n",
        "print(np_arr[np_mask])\n",
        "print(pt_arr[pt_mask])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "After setting elements < 3 to 100:\n",
            "[100 100 100   3   4   5   6   7   8   9]\n",
            "tensor([100, 100, 100,   3,   4,   5,   6,   7,   8,   9])\n"
          ]
        }
      ],
      "source": [
        "# Modify via boolean indexing\n",
        "np_arr[np_arr < 3] = 100\n",
        "pt_arr[pt_arr < 3] = 100\n",
        "print(\"\\nAfter setting elements < 3 to 100:\")\n",
        "print(np_arr)\n",
        "print(pt_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Concatenation and Stacking (Combining Arrays/Tensors)\n",
        "\n",
        "Use these to merge along an existing axis or create a new axis.\n",
        "\n",
        "| Operation | NumPy | PyTorch | Result |\n",
        "|-----------|-------|---------|--------|\n",
        "| **Concatenate (existing axis)** | `np.concatenate([a, b], axis=dim)` | `torch.cat([a, b], dim=dim)` | Joins along axis `dim` |\n",
        "| **Vertical stack (rows)** | `np.vstack([a, b])` | `torch.vstack([a, b])` | Stacks along first axis |\n",
        "| **Horizontal stack (cols)** | `np.hstack([a, b])` | `torch.hstack([a, b])` | Stacks along second axis |\n",
        "| **Stack (new axis)** | `np.stack([a, b], axis=dim)` | `torch.stack([a, b], dim=dim)` | Adds a new axis |\n",
        "\n",
        "Notes:\n",
        "- Shapes must match on all non-concatenated axes.\n",
        "- Use `stack` when you want to add a new dimension; use `concatenate/cat` when joining along an existing dimension.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = np.arange(6).reshape(2, 3)\n",
        "b = np.arange(100, 106).reshape(2, 3)\n",
        "A = torch.arange(6).reshape(2, 3)\n",
        "B = torch.arange(100, 106).reshape(2, 3)\n",
        "\n",
        "print(\"a:\\n\", a)\n",
        "print(\"b:\\n\", b)\n",
        "print(\"\\nConcatenate rows (axis=0 / dim=0):\")\n",
        "print(np.concatenate([a, b], axis=0))\n",
        "print(torch.cat([A, B], dim=0))\n",
        "\n",
        "print(\"\\nConcatenate cols (axis=1 / dim=1):\")\n",
        "print(np.concatenate([a, b], axis=1))\n",
        "print(torch.cat([A, B], dim=1))\n",
        "\n",
        "print(\"\\nStack along new axis (axis=0 / dim=0):\")\n",
        "print(np.stack([a, b], axis=0).shape)  # (2, 2, 3)\n",
        "print(torch.stack([A, B], dim=0).shape)  # (2, 2, 3)\n",
        "\n",
        "# Convenience helpers (vstack/hstack)\n",
        "print(\"\\nnp.vstack / torch.vstack:\")\n",
        "print(np.vstack([a, b]).shape)\n",
        "print(torch.vstack([A, B]).shape)\n",
        "print(\"\\nnp.hstack / torch.hstack:\")\n",
        "print(np.hstack([a, b]).shape)\n",
        "print(torch.hstack([A, B]).shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Special Matrices\n",
        "\n",
        "> **TIME: 4:12‚Äì4:14 PM**\n",
        "\n",
        "\n",
        "### General Syntax\n",
        "\n",
        "\n",
        "### üìê Mathematical Definitions\n",
        "\n",
        "#### **Identity Matrix**\n",
        "\n",
        "The $n \\times n$ identity matrix $\\mathbf{I}_n$ is defined as:\n",
        "\n",
        "$$\n",
        "(\\mathbf{I}_n)_{ij} = \\begin{cases}\n",
        "1 & \\text{if } i = j \\\\\\\n",
        "0 & \\text{if } i \\neq j\n",
        "\\end{cases}\n",
        "= \\delta_{ij}\n",
        "$$\n",
        "\n",
        "where $\\delta_{ij}$ is the Kronecker delta.\n",
        "\n",
        "**Example:** $\\mathbf{I}_3 = \\begin{bmatrix} 1 & 0 & 0 \\\\\\\\ 0 & 1 & 0 \\\\\\\\ 0 & 0 & 1 \\end{bmatrix}$\n",
        "\n",
        "#### **Diagonal Matrix**\n",
        "\n",
        "A diagonal matrix $\\mathbf{D}$ with diagonal entries $d_1, d_2, \\ldots, d_n$:\n",
        "\n",
        "$$\n",
        "\\mathbf{D} = \\text{diag}(d_1, d_2, \\ldots, d_n) = \\begin{bmatrix}\n",
        "d_1 & 0 & \\cdots & 0 \\\\\\\n",
        "0 & d_2 & \\cdots & 0 \\\\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\\\n",
        "0 & 0 & \\cdots & d_n\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### **Random Matrices**\n",
        "\n",
        "- **Uniform distribution:** Each entry $\\mathbf{A}_{ij} \\sim \\mathcal{U}(0, 1)$ (uniformly distributed between 0 and 1)\n",
        "\n",
        "- **Normal (Gaussian) distribution:** Each entry $\\mathbf{A}_{ij} \\sim \\mathcal{N}(0, 1)$ (mean 0, standard deviation 1)\n",
        "\n",
        "$$\n",
        "\\mathcal{N}(\\mu, \\sigma^2): \\quad f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "| Matrix Type | NumPy | PyTorch | Documentation |\n",
        "|-------------|-------|---------|---------------|\n",
        "| **Identity** | `np.eye(n)` | `torch.eye(n)` | [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.eye.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.eye.html) |\n",
        "| **Diagonal (create)** | `np.diag(vector)` | `torch.diag(vector)` | [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.diag.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.diag.html) |\n",
        "| **Diagonal (extract)** | `np.diag(matrix)` | `torch.diag(matrix)` | Same as above |\n",
        "| **Random uniform** | `np.random.rand(m, n)` | `torch.rand(m, n)` | [NumPy](https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.rand.html) |\n",
        "| **Random normal** | `np.random.randn(m, n)` | `torch.randn(m, n)` | [NumPy](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.randn.html) |\n",
        "| **Random integers** | `np.random.randint(low, high, size)` | `torch.randint(low, high, size)` | [NumPy](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randint.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.randint.html) |\n",
        "| **Set seed** | `np.random.seed(n)` | `torch.manual_seed(n)` | [NumPy](https://numpy.org/doc/stable/reference/random/generated/numpy.random.seed.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.manual_seed.html) |\n",
        "\n",
        "\n",
        "### üí° Self-Study Note: Random Seeds\n",
        "**Reproducibility tip**: Always set seeds when debugging or sharing code! NumPy and PyTorch have independent random states, so you must set both:\n",
        "```python\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "```\n",
        "**Note**: In production, avoid fixed seeds to ensure proper randomness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Identity matrix (5x5):\n",
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1.]]\n",
            "tensor([[1., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 1.]])\n",
            "\n",
            "Diagonal matrix:\n",
            "[[0 0 0 0 0]\n",
            " [0 1 0 0 0]\n",
            " [0 0 2 0 0]\n",
            " [0 0 0 3 0]\n",
            " [0 0 0 0 4]]\n",
            "tensor([[0, 0, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0],\n",
            "        [0, 0, 2, 0, 0],\n",
            "        [0, 0, 0, 3, 0],\n",
            "        [0, 0, 0, 0, 4]])\n",
            "\n",
            "Extracted diagonal:\n",
            "[ 0.45519952 -0.49114465 -0.10803572 -0.71871658]\n",
            "tensor([-0.0223, -0.0888, -1.3211,  1.1827])\n"
          ]
        }
      ],
      "source": [
        "# Identity matrix\n",
        "np_eye = np.eye(5)\n",
        "pt_eye = torch.eye(5)\n",
        "print(\"Identity matrix (5x5):\")\n",
        "print(np_eye)\n",
        "print(pt_eye)\n",
        "\n",
        "# Diagonal matrix\n",
        "np_diag = np.diag(np.arange(5))\n",
        "pt_diag = torch.diag(torch.arange(5))\n",
        "print(\"\\nDiagonal matrix:\")\n",
        "print(np_diag)\n",
        "print(pt_diag)\n",
        "\n",
        "# Extract diagonal from matrix\n",
        "np_M = np.random.randn(4, 4)\n",
        "pt_M = torch.randn(4, 4)\n",
        "print(\"\\nExtracted diagonal:\")\n",
        "print(np.diag(np_M))\n",
        "print(torch.diag(pt_M))\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Random uniform [0, 1)\n",
        "print(\"Uniform random [0,1):\")\n",
        "print(np.random.rand(2, 2))\n",
        "print(torch.rand(2, 2))\n",
        "\n",
        "# Random normal (Gaussian, mean=0, std=1)\n",
        "print(\"\\nNormal distribution:\")\n",
        "print(np.random.randn(2, 2))\n",
        "print(torch.randn(2, 2))\n",
        "\n",
        "# Random integers\n",
        "print(\"\\nRandom integers [0, 10):\")\n",
        "print(np.random.randint(0, 10, size=(2, 2)))\n",
        "print(torch.randint(0, 10, (2, 2)))\n",
        "\n",
        "# Reset seed to get same sequence\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "print(\"\\nAfter resetting seed (should match first output):\")\n",
        "print(np.random.rand(2, 2))\n",
        "print(torch.rand(2, 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Broadcasting (Please read after the lecture)\n",
        "\n",
        "Automatically expand dimensions to match shapes\n",
        "\n",
        "Please read after the lecture:\n",
        "- Broadcasting Rules with NumPy: [https://numpy.org/doc/stable/user/basics.broadcasting.html](https://numpy.org/doc/stable/user/basics.broadcasting.html)\n",
        "\n",
        "- Broadcasting Rules with PyTorch: [https://docs.pytorch.org/docs/stable/notes/broadcasting.html](https://docs.pytorch.org/docs/stable/notes/broadcasting.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Self-Study Note: Broadcasting Rules\n",
        "**The magic of broadcasting**: Arrays with different shapes can be used together if:\n",
        "1. They have the same number of dimensions, OR one can be prepended with 1s\n",
        "2. For each dimension, sizes must match OR one must be 1\n",
        "\n",
        "**Example**: (4,1) + (1,3) ‚Üí (4,3) because dimension sizes are compatible\n",
        "\n",
        "**Mathematical Example:**\n",
        "\n",
        "Adding a column vector to a row vector:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 0 \\\\\\\\ 10 \\\\\\\\ 20 \\end{bmatrix}_{3 \\times 1}\n",
        "+\n",
        "\\begin{bmatrix} 0 & 1 & 2 \\end{bmatrix}_{1 \\times 3}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0+0 & 0+1 & 0+2 \\\\\\\n",
        "10+0 & 10+1 & 10+2 \\\\\\\n",
        "20+0 & 20+1 & 20+2\n",
        "\\end{bmatrix}_{3 \\times 3}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0 & 1 & 2 \\\\\\\n",
        "10 & 11 & 12 \\\\\\\n",
        "20 & 21 & 22\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "### General Syntax\n",
        "\n",
        "Both NumPy and PyTorch follow identical broadcasting rules!\n",
        "\n",
        "| Operation | NumPy Example | PyTorch Example | Documentation |\n",
        "|-----------|---------------|-----------------|---------------|\n",
        "| **Scalar + Array** | `arr + 5` | `tensor + 5` | [NumPy](https://numpy.org/doc/stable/user/basics.broadcasting.html) \\| [PyTorch](https://pytorch.org/docs/stable/notes/broadcasting.html) |\n",
        "| **Array + Vector** | `(m,n) + (n,)` ‚Üí `(m,n)` | `(m,n) + (n,)` ‚Üí `(m,n)` | Same as above |\n",
        "| **Column + Row** | `(m,1) + (1,n)` ‚Üí `(m,n)` | `(m,1) + (1,n)` ‚Üí `(m,n)` | Same as above |\n",
        "| **Automatic expansion** | Dimensions of size 1 are stretched | Dimensions of size 1 are stretched | Same as above |\n",
        "\n",
        "**Broadcasting Rules:**\n",
        "1. If arrays have different ranks, prepend 1s to the shape of the smaller rank array\n",
        "2. Arrays are compatible when for each dimension, sizes match OR one size is 1\n",
        "3. After broadcasting, each array behaves as if it had the larger shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape: (4, 1)\n",
            "[[ 0]\n",
            " [10]\n",
            " [20]\n",
            " [30]]\n",
            "\n",
            "Y shape: (1, 3)\n",
            "[[0 1 2]]\n",
            "\n",
            "X + Y (broadcasting):\n",
            "[[ 0  1  2]\n",
            " [10 11 12]\n",
            " [20 21 22]\n",
            " [30 31 32]]\n",
            "tensor([[ 0,  1,  2],\n",
            "        [10, 11, 12],\n",
            "        [20, 21, 22],\n",
            "        [30, 31, 32]])\n",
            "\n",
            "Matrix A[i,j] = i*j:\n",
            "[[ 0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  1  2  3  4  5  6  7  8  9]\n",
            " [ 0  2  4  6  8 10 12 14 16 18]\n",
            " [ 0  3  6  9 12 15 18 21 24 27]\n",
            " [ 0  4  8 12 16 20 24 28 32 36]]\n",
            "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
            "        [ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18],\n",
            "        [ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27],\n",
            "        [ 0,  4,  8, 12, 16, 20, 24, 28, 32, 36]])\n"
          ]
        }
      ],
      "source": [
        "# Broadcasting example: (4,1) + (1,3) ‚Üí (4,3)\n",
        "np_X = np.arange(4).reshape(-1, 1) * 10  # shape (4,1)\n",
        "np_Y = np.arange(3).reshape(1, -1)        # shape (1,3)\n",
        "\n",
        "pt_X = torch.arange(4).reshape(-1, 1) * 10\n",
        "pt_Y = torch.arange(3).reshape(1, -1)\n",
        "\n",
        "print(f\"X shape: {np_X.shape}\")\n",
        "print(np_X)\n",
        "print(f\"\\nY shape: {np_Y.shape}\")\n",
        "print(np_Y)\n",
        "\n",
        "print(\"\\nX + Y (broadcasting):\")\n",
        "print(np_X + np_Y)\n",
        "print(pt_X + pt_Y)\n",
        "\n",
        "# Creating matrix A[i,j] = i*j using broadcasting\n",
        "np_i = np.arange(5).reshape(-1, 1)  # column vector\n",
        "np_j = np.arange(10).reshape(1, -1)  # row vector\n",
        "np_A = np_i * np_j\n",
        "\n",
        "pt_i = torch.arange(5).reshape(-1, 1)\n",
        "pt_j = torch.arange(10).reshape(1, -1)\n",
        "pt_A = pt_i * pt_j\n",
        "\n",
        "print(\"\\nMatrix A[i,j] = i*j:\")\n",
        "print(np_A)\n",
        "print(pt_A)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Matrix Operations\n",
        "\n",
        "> **TIME: 4:14‚Äì4:15 PM (Summary)**\n",
        "\n",
        "\n",
        "### General Syntax\n",
        "\n",
        "\n",
        "### üìê Mathematical Definitions\n",
        "\n",
        "#### **Transpose**\n",
        "\n",
        "For $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, the transpose $\\mathbf{A}^T \\in \\mathbb{R}^{n \\times m}$:\n",
        "\n",
        "$$\n",
        "(\\mathbf{A}^T)_{ij} = \\mathbf{A}_{ji}\n",
        "$$\n",
        "\n",
        "#### **Matrix Multiplication** (@ operator)\n",
        "\n",
        "For $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{B} \\in \\mathbb{R}^{n \\times p}$:\n",
        "\n",
        "$$\n",
        "(\\mathbf{A} \\mathbf{B})_{ij} = \\sum_{k=1}^{n} \\mathbf{A}_{ik} \\mathbf{B}_{kj}\n",
        "$$\n",
        "\n",
        "#### **Element-wise Multiplication** (* operator)\n",
        "\n",
        "For $\\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{m \\times n}$ (same dimensions):\n",
        "\n",
        "$$\n",
        "(\\mathbf{A} \\odot \\mathbf{B})_{ij} = \\mathbf{A}_{ij} \\cdot \\mathbf{B}_{ij}\n",
        "$$\n",
        "\n",
        "Also called **Hadamard product** or **element-wise product**.\n",
        "\n",
        "#### **Dot Product** (for vectors)\n",
        "\n",
        "For vectors $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^{n}$:\n",
        "\n",
        "$$\n",
        "\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i = \\mathbf{a}^T \\mathbf{b}\n",
        "$$\n",
        "\n",
        "#### **Matrix Inverse**\n",
        "\n",
        "For a square matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, if it exists, $\\mathbf{A}^{-1}$ satisfies:\n",
        "\n",
        "$$\n",
        "\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_n\n",
        "$$\n",
        "\n",
        "where $\\mathbf{I}_n$ is the $n \\times n$ identity matrix.\n",
        "\n",
        "#### **Determinant**\n",
        "\n",
        "For a square matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$:\n",
        "\n",
        "$$\n",
        "\\det(\\mathbf{A}) = \\sum_{\\sigma \\in S_n} \\text{sgn}(\\sigma) \\prod_{i=1}^{n} \\mathbf{A}_{i,\\sigma(i)}\n",
        "$$\n",
        "\n",
        "For $2 \\times 2$ matrices: $\\det\\begin{bmatrix} a & b \\\\\\\\ c & d \\end{bmatrix} = ad - bc$\n",
        "\n",
        "#### **Eigenvalues and Eigenvectors**\n",
        "\n",
        "For a square matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, $\\lambda$ is an eigenvalue and $\\mathbf{v}$ is an eigenvector if:\n",
        "\n",
        "$$\n",
        "\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{v} \\neq \\mathbf{0}$.\n",
        "\n",
        "---\n",
        "\n",
        "| Operation | NumPy | PyTorch | Documentation |\n",
        "|-----------|-------|---------|---------------|\n",
        "| **Transpose** | `arr.T` or `np.transpose(arr)` | `tensor.T` or `torch.t(tensor)` | [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.t.html) |\n",
        "| **Matrix multiply** | `A @ B` or `np.matmul(A, B)` | `A @ B` or `torch.matmul(A, B)` | [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.matmul.html) |\n",
        "| **Element-wise multiply** | `A * B` or `np.multiply(A, B)` | `A * B` or `torch.mul(A, B)` | [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.multiply.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.mul.html) |\n",
        "| **Dot product** | `np.dot(a, b)` | `torch.dot(a, b)` | [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.dot.html) |\n",
        "| **Matrix inverse** | `np.linalg.inv(A)` | `torch.linalg.inv(A)` | [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.linalg.inv.html) |\n",
        "| **Determinant** | `np.linalg.det(A)` | `torch.linalg.det(A)` | [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.linalg.det.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.linalg.det.html) |\n",
        "| **Eigenvalues** | `np.linalg.eig(A)` | `torch.linalg.eig(A)` | [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.linalg.eig.html) |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original (5x3):\n",
            "[[ 0  1  2]\n",
            " [ 3  4  5]\n",
            " [ 6  7  8]\n",
            " [ 9 10 11]\n",
            " [12 13 14]]\n",
            "\n",
            "Transpose (3x5):\n",
            "[[ 0  3  6  9 12]\n",
            " [ 1  4  7 10 13]\n",
            " [ 2  5  8 11 14]]\n",
            "tensor([[ 0,  3,  6,  9, 12],\n",
            "        [ 1,  4,  7, 10, 13],\n",
            "        [ 2,  5,  8, 11, 14]])\n"
          ]
        }
      ],
      "source": [
        "# Transpose\n",
        "np_A = np.arange(15).reshape(5, 3)\n",
        "pt_A = torch.arange(15).reshape(5, 3)\n",
        "print(\"Original (5x3):\")\n",
        "print(np_A)\n",
        "print(\"\\nTranspose (3x5):\")\n",
        "print(np_A.T)\n",
        "print(pt_A.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matrix multiplication: @ operator\n",
        "np_Mat = np.random.randn(10, 10)\n",
        "pt_Mat = torch.randn(10, 10)\n",
        "np_vec = np.arange(10).reshape(10, 1)\n",
        "pt_vec = torch.arange(10).reshape(10, 1)\n",
        "\n",
        "print(\"\\nMatrix multiplication (10,10) @ (10,1):\")\n",
        "np_result = np_Mat @ np_vec\n",
        "pt_result = pt_Mat @ pt_vec\n",
        "print(f\"Result shapes: {np_result.shape}, {pt_result.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Reduction Operations\n",
        "\n",
        "> **TIME: 4:14‚Äì4:15 PM (Summary)**\n",
        "\n",
        "\n",
        "\n",
        "### üìê Mathematical Definitions\n",
        "\n",
        "#### **Sum**\n",
        "\n",
        "For a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$:\n",
        "\n",
        "- **Total sum:** $\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\mathbf{A}_{ij}$\n",
        "\n",
        "- **Sum along axis 0 (collapse rows):** $\\sum_{i=1}^{m} \\mathbf{A}_{ij}$ for each $j$ ‚Üí result shape: $(n,)$\n",
        "\n",
        "- **Sum along axis 1 (collapse columns):** $\\sum_{j=1}^{n} \\mathbf{A}_{ij}$ for each $i$ ‚Üí result shape: $(m,)$\n",
        "\n",
        "#### **Mean (Average)**\n",
        "\n",
        "$$\n",
        "\\text{mean}(\\mathbf{A}) = \\frac{1}{mn} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\mathbf{A}_{ij}\n",
        "$$\n",
        "\n",
        "#### **Minimum and Maximum**\n",
        "\n",
        "$$\n",
        "\\min(\\mathbf{A}) = \\min_{i,j} \\mathbf{A}_{ij}, \\quad \\max(\\mathbf{A}) = \\max_{i,j} \\mathbf{A}_{ij}\n",
        "$$\n",
        "\n",
        "#### **Cumulative Sum**\n",
        "\n",
        "For a vector $\\mathbf{a} = [a_1, a_2, \\ldots, a_n]$:\n",
        "\n",
        "$$\n",
        "\\text{cumsum}(\\mathbf{a}) = \\left[a_1, \\, a_1 + a_2, \\, a_1 + a_2 + a_3, \\, \\ldots, \\, \\sum_{i=1}^{n} a_i\\right]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "Aggregate over axes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np_X = np.arange(50).reshape(10, 5)\n",
        "pt_X = torch.arange(50).reshape(10, 5)\n",
        "\n",
        "print(\"Sum of all elements:\")\n",
        "print(np.sum(np_X), np_X.sum())\n",
        "print(torch.sum(pt_X), pt_X.sum())\n",
        "\n",
        "print(\"\\nSum along axis 0 (collapse rows):\")\n",
        "print(np.sum(np_X, axis=0))\n",
        "print(torch.sum(pt_X, dim=0))\n",
        "\n",
        "print(\"\\nSum along axis 1 (collapse columns):\")\n",
        "print(np.sum(np_X, axis=1))\n",
        "print(torch.sum(pt_X, dim=1))\n",
        "\n",
        "print(\"\\nMin, Max, Mean:\")\n",
        "print(f\"NumPy: min={np_X.min()}, max={np_X.max()}, mean={np_X.mean():.2f}\")\n",
        "print(f\"PyTorch: min={pt_X.min()}, max={pt_X.max()}, mean={pt_X.float().mean():.2f}\")\n",
        "\n",
        "# Cumulative sum\n",
        "np_Y = np.arange(10)\n",
        "pt_Y = torch.arange(10)\n",
        "print(\"\\nCumulative sum:\")\n",
        "print(np.cumsum(np_Y))\n",
        "print(torch.cumsum(pt_Y, dim=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. More Element-wise Operations\n",
        "\n",
        "### General Syntax\n",
        "\n",
        "\n",
        "### üìê Mathematical Definitions\n",
        "\n",
        "All operations below are **element-wise** (applied to each element independently).\n",
        "\n",
        "For matrices $\\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{m \\times n}$ and scalar $c$:\n",
        "\n",
        "#### **Arithmetic Operations**\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{Addition:} \\quad & (\\mathbf{A} + c)_{ij} = \\mathbf{A}_{ij} + c \\\\\\\n",
        "\\text{Subtraction:} \\quad & (\\mathbf{A} - c)_{ij} = \\mathbf{A}_{ij} - c \\\\\\\n",
        "\\text{Multiplication:} \\quad & (\\mathbf{A} \\cdot c)_{ij} = c \\cdot \\mathbf{A}_{ij} \\\\\\\n",
        "\\text{Division:} \\quad & (\\mathbf{A} / c)_{ij} = \\mathbf{A}_{ij} / c \\\\\\\n",
        "\\text{Power:} \\quad & (\\mathbf{A}^c)_{ij} = (\\mathbf{A}_{ij})^c\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "#### **Mathematical Functions**\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{Square root:} \\quad & \\sqrt{\\mathbf{A}}_{ij} = \\sqrt{\\mathbf{A}_{ij}} \\\\\\\n",
        "\\text{Exponential:} \\quad & (e^{\\mathbf{A}})_{ij} = e^{\\mathbf{A}_{ij}} \\\\\\\n",
        "\\text{Natural log:} \\quad & (\\ln \\mathbf{A})_{ij} = \\ln(\\mathbf{A}_{ij}) \\\\\\\n",
        "\\text{Absolute value:} \\quad & |\\mathbf{A}|_{ij} = |\\mathbf{A}_{ij}| \\\\\\\n",
        "\\text{Trigonometric:} \\quad & (\\sin \\mathbf{A})_{ij} = \\sin(\\mathbf{A}_{ij})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "| Operation | NumPy | PyTorch | Documentation |\n",
        "|-----------|-------|---------|---------------|\n",
        "| **Addition** | `arr + 5` or `np.add(arr, 5)` | `tensor + 5` or `torch.add(tensor, 5)` | [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.add.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.add.html) |\n",
        "| **Subtraction** | `arr - 5` or `np.subtract(arr, 5)` | `tensor - 5` or `torch.sub(tensor, 5)` | [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.subtract.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.sub.html) |\n",
        "| **Multiplication** | `arr * 2` or `np.multiply(arr, 2)` | `tensor * 2` or `torch.mul(tensor, 2)` | [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.multiply.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.mul.html) |\n",
        "| **Division** | `arr / 2` or `np.divide(arr, 2)` | `tensor / 2` or `torch.div(tensor, 2)` | [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.divide.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.div.html) |\n",
        "| **Power** | `arr ** 2` or `np.power(arr, 2)` | `tensor ** 2` or `torch.pow(tensor, 2)` | [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.power.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.pow.html) |\n",
        "| **Square root** | `np.sqrt(arr)` | `torch.sqrt(tensor)` | [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.sqrt.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.sqrt.html) |\n",
        "| **Exponential** | `np.exp(arr)` | `torch.exp(tensor)` | [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.exp.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.exp.html) |\n",
        "| **Logarithm** | `np.log(arr)` | `torch.log(tensor)` | [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.log.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.log.html) |\n",
        "| **Absolute** | `np.abs(arr)` | `torch.abs(tensor)` | [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.abs.html) \\| [PyTorch](https://pytorch.org/docs/stable/generated/torch.abs.html) |\n",
        "| **Trigonometric** | `np.sin(arr)`, `np.cos(arr)`, etc. | `torch.sin(tensor)`, `torch.cos(tensor)`, etc. | [NumPy trig](https://numpy.org/doc/stable/reference/routines.math.html) \\| [PyTorch trig](https://pytorch.org/docs/stable/torch.html#pointwise-ops) |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np_arr = np.arange(10)\n",
        "pt_arr = torch.arange(10)\n",
        "\n",
        "# Vectorized operations (no loops needed!)\n",
        "print(\"Original:\", np_arr)\n",
        "print(\"Squared (NumPy):\", np_arr**2)\n",
        "print(\"Squared (PyTorch):\", pt_arr**2)\n",
        "\n",
        "# Element-wise functions\n",
        "print(\"\\nSquare root:\")\n",
        "print(np.sqrt(np_arr.astype(float)))\n",
        "print(torch.sqrt(pt_arr.float()))\n",
        "\n",
        "print(\"\\nExponential:\")\n",
        "print(np.exp(np_arr[:5]))\n",
        "print(torch.exp(pt_arr[:5].float()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. GPU Acceleration (PyTorch)\n",
        "\n",
        "PyTorch makes it easy to move tensors to GPU\n",
        "\n",
        "### General Syntax\n",
        "\n",
        "**NumPy**: CPU-only (no built-in GPU support)\n",
        "\n",
        "**PyTorch**: Full GPU support!\n",
        "\n",
        "| Operation | PyTorch | Documentation |\n",
        "|-----------|---------|---------------|\n",
        "| **Check GPU availability** | `torch.cuda.is_available()` | [CUDA semantics](https://pytorch.org/docs/stable/notes/cuda.html) |\n",
        "| **Create tensor on GPU** | `torch.tensor(data, device='cuda')` | [Tensor creation](https://pytorch.org/docs/stable/torch.html#creation-ops) |\n",
        "| **Move tensor to GPU** | `tensor.to('cuda')` or `tensor.cuda()` | [to method](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html) |\n",
        "| **Move tensor to CPU** | `tensor.to('cpu')` or `tensor.cpu()` | Same as above |\n",
        "| **Get current device** | `tensor.device` | [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device) |\n",
        "| **Specify device** | `device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')` | [torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device) |\n",
        "| **Apple Silicon (MPS)** | `torch.backends.mps.is_available()`, `tensor.to('mps')` | [MPS backend](https://pytorch.org/docs/stable/notes/mps.html) |\n",
        "\n",
        "**Note**: All operations between tensors must be on the same device!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Math Operations\n",
        "\n",
        "Both libraries provide extensive mathematical functions beyond basic arithmetic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Why NumPy and PyTorch Are Essential\n",
        "\n",
        "| Operation | Python Lists | NumPy/PyTorch | Speedup |\n",
        "|-----------|-------------|---------------|---------|\n",
        "| Element-wise ops | `[x**2 for x in data]` | `data**2` | **10-50x** |\n",
        "| Reductions | `sum(data)` | `data.sum()` | **50-100x** |\n",
        "| Matrix ops | Nested loops | `A @ B` | **100-1000x** |\n",
        "| GPU compute | ‚ùå Not possible | ‚úÖ `.to(\"cuda\")` | **1000x+** |\n",
        "\n",
        "**Key Advantages:**\n",
        "1. **Vectorization**: Operations on entire arrays at once (no Python loops!)\n",
        "2. **Optimized C/C++ code**: NumPy/PyTorch are written in compiled languages\n",
        "3. **Memory efficiency**: Contiguous memory, less overhead than Python lists\n",
        "4. **Hardware acceleration**: Can leverage GPUs, SIMD instructions, etc.\n",
        "\n",
        "**When to use Python lists:**\n",
        "- Small data (< 100 elements)\n",
        "- Mixed data types (e.g., `[1, \"hello\", 3.14]`)\n",
        "- Simple scripting where performance doesn't matter\n",
        "\n",
        "**When to use NumPy/PyTorch:**\n",
        "- Numerical computing (always!)\n",
        "- Large datasets\n",
        "- Machine learning / scientific computing\n",
        "- Any performance-critical code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚è±Ô∏è Performance Comparison: Python vs NumPy vs PyTorch\n",
        "\n",
        "Let's measure the actual performance difference!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "TEST 1: Element-wise Squaring (100,000 elements)\n",
            "======================================================================\n",
            "\n",
            "1Ô∏è‚É£  Python list comprehension:\n",
            "3.35 ms ¬± 48.7 Œºs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n",
            "\n",
            "2Ô∏è‚É£  NumPy vectorized:\n",
            "40.6 Œºs ¬± 5.17 Œºs per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)\n",
            "\n",
            "3Ô∏è‚É£  PyTorch vectorized:\n",
            "87.3 Œºs ¬± 5.11 Œºs per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)\n",
            "\n",
            "======================================================================\n",
            "TEST 2: Sum of All Elements (100,000 elements)\n",
            "======================================================================\n",
            "\n",
            "1Ô∏è‚É£  Python sum():\n",
            "278 Œºs ¬± 864 ns per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n",
            "\n",
            "2Ô∏è‚É£  NumPy sum():\n",
            "10.8 Œºs ¬± 446 ns per loop (mean ¬± std. dev. of 7 runs, 100,000 loops each)\n",
            "\n",
            "3Ô∏è‚É£  PyTorch sum():\n",
            "46.8 Œºs ¬± 323 ns per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)\n",
            "\n",
            "======================================================================\n",
            "TEST 3: Matrix Multiplication\n",
            "======================================================================\n",
            "\n",
            "‚ö†Ô∏è  Note: Python nested loops are VERY slow, using 100√ó100\n",
            "\n",
            "1Ô∏è‚É£  Python nested loops (100√ó100):\n",
            "48.2 ms ¬± 4.01 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n",
            "\n",
            "2Ô∏è‚É£  NumPy @ operator (500√ó500):\n",
            "915 Œºs ¬± 56.2 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n",
            "\n",
            "3Ô∏è‚É£  PyTorch @ operator (500√ó500):\n",
            "387 Œºs ¬± 111 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n",
            "\n",
            "======================================================================\n",
            "üöÄ Key Takeaway: NumPy/PyTorch are 10-1000x faster!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Test 1: Element-wise operations\n",
        "print(\"=\" * 70)\n",
        "print(\"TEST 1: Element-wise Squaring (100,000 elements)\")\n",
        "print(\"=\" * 70)\n",
        "n = 100_000\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£  Python list comprehension:\")\n",
        "data_py = list(range(n))\n",
        "%timeit [x**2 for x in data_py]\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£  NumPy vectorized:\")\n",
        "data_np = np.arange(n)\n",
        "%timeit data_np**2\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£  PyTorch vectorized:\")\n",
        "data_pt = torch.arange(n)\n",
        "%timeit data_pt**2\n",
        "\n",
        "# Test 2: Sum reduction\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TEST 2: Sum of All Elements (100,000 elements)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£  Python sum():\")\n",
        "%timeit sum(data_py)\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£  NumPy sum():\")\n",
        "%timeit data_np.sum()\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£  PyTorch sum():\")\n",
        "%timeit data_pt.sum()\n",
        "\n",
        "# Test 3: Matrix multiplication\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TEST 3: Matrix Multiplication\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create smaller matrices for Python (to avoid waiting too long)\n",
        "print(\"\\n‚ö†Ô∏è  Note: Python nested loops are VERY slow, using 100√ó100\")\n",
        "size_small = 100\n",
        "a_py = [[float(i*size_small + j) for j in range(size_small)] for i in range(size_small)]\n",
        "b_py = [[float(i*size_small + j) for j in range(size_small)] for i in range(size_small)]\n",
        "\n",
        "print(f\"\\n1Ô∏è‚É£  Python nested loops ({size_small}√ó{size_small}):\")\n",
        "%timeit result = [[sum(a_py[i][k] * b_py[k][j] for k in range(size_small)) for j in range(size_small)] for i in range(size_small)]\n",
        "\n",
        "# NumPy and PyTorch can handle larger matrices\n",
        "size = 500\n",
        "a_np = np.arange(size*size, dtype=float).reshape(size, size)\n",
        "b_np = np.arange(size*size, dtype=float).reshape(size, size)\n",
        "\n",
        "print(f\"\\n2Ô∏è‚É£  NumPy @ operator ({size}√ó{size}):\")\n",
        "%timeit a_np @ b_np\n",
        "\n",
        "a_pt = torch.arange(size*size, dtype=torch.float32).reshape(size, size)\n",
        "b_pt = torch.arange(size*size, dtype=torch.float32).reshape(size, size)\n",
        "\n",
        "print(f\"\\n3Ô∏è‚É£  PyTorch @ operator ({size}√ó{size}):\")\n",
        "%timeit a_pt @ b_pt\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ Key Takeaway: NumPy/PyTorch are 10-1000x faster!\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "**NumPy** is the foundation for numerical computing in Python:\n",
        "- Fast array operations\n",
        "- Rich ecosystem of scientific libraries\n",
        "- CPU-only\n",
        "\n",
        "**PyTorch** extends the NumPy paradigm with:\n",
        "- GPU acceleration for massive speedups\n",
        "- Automatic differentiation for machine learning\n",
        "- Nearly identical API for basic operations\n",
        "\n",
        "**Best practice:** Use NumPy for general scientific computing and data analysis. Use PyTorch when you need gradients or GPU acceleration (especially for deep learning).\n",
        "\n",
        "Both libraries are essential tools in modern scientific Python!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cme193-autumn-2025",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
